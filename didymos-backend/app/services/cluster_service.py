"""
Knowledge Graph Clustering Service
Neo4j GDS ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œ ì»¤ë®¤ë‹ˆí‹° íƒì§€ ë° LLM ê¸°ë°˜ ìš”ì•½

Graph-based Context Analysis:
- ë‹¨ìˆœ mention_count ëŒ€ì‹  ê·¸ë˜í”„ ì¤‘ì‹¬ì„±(centrality)ìœ¼ë¡œ ì—”í‹°í‹° ì¤‘ìš”ë„ ê²°ì •
- ì—¬ëŸ¬ ë…¸íŠ¸ë¥¼ ì—°ê²°í•˜ëŠ” "ë‹¤ë¦¬" ì—­í•  ì—”í‹°í‹°ê°€ ë” ë†’ì€ ì ìˆ˜
- í´ëŸ¬ìŠ¤í„° ì´ë¦„ë„ ë…¸íŠ¸ ì œëª©ì´ ì•„ë‹Œ ê·¸ë˜í”„ í—ˆë¸Œ ì—”í‹°í‹° ê¸°ë°˜ìœ¼ë¡œ ê²°ì •
"""
from typing import Dict, Any, List, Optional, Tuple
import logging
from datetime import datetime, timedelta
import json
import numpy as np
from collections import defaultdict

logger = logging.getLogger(__name__)

# ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë¬´ì˜ë¯¸í•œ ì¼ë°˜ ì—”í‹°í‹° ë¸”ë™ë¦¬ìŠ¤íŠ¸
# ì´ ì—”í‹°í‹°ë“¤ì€ í—ˆë¸Œ ì„ ì •ì—ì„œ ì œì™¸ë¨ (ë¹ˆë„ê°€ ë†’ì§€ë§Œ í´ëŸ¬ìŠ¤í„° íŠ¹ì„±ì„ ëŒ€í‘œí•˜ì§€ ì•ŠìŒ)
GENERIC_ENTITY_BLACKLIST = {
    # ê¸°ê´€/ì¡°ì§ (ë„ˆë¬´ ì¼ë°˜ì )
    "ì„œìš¸ëŒ€í•™êµ", "ì„œìš¸ëŒ€", "snu", "seoul national university",
    "ëŒ€í•™êµ", "ëŒ€í•™", "ì—°êµ¬ì‹¤", "ì—°êµ¬ì†Œ", "í•™êµ",
    # ì¼ë°˜ ê°œë…
    "ì—°êµ¬", "ë…¼ë¬¸", "í”„ë¡œì íŠ¸", "íšŒì˜", "ë¯¸íŒ…", "ì •ë¦¬", "ë©”ëª¨", "ë…¸íŠ¸",
    "research", "paper", "project", "meeting", "note", "memo",
    # ë‚ ì§œ/ì‹œê°„
    "2024", "2025", "ì˜¤ëŠ˜", "ë‚´ì¼", "ì´ë²ˆì£¼",
    # ê¸°íƒ€ ì¼ë°˜
    "todo", "task", "idea", "ê°œë…", "ì •ì˜", "ìš”ì•½",
}

# UMAP + HDBSCAN imports (lazy import for performance)
try:
    import umap
    import hdbscan
    CLUSTERING_AVAILABLE = True
except ImportError:
    CLUSTERING_AVAILABLE = False
    logger.warning("UMAP or HDBSCAN not available. Semantic clustering disabled.")


def compute_entity_graph_centrality(
    client,
    note_ids: List[str],
    include_types: List[str] = ["Topic", "Project", "Task", "Person"],
    vault_total_notes: int = None,
    include_entity_node: bool = True
) -> Dict[str, Dict[str, Any]]:
    """
    ê·¸ë˜í”„ êµ¬ì¡° ê¸°ë°˜ ì—”í‹°í‹° ì¤‘ìš”ë„ ê³„ì‚° (IDF ê°€ì¤‘ì¹˜ í¬í•¨)

    ë…¸íŠ¸ ê°„ ì—°ê²°ì„±ì„ ë¶„ì„í•˜ì—¬ "ë‹¤ë¦¬" ì—­í• ì„ í•˜ëŠ” ì—”í‹°í‹°ì— ë†’ì€ ì ìˆ˜ ë¶€ì—¬:
    1. degree_centrality: ì—°ê²°ëœ ë…¸íŠ¸ ìˆ˜
    2. bridge_score: ì„œë¡œ ë‹¤ë¥¸ ë…¸íŠ¸ë“¤ì„ ì—°ê²°í•˜ëŠ” ì •ë„
    3. co_occurrence_score: ë‹¤ë¥¸ ì—”í‹°í‹°ë“¤ê³¼ í•¨ê»˜ ë“±ì¥í•˜ëŠ” ë¹ˆë„
    4. idf_weight: ì „ì²´ vaultì—ì„œ í¬ì†Œí• ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜ (TF-IDFì˜ IDF)

    Args:
        client: Neo4j í´ë¼ì´ì–¸íŠ¸
        note_ids: ë¶„ì„í•  ë…¸íŠ¸ ID ë¦¬ìŠ¤íŠ¸
        include_types: í¬í•¨í•  ì—”í‹°í‹° íƒ€ì…
        vault_total_notes: ì „ì²´ vaultì˜ ë…¸íŠ¸ ìˆ˜ (IDF ê³„ì‚°ìš©)
        include_entity_node: Graphiti EntityNodeë„ í¬í•¨í• ì§€ ì—¬ë¶€

    Returns:
        {entity_id: {name, type, centrality_score, degree, bridge_score, co_occurrence, idf_weight}}
    """
    if not note_ids:
        return {}

    # PKM íƒ€ì… í•„í„° + EntityNode (Graphiti) ì§€ì›
    type_conditions = [f"'{t}' IN labels(entity)" for t in include_types]
    if include_entity_node:
        # EntityNode with PKM labels (hybrid mode)
        type_conditions.append("'EntityNode' IN labels(entity)")
    type_filter = " OR ".join(type_conditions)

    # Step 1: ê° ì—”í‹°í‹°ì˜ degree (ì—°ê²°ëœ ë…¸íŠ¸ ìˆ˜) ê³„ì‚°
    cypher_degree = f"""
    MATCH (note:Note)-[:MENTIONS]->(entity)
    WHERE note.note_id IN $note_ids AND ({type_filter})
    WITH entity,
         entity.id as entity_id,
         entity.name as entity_name,
         labels(entity)[0] as entity_type,
         COLLECT(DISTINCT note.note_id) as connected_notes
    RETURN entity_id,
           COALESCE(entity_name, entity_id) as entity_name,
           entity_type,
           SIZE(connected_notes) as degree,
           connected_notes
    """

    degree_result = client.query(cypher_degree, {"note_ids": note_ids})

    if not degree_result:
        return {}

    # ì—”í‹°í‹°ë³„ ì—°ê²° ì •ë³´ ìˆ˜ì§‘
    entity_info = {}
    entity_notes = {}  # entity_id -> set of note_ids

    for row in degree_result:
        entity_id = row["entity_id"]
        entity_info[entity_id] = {
            "name": row["entity_name"],
            "type": row["entity_type"],
            "degree": row["degree"],
            "connected_notes": set(row["connected_notes"])
        }
        entity_notes[entity_id] = set(row["connected_notes"])

    # Step 2: Co-occurrence ë¶„ì„ (ê°™ì€ ë…¸íŠ¸ì— ë“±ì¥í•˜ëŠ” ì—”í‹°í‹° ìŒ)
    # ë§ì€ ë‹¤ë¥¸ ì—”í‹°í‹°ì™€ í•¨ê»˜ ë“±ì¥í•˜ëŠ” ì—”í‹°í‹° = í—ˆë¸Œ ì—­í• 
    cypher_cooccurrence = f"""
    MATCH (note:Note)-[:MENTIONS]->(e1)
    MATCH (note)-[:MENTIONS]->(e2)
    WHERE note.note_id IN $note_ids
      AND ({type_filter.replace('entity', 'e1')})
      AND ({type_filter.replace('entity', 'e2')})
      AND e1.id < e2.id
    WITH e1.id as entity1, e2.id as entity2, COUNT(DISTINCT note) as shared_notes
    WHERE shared_notes > 0
    RETURN entity1, entity2, shared_notes
    """

    cooccurrence_result = client.query(cypher_cooccurrence, {"note_ids": note_ids})

    # ê° ì—”í‹°í‹°ì˜ co-occurrence íŒŒíŠ¸ë„ˆ ìˆ˜ ê³„ì‚°
    co_occurrence_count = defaultdict(int)
    co_occurrence_strength = defaultdict(float)  # ì—°ê²° ê°•ë„ í•©ì‚°

    for row in (cooccurrence_result or []):
        e1, e2 = row["entity1"], row["entity2"]
        shared = row["shared_notes"]
        co_occurrence_count[e1] += 1
        co_occurrence_count[e2] += 1
        co_occurrence_strength[e1] += shared
        co_occurrence_strength[e2] += shared

    # Step 3: Bridge Score + IDF ê°€ì¤‘ì¹˜ ê³„ì‚°
    # ì„œë¡œ ë‹¤ë¥¸ ë…¸íŠ¸ ê·¸ë£¹ì„ ì—°ê²°í•˜ëŠ” ì—”í‹°í‹°ì— ë†’ì€ ì ìˆ˜
    # ë‹¨, ì „ì²´ vaultì—ì„œ ë„ˆë¬´ ë§ì´ ë“±ì¥í•˜ëŠ” ì—”í‹°í‹°ëŠ” IDFë¡œ ê°ì 

    total_notes = len(note_ids)
    max_degree = max((info["degree"] for info in entity_info.values()), default=1)
    max_cooccurrence = max(co_occurrence_count.values(), default=1)

    # IDF ê³„ì‚°ìš© vault ì „ì²´ ë…¸íŠ¸ ìˆ˜ (ì—†ìœ¼ë©´ í´ëŸ¬ìŠ¤í„° ë…¸íŠ¸ ìˆ˜ ì‚¬ìš©)
    vault_size = vault_total_notes if vault_total_notes else total_notes

    for entity_id, info in entity_info.items():
        degree = info["degree"]
        entity_name = info.get("name", entity_id)

        # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì²´í¬ - ì¼ë°˜ì ì¸ ì—”í‹°í‹°ëŠ” ë‚®ì€ ì ìˆ˜
        is_generic = entity_name.lower() in GENERIC_ENTITY_BLACKLIST or entity_id.lower() in GENERIC_ENTITY_BLACKLIST
        generic_penalty = 0.1 if is_generic else 1.0

        # Degree centrality (ì •ê·œí™”)
        degree_centrality = degree / max(total_notes, 1)

        # Co-occurrence score (ì •ê·œí™”)
        co_count = co_occurrence_count.get(entity_id, 0)
        co_score = co_count / max(max_cooccurrence, 1)

        # Bridge score: ì—°ê²° ê°•ë„ì˜ ë¶„ì‚° (ë§ì€ ë…¸íŠ¸ì— ê³ ë¥´ê²Œ ë“±ì¥í• ìˆ˜ë¡ ë†’ìŒ)
        connected = info["connected_notes"]
        if len(connected) > 1:
            # ì—°ê²°ëœ ë…¸íŠ¸ë“¤ì´ ë¶„ì‚°ë˜ì–´ ìˆì„ìˆ˜ë¡ bridge score ë†’ìŒ
            bridge_score = min(1.0, len(connected) / max(3, total_notes * 0.2))
        else:
            bridge_score = 0.0

        # IDF ê°€ì¤‘ì¹˜: ì „ì²´ vaultì—ì„œ í¬ì†Œí• ìˆ˜ë¡ ë†’ìŒ
        # log(N / df) where N=ì´ ë¬¸ì„œìˆ˜, df=ì—”í‹°í‹°ê°€ ë“±ì¥í•˜ëŠ” ë¬¸ì„œìˆ˜
        # ëª¨ë“  ë…¸íŠ¸ì— ë“±ì¥í•˜ë©´ idf=0, ì¼ë¶€ì—ë§Œ ë“±ì¥í•˜ë©´ idf ë†’ìŒ
        if vault_size > 0 and degree > 0:
            # í´ëŸ¬ìŠ¤í„° ë‚´ ë¹„ìœ¨ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ê°ì  (50% ì´ìƒ ë…¸íŠ¸ì— ë“±ì¥ ì‹œ)
            cluster_coverage = degree / total_notes
            if cluster_coverage > 0.5:
                # í´ëŸ¬ìŠ¤í„°ì˜ 50% ì´ìƒ ë…¸íŠ¸ì— ë“±ì¥ = ë„ˆë¬´ ì¼ë°˜ì 
                idf_weight = max(0.3, 1.0 - (cluster_coverage - 0.5) * 1.5)
            else:
                idf_weight = 1.0
        else:
            idf_weight = 1.0

        # ì¢…í•© ì¤‘ì‹¬ì„± ì ìˆ˜ (ê°€ì¤‘ í•©ì‚° + IDF + ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒ¨ë„í‹°)
        # degree: 30%, co-occurrence: 25%, bridge: 25%, idf: 20%
        raw_centrality = (
            0.30 * degree_centrality +
            0.25 * co_score +
            0.25 * bridge_score +
            0.20 * idf_weight
        )

        # ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒ¨ë„í‹° ì ìš©
        centrality_score = raw_centrality * generic_penalty

        info["degree_centrality"] = degree_centrality
        info["co_occurrence_count"] = co_count
        info["co_occurrence_score"] = co_score
        info["bridge_score"] = bridge_score
        info["idf_weight"] = idf_weight
        info["is_generic"] = is_generic
        info["centrality_score"] = centrality_score

    return entity_info


def find_cluster_hub_entities(
    entity_info: Dict[str, Dict[str, Any]],
    top_k: int = 3,
    exclude_generic: bool = True
) -> List[Tuple[str, str, float]]:
    """
    í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œ í—ˆë¸Œ ì—­í• ì„ í•˜ëŠ” ìƒìœ„ ì—”í‹°í‹° ì°¾ê¸°

    Args:
        entity_info: ì—”í‹°í‹° ì •ë³´ ë”•ì…”ë„ˆë¦¬
        top_k: ë°˜í™˜í•  ìƒìœ„ ì—”í‹°í‹° ìˆ˜
        exclude_generic: ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì—”í‹°í‹° ì œì™¸ ì—¬ë¶€

    Returns:
        [(entity_id, entity_name, centrality_score), ...]
    """
    if not entity_info:
        return []

    # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì—”í‹°í‹° í•„í„°ë§ (ì„ íƒì )
    filtered_entities = entity_info.items()
    if exclude_generic:
        filtered_entities = [
            (eid, info) for eid, info in entity_info.items()
            if not info.get("is_generic", False)
        ]

    # centrality_scoreë¡œ ì •ë ¬
    sorted_entities = sorted(
        filtered_entities,
        key=lambda x: x[1].get("centrality_score", 0),
        reverse=True
    )

    # ìƒìœ„ kê°œ ë°˜í™˜ (ë¸”ë™ë¦¬ìŠ¤íŠ¸ í•„í„°ë§ í›„ì—ë„ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ë¸”ë™ë¦¬ìŠ¤íŠ¸ í¬í•¨)
    result = [
        (eid, info["name"], info["centrality_score"])
        for eid, info in sorted_entities[:top_k]
    ]

    # ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì—”í‹°í‹°ë„ í¬í•¨
    if len(result) < top_k and exclude_generic:
        all_sorted = sorted(
            entity_info.items(),
            key=lambda x: x[1].get("centrality_score", 0),
            reverse=True
        )
        for eid, info in all_sorted:
            if len(result) >= top_k:
                break
            if (eid, info["name"], info["centrality_score"]) not in result:
                result.append((eid, info["name"], info["centrality_score"]))

    return result[:top_k]


def generate_cluster_name_from_graph(
    hub_entities: List[Tuple[str, str, float]],
    type_counts: Dict[str, int]
) -> str:
    """
    ê·¸ë˜í”„ í—ˆë¸Œ ì—”í‹°í‹° ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° ì´ë¦„ ìƒì„±

    ë…¸íŠ¸ ì œëª©ì´ ì•„ë‹Œ, ê·¸ë˜í”„ì—ì„œ ì¤‘ì‹¬ ì—­í• ì„ í•˜ëŠ” ê°œë…ìœ¼ë¡œ ëª…ëª…
    """
    if not hub_entities:
        # í´ë°±: íƒ€ì… ê¸°ë°˜ ì´ë¦„
        dominant_type = max(type_counts.items(), key=lambda x: x[1])[0] if type_counts else "Unknown"
        return f"{dominant_type.title()} Cluster"

    # ìƒìœ„ 1-2ê°œ í—ˆë¸Œ ì—”í‹°í‹°ë¡œ ì´ë¦„ êµ¬ì„±
    top_names = [name for _, name, _ in hub_entities[:2]]

    if len(top_names) == 1:
        return f"{top_names[0]} ì¤‘ì‹¬"
    else:
        return f"{top_names[0]} & {top_names[1]}"


def compute_clusters_louvain(
    client,
    vault_id: str,
    target_clusters: int = 10,
    include_types: List[str] = ["Topic", "Project", "Task", "Person"],
    folder_prefix: str = None,
    include_entity_node: bool = True
) -> Dict[str, Any]:
    """
    Louvain ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ ì»¤ë®¤ë‹ˆí‹° íƒì§€

    Args:
        client: Neo4j Bolt í´ë¼ì´ì–¸íŠ¸
        vault_id: Vault ID
        target_clusters: ëª©í‘œ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
        include_types: í´ëŸ¬ìŠ¤í„°ë§ì— í¬í•¨í•  ë…¸ë“œ íƒ€ì…
        folder_prefix: í´ë” ê²½ë¡œ í•„í„° (ì˜ˆ: '1_í”„ë¡œì íŠ¸/')
        include_entity_node: Graphiti EntityNodeë„ í¬í•¨í• ì§€ ì—¬ë¶€

    Returns:
        í´ëŸ¬ìŠ¤í„° ë°ì´í„°
    """
    try:
        # Step 1: ì„œë¸Œê·¸ë˜í”„ íˆ¬ì˜ (Notes ì œì™¸, Entitiesë§Œ)
        # PKM íƒ€ì… + EntityNode (Graphiti) ì§€ì›
        type_conditions = [f"'{t}' IN labels(entity)" for t in include_types]
        if include_entity_node:
            type_conditions.append("'EntityNode' IN labels(entity)")
        type_filter = " OR ".join(type_conditions)
        folder_filter = ""
        if folder_prefix:
            folder_filter = "AND note.note_id STARTS WITH $folder_prefix"

        cypher_projection = f"""
        MATCH (v:Vault {{id: $vault_id}})-[:HAS_NOTE]->(note:Note)
        MATCH (note)-[r:MENTIONS]->(entity)
        WHERE ({type_filter}) {folder_filter}
        WITH entity, COUNT(DISTINCT note) as mention_count
        WHERE mention_count > 0
        RETURN entity.id as entity_id, labels(entity)[0] as type, mention_count
        LIMIT 1000
        """

        params = {"vault_id": vault_id}
        if folder_prefix:
            params["folder_prefix"] = folder_prefix

        entities = client.query(cypher_projection, params)

        if not entities or len(entities) == 0:
            logger.warning(f"No entities found for vault {vault_id}")
            return {
                "clusters": [],
                "edges": [],
                "total_nodes": 0,
                "method": "louvain",
                "computed_at": datetime.utcnow().isoformat()
            }

        # Step 2: ì—”í‹°í‹° ê°„ ê´€ê³„ ê°€ì ¸ì˜¤ê¸° (ê³µí†µ ë…¸íŠ¸ë¡œ ì—°ê²°)
        type_filter_e1 = " OR ".join([f"'{t}' IN labels(e1)" for t in include_types])
        type_filter_e2 = " OR ".join([f"'{t}' IN labels(e2)" for t in include_types])

        cypher_relations = f"""
        MATCH (v:Vault {{id: $vault_id}})-[:HAS_NOTE]->(note:Note)
        MATCH (note)-[:MENTIONS]->(e1)
        MATCH (note)-[:MENTIONS]->(e2)
        WHERE id(e1) < id(e2)
          AND ({type_filter_e1})
          AND ({type_filter_e2})
          {folder_filter}
        WITH e1, e2, COUNT(DISTINCT note) as shared_notes
        WHERE shared_notes > 0
        RETURN e1.id as from_id, e2.id as to_id, shared_notes as weight
        """

        relations = client.query(cypher_relations, params)

        # Step 3: ê°„ë‹¨í•œ í´ëŸ¬ìŠ¤í„°ë§ (Neo4j GDS ì—†ì´ Pythonì—ì„œ ì²˜ë¦¬)
        # ì‹¤ì œë¡œëŠ” networkx ë˜ëŠ” Neo4j GDSë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ,
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ connected components ê¸°ë°˜ìœ¼ë¡œ ì‹œì‘

        clusters = _simple_clustering(entities, relations, target_clusters)

        return {
            "clusters": clusters,
            "edges": [],
            "total_nodes": len(entities),
            "method": "louvain",
            "computed_at": datetime.utcnow().isoformat()
        }

    except Exception as e:
        logger.error(f"Cluster computation failed: {e}")
        raise


def _simple_clustering(
    entities: List[Dict],
    relations: List[Dict],
    target_clusters: int
) -> List[Dict[str, Any]]:
    """
    ê°„ë‹¨í•œ ì—°ê²° ì„±ë¶„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
    (ì¶”í›„ networkx ë˜ëŠ” Neo4j GDSë¡œ êµì²´ ì˜ˆì •)
    """
    # ì—”í‹°í‹°ë¥¼ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
    clusters_by_type = {}
    for entity in entities:
        entity_type = entity.get("type", "Unknown")
        if entity_type not in clusters_by_type:
            clusters_by_type[entity_type] = []
        clusters_by_type[entity_type].append(entity)

    # ê° íƒ€ì…ì„ í´ëŸ¬ìŠ¤í„°ë¡œ ë³€í™˜
    clusters = []
    cluster_id = 0

    for entity_type, entity_list in clusters_by_type.items():
        cluster_id += 1
        node_count = len(entity_list)

        # mention_count í•©ì‚°
        total_mentions = sum(e.get("mention_count", 0) for e in entity_list)
        importance_score = min(10.0, total_mentions / 10.0)

        clusters.append({
            "id": f"cluster_{cluster_id}",
            "name": f"{entity_type} Cluster",
            "level": 1,
            "node_count": node_count,
            "entity_ids": [e.get("entity_id") for e in entity_list if e.get("entity_id")],
            "summary": f"Contains {node_count} {entity_type} entities",
            "key_insights": [
                f"Total mentions: {total_mentions}",
                f"Entity type: {entity_type}"
            ],
            "sample_entities": [],
            "sample_notes": [],
            "note_ids": [],
            "recent_updates": 0,
            "importance_score": importance_score,
            "last_updated": datetime.utcnow().isoformat(),
            "last_computed": datetime.utcnow().isoformat(),
            "clustering_method": "type_based",
            "is_manual": False,
            "contains_types": {entity_type.lower(): node_count}
        })

    return clusters


def compute_clusters_semantic(
    client,
    vault_id: str,
    target_clusters: int = 10,
    include_types: List[str] = ["Topic", "Project", "Task", "Person"],
    folder_prefix: str = None,
    include_entity_node: bool = True
) -> Dict[str, Any]:
    """
    UMAP + HDBSCANì„ ì‚¬ìš©í•œ ì˜ë¯¸ë¡ ì  í´ëŸ¬ìŠ¤í„°ë§

    ë…¸íŠ¸ ì„ë² ë”©ì„ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§í•œ í›„, ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ ì–¸ê¸‰ëœ ì—”í‹°í‹°ë¥¼ ì§‘ê³„

    Args:
        client: Neo4j Bolt í´ë¼ì´ì–¸íŠ¸
        vault_id: Vault ID
        target_clusters: ëª©í‘œ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ (ì°¸ê³ ìš©, HDBSCANì´ ìë™ ê²°ì •)
        include_types: í´ëŸ¬ìŠ¤í„°ë§ì— í¬í•¨í•  ë…¸ë“œ íƒ€ì…
        folder_prefix: í´ë” ê²½ë¡œ í•„í„° (ì˜ˆ: '1_í”„ë¡œì íŠ¸/')
        include_entity_node: Graphiti EntityNodeë„ í¬í•¨í• ì§€ ì—¬ë¶€

    Returns:
        í´ëŸ¬ìŠ¤í„° ë°ì´í„°
    """
    if not CLUSTERING_AVAILABLE:
        logger.error("UMAP/HDBSCAN not available. Falling back to type-based clustering.")
        return _semantic_fallback_to_type_based(
            client,
            vault_id,
            target_clusters,
            include_types,
            reason="dependency_missing",
            folder_prefix=folder_prefix
        )

    try:
        # Step 1: Neo4jì—ì„œ ë…¸íŠ¸ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸° (í´ë” í•„í„° ì ìš©)
        folder_filter = ""
        if folder_prefix:
            folder_filter = "AND note.note_id STARTS WITH $folder_prefix"

        cypher_embeddings = f"""
        MATCH (v:Vault {{id: $vault_id}})-[:HAS_NOTE]->(note:Note)
        WHERE note.embedding IS NOT NULL {folder_filter}
        RETURN note.note_id as note_id,
               note.title as note_title,
               toString(note.updated_at) as updated_at,
               note.embedding as embedding
        """

        params = {"vault_id": vault_id}
        if folder_prefix:
            params["folder_prefix"] = folder_prefix

        results = client.query(cypher_embeddings, params)

        if not results or len(results) == 0:
            logger.warning(f"No notes with embeddings found for vault {vault_id}")
            return _semantic_fallback_to_type_based(
                client,
                vault_id,
                target_clusters,
                include_types,
                reason="no_embeddings",
                folder_prefix=folder_prefix
            )

        # Step 2: ì„ë² ë”© ë°°ì—´ë¡œ ë³€í™˜
        note_ids = [r["note_id"] for r in results]
        note_titles = [r["note_title"] for r in results]
        note_updated_raw = [r.get("updated_at") for r in results]

        # ì„ë² ë”©ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
        embeddings = np.array([r["embedding"] for r in results], dtype=float)

        if len(embeddings) < 5:
            logger.warning(f"Not enough embeddings for semantic clustering ({len(embeddings)} < 5)")
            return _semantic_fallback_to_type_based(
                client,
                vault_id,
                target_clusters,
                include_types,
                reason="insufficient_samples",
                folder_prefix=folder_prefix
            )

        logger.info(f"Found {len(embeddings)} notes with embeddings (shape: {embeddings.shape})")

        # Step 3: UMAP ì°¨ì› ì¶•ì†Œ
        logger.info("Running UMAP dimensionality reduction...")

        # UMAP íŒŒë¼ë¯¸í„° ë™ì  ì¡°ì •
        n_samples = len(embeddings)
        n_components = min(5, n_samples - 1)  # ìƒ˜í”Œ ìˆ˜ë³´ë‹¤ ì‘ê²Œ
        n_neighbors = max(2, min(15, n_samples - 1))  # ìµœì†Œ 2, ìµœëŒ€ 15

        reducer = umap.UMAP(
            n_components=n_components,
            n_neighbors=n_neighbors,
            min_dist=0.1,
            metric='cosine',
            random_state=42
        )
        reduced_embeddings = reducer.fit_transform(embeddings)

        logger.info(f"UMAP completed: {embeddings.shape} â†’ {reduced_embeddings.shape}")

        # Step 4: HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
        logger.info("Running HDBSCAN clustering...")

        # ë” ì„¸ë¶„í™”ëœ í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ì¡°ì •
        # - min_cluster_size: ì‘ì„ìˆ˜ë¡ ë” ë§ì€ ì‘ì€ í´ëŸ¬ìŠ¤í„° í—ˆìš©
        # - min_samples: 1ì´ë©´ ë…¸ì´ì¦ˆ ìµœì†Œí™”
        # - cluster_selection_epsilon: ì‘ì„ìˆ˜ë¡ ë” ì„¸ë¶„í™”ë¨
        n_samples = len(embeddings)
        min_cluster_size = max(5, n_samples // 50)  # ë” ì‘ì€ í´ëŸ¬ìŠ¤í„° í—ˆìš© (5ê°œ ë˜ëŠ” ë…¸íŠ¸ì˜ 2%)

        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=2,
            cluster_selection_epsilon=0.1,  # ë” ì„¸ë¶„í™”
            cluster_selection_method='eom',  # Excess of Mass - ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ì— ì í•©
            metric='euclidean'
        )
        cluster_labels = clusterer.fit_predict(reduced_embeddings)

        # í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ (ë…¸ì´ì¦ˆ ì œì™¸)
        unique_labels = set(cluster_labels)
        unique_labels.discard(-1)  # ë…¸ì´ì¦ˆ ì œê±°
        n_clusters = len(unique_labels)

        logger.info(f"HDBSCAN found {n_clusters} clusters (+ {sum(cluster_labels == -1)} noise points)")

        if n_clusters == 0:
            logger.warning("HDBSCAN returned no clusters. Falling back to type-based clustering.")
            return _semantic_fallback_to_type_based(
                client,
                vault_id,
                target_clusters,
                include_types,
                reason="no_clusters",
                folder_prefix=folder_prefix
            )

        # Step 5: ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ ê·¸ë˜í”„ ì¤‘ì‹¬ì„± ê¸°ë°˜ ì—”í‹°í‹° ë¶„ì„
        # mention_count ëŒ€ì‹  centrality_scoreë¡œ ì¤‘ìš”ë„ ê²°ì •
        clusters = []

        for cluster_id in sorted(unique_labels):
            # í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ë…¸íŠ¸ë“¤
            cluster_mask = cluster_labels == cluster_id
            cluster_note_ids = [note_ids[i] for i in range(len(note_ids)) if cluster_mask[i]]
            cluster_note_titles = [note_titles[i] for i in range(len(note_titles)) if cluster_mask[i]]
            cluster_note_updates = [
                _parse_datetime(note_updated_raw[i]) for i in range(len(note_updated_raw)) if cluster_mask[i]
            ]

            # ê·¸ë˜í”„ ì¤‘ì‹¬ì„± ë¶„ì„ (í•µì‹¬ ë³€ê²½ì !)
            entity_info = compute_entity_graph_centrality(
                client=client,
                note_ids=cluster_note_ids,
                include_types=include_types
            )

            if not entity_info:
                continue  # ì—”í‹°í‹°ê°€ ì—†ëŠ” í´ëŸ¬ìŠ¤í„°ëŠ” ìŠ¤í‚µ

            # ì¤‘ì‹¬ì„± ì ìˆ˜ë¡œ ì •ë ¬ëœ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
            sorted_entities = sorted(
                entity_info.items(),
                key=lambda x: x[1].get("centrality_score", 0),
                reverse=True
            )

            cluster_entity_ids = [eid for eid, _ in sorted_entities]
            cluster_entity_names = [info["name"] for _, info in sorted_entities]
            cluster_types = [info["type"] for _, info in sorted_entities]

            # í´ëŸ¬ìŠ¤í„° ë‚´ íƒ€ì… ë¶„í¬
            type_counts = {}
            for t in cluster_types:
                type_counts[t.lower()] = type_counts.get(t.lower(), 0) + 1

            # í—ˆë¸Œ ì—”í‹°í‹° ì°¾ê¸° (ê·¸ë˜í”„ì—ì„œ ì¤‘ì‹¬ ì—­í• )
            hub_entities = find_cluster_hub_entities(entity_info, top_k=3)

            # í´ëŸ¬ìŠ¤í„° ì´ë¦„: ê·¸ë˜í”„ í—ˆë¸Œ ì—”í‹°í‹° ê¸°ë°˜ (ë…¸íŠ¸ ì œëª© ì•„ë‹˜!)
            cluster_name = generate_cluster_name_from_graph(hub_entities, type_counts)

            # ì¤‘ìš”ë„ ì ìˆ˜: ì¤‘ì‹¬ì„± ê¸°ë°˜ + recency ë³´ë„ˆìŠ¤
            avg_centrality = sum(info.get("centrality_score", 0) for info in entity_info.values()) / max(len(entity_info), 1)
            recency_bonus, recent_updates = _compute_recency_bonus(cluster_note_updates)
            importance_score = min(10.0, (avg_centrality * 8.0) + recency_bonus)

            # í—ˆë¸Œ ì—”í‹°í‹° ì •ë³´ ì¶”ê°€
            hub_info = [
                {"id": eid, "name": name, "centrality": round(score, 3)}
                for eid, name, score in hub_entities
            ]

            clusters.append({
                "id": f"cluster_{cluster_id + 1}",
                "name": cluster_name,
                "level": 1,
                "node_count": len(cluster_entity_ids),
                "entity_ids": cluster_entity_ids,
                "sample_entities": cluster_entity_names[:10],  # ì¤‘ì‹¬ì„± ìˆœ ìƒìœ„ 10ê°œ
                "sample_notes": cluster_note_titles[:5],
                "note_ids": cluster_note_ids[:20],
                "recent_updates": recent_updates,
                "summary": f"{cluster_name} í´ëŸ¬ìŠ¤í„° ({len(cluster_entity_ids)} ì—”í‹°í‹°)",
                "key_insights": _build_graph_insights(hub_entities, recent_updates, type_counts, len(cluster_note_ids)),
                "contains_types": type_counts,
                "importance_score": importance_score,
                "hub_entities": hub_info,  # ìƒˆë¡œìš´ í•„ë“œ: í—ˆë¸Œ ì—”í‹°í‹° ì •ë³´
                "last_updated": datetime.utcnow().isoformat(),
                "last_computed": datetime.utcnow().isoformat(),
                "clustering_method": "umap_hdbscan_graph_centrality",
                "is_manual": False
            })

        logger.info(f"Successfully created {len(clusters)} semantic clusters")

        # ì´ ì—”í‹°í‹° ìˆ˜ ê³„ì‚°
        total_entities = sum(c["node_count"] for c in clusters)

        if len(clusters) == 0:
            logger.warning("Semantic clustering produced empty clusters. Falling back to type-based clustering.")
            return _semantic_fallback_to_type_based(
                client,
                vault_id,
                target_clusters,
                include_types,
                reason="empty_result",
                folder_prefix=folder_prefix
            )

        edges = _build_cluster_edges(clusters)

        return {
            "clusters": clusters,
            "edges": edges,
            "total_nodes": total_entities,
            "method": "umap_hdbscan",
            "computed_at": datetime.utcnow().isoformat()
        }

    except Exception as e:
        logger.error(f"Semantic clustering failed: {e}")
        return _semantic_fallback_to_type_based(
            client,
            vault_id,
            target_clusters,
            include_types,
            reason="exception",
            folder_prefix=folder_prefix
        )


def _semantic_fallback_to_type_based(
    client,
    vault_id: str,
    target_clusters: int,
    include_types: List[str],
    reason: str,
    folder_prefix: str = None
) -> Dict[str, Any]:
    """
    ì˜ë¯¸ë¡ ì  í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨ ì‹œ íƒ€ì… ê¸°ë°˜ìœ¼ë¡œ í´ë°±
    """
    logger.warning(f"Semantic clustering fallback triggered: {reason}")
    fallback = compute_clusters_louvain(
        client=client,
        vault_id=vault_id,
        target_clusters=target_clusters,
        include_types=include_types,
        folder_prefix=folder_prefix
    )
    fallback["method"] = f"umap_hdbscan_fallback:{reason}"
    return fallback


def _parse_datetime(value: Optional[str]) -> Optional[datetime]:
    """ISO ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ íŒŒì‹± (ì—†ìœ¼ë©´ None)"""
    if not value:
        return None
    try:
        if isinstance(value, datetime):
            return value
        if hasattr(value, "isoformat"):
            return datetime.fromisoformat(value.isoformat())
        # Neo4jì˜ toString(datetime) ê²°ê³¼ëŠ” Zë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŒ
        value_norm = value.replace("Z", "+00:00")
        return datetime.fromisoformat(value_norm)
    except Exception:
        return None


def _compute_recency_bonus(updated_at_list: List[Optional[datetime]]) -> Tuple[float, int]:
    """ìµœê·¼ 7ì¼ ì—…ë°ì´íŠ¸ ìˆ˜ì™€ ë³´ë„ˆìŠ¤ ì ìˆ˜ ê³„ì‚°"""
    from datetime import timezone
    now = datetime.now(timezone.utc)
    recent_threshold = now - timedelta(days=7)
    recent_updates = 0

    # timezone-awareë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
    valid_dates = []
    for dt in updated_at_list:
        if dt is None:
            continue
        # timezone-naiveì¸ ê²½ìš° UTCë¡œ ê°€ì •
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        valid_dates.append(dt)

    for dt in valid_dates:
        if dt >= recent_threshold:
            recent_updates += 1

    if not valid_dates:
        return 0.0, 0

    latest = max(valid_dates)
    days_since_latest = max((now - latest).days, 0)
    recency_bonus = max(0.0, 3.0 - (days_since_latest / 10.0))  # ìµœëŒ€ 3ì 
    return recency_bonus, recent_updates


def _build_auto_insights(
    cluster_name: str,
    total_mentions: int,
    recent_updates: int,
    type_counts: Dict[str, int]
) -> List[str]:
    """ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸"""
    insights = []
    insights.append(f"{total_mentions} mentions across entities.")
    if recent_updates > 0:
        insights.append(f"{recent_updates} notes updated in the last 7 days.")
    else:
        insights.append("No recent updates in the last 7 days.")

    dominant_type = None
    if type_counts:
        dominant_type = max(type_counts.items(), key=lambda kv: kv[1])[0]
        insights.append(f"Dominant type: {dominant_type} ({type_counts[dominant_type]})")

    if dominant_type == "task":
        insights.append("Next action: review and reprioritize tasks in this cluster.")
    elif dominant_type == "project":
        insights.append("Next action: identify active project milestones.")
    else:
        insights.append("Next action: link related notes to strengthen context.")

    return insights[:4]


def _build_graph_insights(
    hub_entities: List[Tuple[str, str, float]],
    recent_updates: int,
    type_counts: Dict[str, int],
    note_count: int
) -> List[str]:
    """
    ê·¸ë˜í”„ ì¤‘ì‹¬ì„± ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±

    í—ˆë¸Œ ì—”í‹°í‹°ì˜ ì—­í• ê³¼ í´ëŸ¬ìŠ¤í„° êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ ì œê³µ
    """
    insights = []

    # í—ˆë¸Œ ì—”í‹°í‹° ì •ë³´
    if hub_entities:
        top_hub = hub_entities[0]
        hub_name, hub_score = top_hub[1], top_hub[2]
        if hub_score > 0.7:
            insights.append(f"'{hub_name}'ì´(ê°€) í´ëŸ¬ìŠ¤í„°ì˜ í•µì‹¬ í—ˆë¸Œ ì—­í•  (ì¤‘ì‹¬ì„±: {hub_score:.2f})")
        elif hub_score > 0.4:
            insights.append(f"'{hub_name}'ì´(ê°€) ì£¼ìš” ì—°ê²°ì  (ì¤‘ì‹¬ì„±: {hub_score:.2f})")
        else:
            insights.append(f"'{hub_name}'ì´(ê°€) í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ ê°œë…")

        if len(hub_entities) >= 2:
            second_hub = hub_entities[1][1]
            insights.append(f"ê´€ë ¨ í•µì‹¬ ê°œë…: {second_hub}")

    # ë…¸íŠ¸ ì—°ê²°ì„± ë¶„ì„
    if note_count > 10:
        insights.append(f"{note_count}ê°œ ë…¸íŠ¸ê°€ ì´ ì£¼ì œë¡œ ì—°ê²°ë¨ - í™œë°œí•œ ì˜ì—­")
    elif note_count > 5:
        insights.append(f"{note_count}ê°œ ë…¸íŠ¸ ì—°ê²° - ì„±ì¥ ì¤‘ì¸ ì˜ì—­")
    else:
        insights.append(f"{note_count}ê°œ ë…¸íŠ¸ë¡œ ì‹œì‘í•˜ëŠ” ì˜ì—­")

    # ìµœê·¼ í™œë™
    if recent_updates > 3:
        insights.append(f"ìµœê·¼ 7ì¼ê°„ {recent_updates}íšŒ ì—…ë°ì´íŠ¸ - í˜„ì¬ í™œë°œíˆ ì‘ì—… ì¤‘")
    elif recent_updates > 0:
        insights.append(f"ìµœê·¼ {recent_updates}íšŒ ì—…ë°ì´íŠ¸")

    # íƒ€ì… ë¶„í¬ ê¸°ë°˜ ì œì•ˆ
    if type_counts:
        dominant_type = max(type_counts.items(), key=lambda kv: kv[1])[0]
        if dominant_type == "task":
            insights.append("ğŸ¯ ì•¡ì…˜: ì—°ê²°ëœ íƒœìŠ¤í¬ë“¤ ìš°ì„ ìˆœìœ„ ê²€í† ")
        elif dominant_type == "project":
            insights.append("ğŸ“‹ ì•¡ì…˜: í”„ë¡œì íŠ¸ ë§ˆì¼ìŠ¤í†¤ ì ê²€")
        elif dominant_type == "topic":
            insights.append("ğŸ’¡ ì•¡ì…˜: ê´€ë ¨ ë…¸íŠ¸ë“¤ì„ ì—°ê²°í•´ ì§€ì‹ ë„¤íŠ¸ì›Œí¬ ê°•í™”")

    return insights[:5]


def _build_cluster_edges(clusters: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    í´ëŸ¬ìŠ¤í„° ê°„ ê³µìœ  ì—”í‹°í‹° ê¸°ë°˜ ê´€ê³„ ìƒì„±
    """
    edges: List[Dict[str, Any]] = []
    if len(clusters) < 2:
        return edges

    for i in range(len(clusters)):
        for j in range(i + 1, len(clusters)):
            entities_i = set(clusters[i].get("entity_ids") or [])
            entities_j = set(clusters[j].get("entity_ids") or [])
            if not entities_i or not entities_j:
                continue

            shared = entities_i.intersection(entities_j)
            if len(shared) == 0:
                continue

            weight = float(len(shared))
            edges.append({
                "from": clusters[i]["id"],
                "to": clusters[j]["id"],
                "relation_type": "RELATED_TO",
                "weight": weight
            })

    return edges


def get_cached_clusters(client, vault_id: str) -> Optional[Dict[str, Any]]:
    """
    ìºì‹œëœ í´ëŸ¬ìŠ¤í„° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

    Args:
        client: Neo4j Bolt í´ë¼ì´ì–¸íŠ¸
        vault_id: Vault ID

    Returns:
        ìºì‹œëœ í´ëŸ¬ìŠ¤í„° ë°ì´í„° ë˜ëŠ” None
    """
    try:
        cypher = """
        MATCH (v:Vault {id: $vault_id})-[:HAS_CLUSTER_CACHE]->(cache:ClusterCache)
        WHERE datetime(cache.expires_at) > datetime()
        RETURN cache.data as data,
               cache.computed_at as computed_at,
               cache.method as method
        ORDER BY cache.computed_at DESC
        LIMIT 1
        """

        result = client.query(cypher, {"vault_id": vault_id})

        if result and len(result) > 0:
            cache_data = result[0]
            raw_data = json.loads(cache_data["data"])
            computed_at = cache_data["computed_at"]
            if not isinstance(computed_at, str):
                computed_at = str(computed_at)

            if isinstance(raw_data, dict) and "clusters" in raw_data:
                clusters = raw_data.get("clusters", [])
                edges = raw_data.get("edges", [])
            else:
                clusters = raw_data
                edges = []

            return {
                "clusters": clusters,
                "edges": edges,
                "computed_at": computed_at,
                "method": cache_data["method"],
                "from_cache": True
            }

        return None

    except Exception as e:
        logger.error(f"Failed to get cached clusters: {e}")
        return None


def save_cluster_cache(
    client,
    vault_id: str,
    clusters: List[Dict],
    method: str,
    ttl_hours: int = 12,
    edges: Optional[List[Dict]] = None
) -> bool:
    """
    í´ëŸ¬ìŠ¤í„° ë°ì´í„° ìºì‹œ ì €ì¥

    Args:
        client: Neo4j Bolt í´ë¼ì´ì–¸íŠ¸
        vault_id: Vault ID
        clusters: í´ëŸ¬ìŠ¤í„° ë°ì´í„°
        method: í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•
        ttl_hours: ìºì‹œ ìœ íš¨ ì‹œê°„ (ì‹œê°„)

    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    try:
        cypher = """
        MATCH (v:Vault {id: $vault_id})
        MERGE (v)-[:HAS_CLUSTER_CACHE]->(cache:ClusterCache)
        SET cache.data = $data,
            cache.method = $method,
            cache.computed_at = datetime(),
            cache.expires_at = datetime() + duration({hours: $ttl_hours})
        RETURN cache
        """

        payload = {
            "clusters": clusters,
            "edges": edges or []
        }

        params = {
            "vault_id": vault_id,
            "data": json.dumps(payload),
            "method": method,
            "ttl_hours": ttl_hours
        }

        result = client.query(cypher, params)
        return result is not None and len(result) > 0

    except Exception as e:
        logger.error(f"Failed to save cluster cache: {e}")
        return False


def invalidate_cluster_cache(client, vault_id: str) -> bool:
    """
    í´ëŸ¬ìŠ¤í„° ìºì‹œ ë¬´íš¨í™”

    Args:
        client: Neo4j Bolt í´ë¼ì´ì–¸íŠ¸
        vault_id: Vault ID

    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    try:
        cypher = """
        MATCH (v:Vault {id: $vault_id})-[:HAS_CLUSTER_CACHE]->(cache:ClusterCache)
        DETACH DELETE cache
        """

        client.query(cypher, {"vault_id": vault_id})
        return True

    except Exception as e:
        logger.error(f"Failed to invalidate cluster cache: {e}")
        return False


def is_cluster_cache_stale(client, vault_id: str, computed_at: Optional[str]) -> bool:
    """
    ìºì‹œê°€ ìµœì‹  ë…¸íŠ¸ ì—…ë°ì´íŠ¸ë³´ë‹¤ ì˜¤ë˜ë˜ì—ˆëŠ”ì§€ íŒë‹¨
    """
    if not computed_at:
        return True

    try:
        computed_dt = _parse_datetime(computed_at)
        if not computed_dt:
            return True

        cypher = """
        MATCH (v:Vault {id: $vault_id})-[:HAS_NOTE]->(n:Note)
        WITH max(n.updated_at) AS last_updated
        RETURN last_updated
        """
        result = client.query(cypher, {"vault_id": vault_id})
        if not result or len(result) == 0 or result[0].get("last_updated") is None:
            return False

        last_updated = result[0]["last_updated"]
        if isinstance(last_updated, str):
            last_updated = _parse_datetime(last_updated)

        if isinstance(last_updated, datetime):
            return last_updated > computed_dt

        return False
    except Exception as e:
        logger.error(f"Failed to check cache staleness: {e}")
        return True


def generate_llm_summaries(
    client,
    vault_id: str,
    clusters: List[Dict]
) -> List[Dict]:
    """
    GPT-5 Minië¥¼ ì‚¬ìš©í•˜ì—¬ ê° í´ëŸ¬ìŠ¤í„°ì˜ ìš”ì•½ ìƒì„± (Phase 11)

    Args:
        client: Neo4j Bolt í´ë¼ì´ì–¸íŠ¸
        vault_id: Vault ID
        clusters: í´ëŸ¬ìŠ¤í„° ë¦¬ìŠ¤íŠ¸

    Returns:
        ìš”ì•½ì´ ì¶”ê°€ëœ í´ëŸ¬ìŠ¤í„° ë¦¬ìŠ¤íŠ¸
    """
    try:
        from app.services.llm_client import generate_batch_cluster_summaries

        logger.info(f"ğŸ¤– Generating LLM summaries for {len(clusters)} clusters...")

        # ê° í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´ ìƒ˜í”Œ ì—”í‹°í‹° ê°€ì ¸ì˜¤ê¸°
        for cluster in clusters:
            try:
                # sample_entitiesê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ê°„ë‹¨í•œ placeholder ìƒì„±
                if not cluster.get('sample_entities'):
                    sample_entities = []
                    contains_types = cluster.get('contains_types', {})
                    for entity_type, count in contains_types.items():
                        sample_entities.append(f"{entity_type.title()} ({count}ê°œ)")
                    cluster['sample_entities'] = sample_entities
                if not cluster.get('sample_notes'):
                    cluster['sample_notes'] = []
                if not cluster.get('note_ids'):
                    cluster['note_ids'] = []

            except Exception as e:
                logger.error(f"Failed to fetch sample entities for {cluster.get('id')}: {e}")
                cluster['sample_entities'] = []
                cluster['sample_notes'] = []

        # GPT-5 Minië¡œ ì¼ê´„ ìš”ì•½ ìƒì„±
        clusters_with_summaries = generate_batch_cluster_summaries(clusters)

        logger.info(f"âœ… LLM summaries generated for {len(clusters_with_summaries)} clusters")
        return clusters_with_summaries

    except Exception as e:
        logger.error(f"Failed to generate LLM summaries: {e}")
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë©”ì‹œì§€ ì¶”ê°€
        for cluster in clusters:
            if 'summary' not in cluster:
                cluster['summary'] = f"í´ëŸ¬ìŠ¤í„° '{cluster.get('name')}'ëŠ” {cluster.get('node_count')}ê°œì˜ ë…¸ë“œë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
            if 'key_insights' not in cluster:
                cluster['key_insights'] = ["LLM ìš”ì•½ ìƒì„± ì‹¤íŒ¨", "ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."]
        return clusters
