"""
Entity Cluster Service - Hybrid Graph + Embedding Clustering

Entity ë…¸ë“œë“¤ì„ RELATES_TO ê·¸ë˜í”„ êµ¬ì¡° + name_embedding ë²¡í„° ìœ ì‚¬ë„ë¡œ
í•˜ì´ë¸Œë¦¬ë“œ í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ 2nd Brain ì‹œê°í™” ì§€ì›.

Flow:
1. Entity ë…¸ë“œì™€ name_embedding ê°€ì ¸ì˜¤ê¸°
2. RELATES_TO ê´€ê³„ë¡œ ê·¸ë˜í”„ ì»¤ë®¤ë‹ˆí‹° íƒì§€ (Louvain)
3. name_embedding ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ì‹œë©˜í‹± í´ëŸ¬ìŠ¤í„°ë§
4. ë‘ ê²°ê³¼ë¥¼ ì¡°í•©í•˜ì—¬ ìµœì¢… í´ëŸ¬ìŠ¤í„° ê²°ì •
"""

import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
import numpy as np
from collections import defaultdict

logger = logging.getLogger(__name__)

# UMAP + HDBSCAN imports (lazy)
try:
    import hdbscan
    HDBSCAN_AVAILABLE = True
except ImportError:
    HDBSCAN_AVAILABLE = False
    logger.warning("HDBSCAN not available. Embedding clustering will use fallback.")

# NetworkX for Louvain community detection
try:
    import networkx as nx
    from networkx.algorithms.community import louvain_communities
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False
    logger.warning("NetworkX not available. Graph clustering will use fallback.")


def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    """ë‘ ë²¡í„°ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    if norm_a == 0 or norm_b == 0:
        return 0.0
    return float(np.dot(a, b) / (norm_a * norm_b))


def get_entities_with_embeddings(
    client,
    limit: int = 1000,
    folder_prefix: str = None,
    min_connections: int = 1
) -> List[Dict[str, Any]]:
    """
    Entity ë…¸ë“œë“¤ ê°€ì ¸ì˜¤ê¸° (embedding ì˜µì…˜)

    ë‘ ê°€ì§€ ê²½ë¡œ ì§€ì›:
    1. Note -[:MENTIONS]-> Entity (ì§ì ‘ ê´€ê³„)
    2. Episodic -[:MENTIONS]-> Entity (Graphiti ê´€ê³„, Episodic.name = 'note_' + note_id)

    Args:
        client: Neo4j í´ë¼ì´ì–¸íŠ¸
        limit: ìµœëŒ€ ì—”í‹°í‹° ìˆ˜
        folder_prefix: í´ë” ê²½ë¡œ í•„í„° (ì˜ˆ: '1_í”„ë¡œì íŠ¸/'). í•´ë‹¹ í´ë”ì˜ ë…¸íŠ¸ê°€ MENTIONSí•˜ëŠ” ì—”í‹°í‹°ë§Œ ë°˜í™˜
        min_connections: ìµœì†Œ ì—°ê²° ë…¸íŠ¸ ìˆ˜ (ê¸°ë³¸ 1). 2ë¡œ ì„¤ì •í•˜ë©´ 2ê°œ ì´ìƒ ë…¸íŠ¸ì—ì„œ ì–¸ê¸‰ëœ ì—”í‹°í‹°ë§Œ ë°˜í™˜

    Returns:
        [{uuid, name, summary, pkm_type, name_embedding, mention_count}, ...]
    """
    folder_condition = "n.note_id STARTS WITH $folder_prefix AND" if folder_prefix else ""
    folder_condition2 = "n2.note_id STARTS WITH $folder_prefix AND" if folder_prefix else ""

    # ë‘ ê²½ë¡œë¥¼ UNIONìœ¼ë¡œ í•©ì¹¨
    cypher = f"""
    // ë°©ë²•1: ì§ì ‘ MENTIONS ê´€ê³„ (Note -> Entity)
    MATCH (n:Note)-[:MENTIONS]->(e:Entity)
    WHERE {folder_condition} e.name IS NOT NULL
    WITH e, collect(DISTINCT n.note_id) as note_ids
    RETURN e.uuid as uuid,
           e.name as name,
           e.summary as summary,
           e.pkm_type as pkm_type,
           e.name_embedding as embedding,
           note_ids

    UNION

    // ë°©ë²•2: Episodic í†µí•œ ì—°ê²° (Graphiti)
    MATCH (ep:Episodic)-[:MENTIONS]->(e:Entity)
    WHERE e.name IS NOT NULL AND ep.name IS NOT NULL AND ep.name STARTS WITH 'note_'
    WITH e, ep, replace(ep.name, 'note_', '') as derived_note_id
    MATCH (n2:Note)
    WHERE {folder_condition2} n2.note_id = derived_note_id
    WITH e, collect(DISTINCT n2.note_id) as note_ids
    WHERE size(note_ids) > 0
    RETURN e.uuid as uuid,
           e.name as name,
           e.summary as summary,
           e.pkm_type as pkm_type,
           e.name_embedding as embedding,
           note_ids
    """

    results = client.query(cypher, {
        "folder_prefix": folder_prefix or "",
        "limit": limit,
        "min_connections": min_connections
    })

    # UNION ê²°ê³¼ ë³‘í•© (ê°™ì€ entityì˜ note_ids í•©ì¹˜ê¸°)
    entity_map = {}
    for row in results or []:
        uuid = row["uuid"]
        if uuid not in entity_map:
            entity_map[uuid] = {
                "uuid": uuid,
                "name": row["name"] or uuid,
                "summary": row.get("summary", ""),
                "pkm_type": row.get("pkm_type", "Topic"),
                "embedding": row.get("embedding"),
                "note_ids": set(row.get("note_ids", []) or [])
            }
        else:
            entity_map[uuid]["note_ids"].update(row.get("note_ids", []) or [])

    # min_connections í•„í„° ë° ì •ë ¬
    filtered = [
        {**data, "mention_count": len(data["note_ids"])}
        for data in entity_map.values()
        if len(data["note_ids"]) >= min_connections
    ]
    filtered.sort(key=lambda x: x["mention_count"], reverse=True)

    # note_idsëŠ” setì´ë¯€ë¡œ listë¡œ ë³€í™˜í•˜ì§€ ì•Šê³  ì œê±° (í•„ìš” ì—†ìŒ)
    entities = []
    for item in filtered[:limit]:
        entities.append({
            "uuid": item["uuid"],
            "name": item["name"],
            "summary": item["summary"],
            "pkm_type": item["pkm_type"],
            "embedding": item["embedding"],
            "mention_count": item["mention_count"]
        })

    return entities


def get_relates_to_edges(client, entity_uuids: List[str]) -> List[Tuple[str, str, float]]:
    """
    Entity ê°„ RELATES_TO ê´€ê³„ ê°€ì ¸ì˜¤ê¸°

    Returns:
        [(from_uuid, to_uuid, weight), ...]
    """
    cypher = """
    MATCH (e1:Entity)-[r:RELATES_TO]->(e2:Entity)
    WHERE e1.uuid IN $uuids AND e2.uuid IN $uuids
    RETURN e1.uuid as from_uuid, e2.uuid as to_uuid,
           COALESCE(r.weight, 1.0) as weight
    """

    results = client.query(cypher, {"uuids": entity_uuids})

    edges = []
    for row in results or []:
        edges.append((row["from_uuid"], row["to_uuid"], row.get("weight", 1.0)))

    return edges


def cluster_by_graph_louvain(
    entity_uuids: List[str],
    edges: List[Tuple[str, str, float]],
    resolution: float = 1.0
) -> Dict[str, int]:
    """
    RELATES_TO ê·¸ë˜í”„ ê¸°ë°˜ Louvain ì»¤ë®¤ë‹ˆí‹° íƒì§€

    Args:
        entity_uuids: ì—”í‹°í‹° UUID ë¦¬ìŠ¤íŠ¸
        edges: (from, to, weight) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        resolution: Louvain í•´ìƒë„ íŒŒë¼ë¯¸í„° (ë†’ì„ìˆ˜ë¡ ë” ë§ì€ í´ëŸ¬ìŠ¤í„°)

    Returns:
        {entity_uuid: cluster_id}
    """
    if not NETWORKX_AVAILABLE:
        # Fallback: ëª¨ë“  ì—”í‹°í‹°ë¥¼ í•˜ë‚˜ì˜ í´ëŸ¬ìŠ¤í„°ë¡œ
        return {uuid: 0 for uuid in entity_uuids}

    if not edges:
        # ì—£ì§€ê°€ ì—†ìœ¼ë©´ ê° ì—”í‹°í‹°ê°€ ë…ë¦½ í´ëŸ¬ìŠ¤í„°
        return {uuid: i for i, uuid in enumerate(entity_uuids)}

    # NetworkX ê·¸ë˜í”„ ìƒì„±
    G = nx.Graph()
    G.add_nodes_from(entity_uuids)

    for from_uuid, to_uuid, weight in edges:
        G.add_edge(from_uuid, to_uuid, weight=weight)

    # Louvain ì»¤ë®¤ë‹ˆí‹° íƒì§€
    try:
        communities = louvain_communities(G, weight='weight', resolution=resolution, seed=42)

        # ê²°ê³¼ë¥¼ dictë¡œ ë³€í™˜
        uuid_to_cluster = {}
        for cluster_id, community in enumerate(communities):
            for uuid in community:
                uuid_to_cluster[uuid] = cluster_id

        # ì—°ê²°ë˜ì§€ ì•Šì€ ë…¸ë“œë“¤ì€ ìƒˆ í´ëŸ¬ìŠ¤í„°ë¡œ
        next_cluster = len(communities)
        for uuid in entity_uuids:
            if uuid not in uuid_to_cluster:
                uuid_to_cluster[uuid] = next_cluster
                next_cluster += 1

        return uuid_to_cluster

    except Exception as e:
        logger.error(f"Louvain clustering failed: {e}")
        return {uuid: i for i, uuid in enumerate(entity_uuids)}


def cluster_by_pkm_type(
    entities: List[Dict[str, Any]]
) -> Dict[str, int]:
    """
    PKM Core 8 Type ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§

    ìƒì‚°ì„± ê·¹ëŒ€í™”ë¥¼ ìœ„í•´ ëª…í™•í•œ 8ê°œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜:
    - Goal: ì¥ê¸° ëª©í‘œ
    - Project: ì§„í–‰ ì¤‘ì¸ í”„ë¡œì íŠ¸
    - Task: ì‹¤í–‰ ê°€ëŠ¥í•œ í• ì¼
    - Topic: ì£¼ì œ/ë¶„ì•¼
    - Concept: ê°œë…/ì•„ì´ë””ì–´
    - Question: íƒêµ¬í•  ì§ˆë¬¸
    - Insight: í†µì°°/ë°œê²¬
    - Resource: ì°¸ê³  ìë£Œ

    Note: Personì€ Topic(3)ìœ¼ë¡œ ë§¤í•‘ë¨ (í•˜ìœ„ í˜¸í™˜ì„±)

    Args:
        entities: ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸

    Returns:
        {entity_uuid: cluster_id}
    """
    # PKM Core 8 Types (Personì€ Topicìœ¼ë¡œ ë§¤í•‘)
    type_to_cluster = {
        "Goal": 0,
        "Project": 1,
        "Task": 2,
        "Topic": 3,
        "Concept": 4,
        "Question": 5,
        "Insight": 6,
        "Resource": 7,
        "Person": 3  # Personì€ Topicìœ¼ë¡œ ë¶„ë¥˜
    }

    return {
        e["uuid"]: type_to_cluster.get(e.get("pkm_type", "Topic"), 3)
        for e in entities
    }


def cluster_by_embedding_hdbscan(
    entities: List[Dict[str, Any]],
    min_cluster_size: int = 5,
    min_samples: int = 2
) -> Dict[str, int]:
    """
    name_embedding ê¸°ë°˜ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ (ë ˆê±°ì‹œ - pkm_type ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´ë¨)

    Args:
        entities: embeddingì´ í¬í•¨ëœ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
        min_cluster_size: ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°
        min_samples: ìµœì†Œ ìƒ˜í”Œ ìˆ˜

    Returns:
        {entity_uuid: cluster_id} (ë…¸ì´ì¦ˆëŠ” -1)
    """
    if not HDBSCAN_AVAILABLE:
        # Fallback: pkm_typeìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ (8 Core Types + Person)
        type_to_cluster = {
            "Goal": 0, "Project": 1, "Task": 2, "Topic": 3,
            "Concept": 4, "Question": 5, "Insight": 6, "Resource": 7,
            "Person": 8
        }
        return {
            e["uuid"]: type_to_cluster.get(e.get("pkm_type", "Topic"), 3)
            for e in entities
        }

    # ì„ë² ë”© ì¶”ì¶œ
    valid_entities = [e for e in entities if e.get("embedding")]
    if len(valid_entities) < min_cluster_size:
        # embedding ì—†ìœ¼ë©´ pkm_type fallback ì‚¬ìš©
        type_to_cluster = {
            "Goal": 0, "Project": 1, "Task": 2, "Topic": 3,
            "Concept": 4, "Question": 5, "Insight": 6, "Resource": 7,
            "Person": 8
        }
        return {
            e["uuid"]: type_to_cluster.get(e.get("pkm_type", "Topic"), 3)
            for e in entities
        }

    embeddings = np.array([e["embedding"] for e in valid_entities], dtype=float)

    # íŒŒë¼ë¯¸í„° ì¡°ì •
    actual_min_cluster_size = max(3, min(min_cluster_size, len(valid_entities) // 10))

    try:
        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=actual_min_cluster_size,
            min_samples=min_samples,
            metric='euclidean',
            cluster_selection_method='eom'
        )
        labels = clusterer.fit_predict(embeddings)

        result = {}
        for i, entity in enumerate(valid_entities):
            result[entity["uuid"]] = int(labels[i])

        # ì„ë² ë”© ì—†ëŠ” ì—”í‹°í‹°ëŠ” ë…¸ì´ì¦ˆë¡œ í‘œì‹œ
        for e in entities:
            if e["uuid"] not in result:
                result[e["uuid"]] = -1

        return result

    except Exception as e:
        logger.error(f"HDBSCAN clustering failed: {e}")
        return {e["uuid"]: 0 for e in entities}


def merge_cluster_assignments(
    graph_clusters: Dict[str, int],
    embedding_clusters: Dict[str, int],
    graph_weight: float = 0.4,
    embedding_weight: float = 0.6
) -> Dict[str, int]:
    """
    ê·¸ë˜í”„ í´ëŸ¬ìŠ¤í„°ì™€ ì„ë² ë”© í´ëŸ¬ìŠ¤í„°ë¥¼ ë³‘í•©

    ì „ëµ:
    1. ì„ë² ë”© í´ëŸ¬ìŠ¤í„°ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš© (ì‹œë©˜í‹± ìœ ì‚¬ë„)
    2. ê°™ì€ ê·¸ë˜í”„ í´ëŸ¬ìŠ¤í„°ì— ìˆìœ¼ë©´ì„œ ì„ë² ë”© í´ëŸ¬ìŠ¤í„°ê°€ ë‹¤ë¥¸ ê²½ìš° ë³‘í•© ê³ ë ¤
    3. ë…¸ì´ì¦ˆ(-1)ëŠ” ê·¸ë˜í”„ í´ëŸ¬ìŠ¤í„° ì‚¬ìš©

    Returns:
        {entity_uuid: final_cluster_id}
    """
    all_uuids = set(graph_clusters.keys()) | set(embedding_clusters.keys())

    # ì„ë² ë”© í´ëŸ¬ìŠ¤í„°ë¥¼ ê¸°ë³¸ìœ¼ë¡œ
    final_clusters = {}

    # ë¨¼ì € ì„ë² ë”© í´ëŸ¬ìŠ¤í„° í• ë‹¹
    for uuid in all_uuids:
        emb_cluster = embedding_clusters.get(uuid, -1)
        graph_cluster = graph_clusters.get(uuid, -1)

        if emb_cluster >= 0:
            # ì„ë² ë”© í´ëŸ¬ìŠ¤í„°ê°€ ìœ íš¨í•˜ë©´ ê·¸ê²ƒ ì‚¬ìš©
            final_clusters[uuid] = emb_cluster
        elif graph_cluster >= 0:
            # ì„ë² ë”©ì´ ë…¸ì´ì¦ˆë©´ ê·¸ë˜í”„ í´ëŸ¬ìŠ¤í„° ì‚¬ìš© (ì˜¤í”„ì…‹ ì ìš©)
            max_emb = max(embedding_clusters.values()) if embedding_clusters else 0
            final_clusters[uuid] = max_emb + 1 + graph_cluster
        else:
            # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ë…ë¦½ í´ëŸ¬ìŠ¤í„°
            final_clusters[uuid] = -1

    # ë…¸ì´ì¦ˆ(-1) ì—”í‹°í‹°ë“¤ì„ ê°€ì¥ ê°€ê¹Œìš´ í´ëŸ¬ìŠ¤í„°ì— ì¬í• ë‹¹
    # (ê·¸ë˜í”„ ì—°ê²°ì´ ìˆëŠ” í´ëŸ¬ìŠ¤í„°ë¡œ)
    noise_uuids = [u for u, c in final_clusters.items() if c == -1]

    for uuid in noise_uuids:
        graph_cluster = graph_clusters.get(uuid, -1)
        if graph_cluster >= 0:
            # ê°™ì€ ê·¸ë˜í”„ í´ëŸ¬ìŠ¤í„°ì— ìˆëŠ” ë‹¤ë¥¸ ì—”í‹°í‹°ë“¤ì˜ í´ëŸ¬ìŠ¤í„° í™•ì¸
            same_graph = [u for u, g in graph_clusters.items() if g == graph_cluster and u != uuid]
            if same_graph:
                # ê°€ì¥ ë§ì´ ë“±ì¥í•˜ëŠ” í´ëŸ¬ìŠ¤í„°ë¡œ í• ë‹¹
                cluster_counts = defaultdict(int)
                for u in same_graph:
                    c = final_clusters.get(u, -1)
                    if c >= 0:
                        cluster_counts[c] += 1

                if cluster_counts:
                    final_clusters[uuid] = max(cluster_counts, key=cluster_counts.get)

    # ì—¬ì „íˆ -1ì¸ ê²ƒë“¤ì—ê²Œ ìƒˆ í´ëŸ¬ìŠ¤í„° ID ë¶€ì—¬
    max_cluster = max(c for c in final_clusters.values() if c >= 0) if any(c >= 0 for c in final_clusters.values()) else -1

    next_id = max_cluster + 1
    for uuid in final_clusters:
        if final_clusters[uuid] == -1:
            final_clusters[uuid] = next_id
            next_id += 1

    return final_clusters


def find_cluster_representative(
    entities: List[Dict[str, Any]],
    cluster_uuids: List[str]
) -> Tuple[str, str]:
    """
    í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ ì—”í‹°í‹° ì°¾ê¸° (ì´ë¦„ì´ ê°€ì¥ ì§§ê³  ëª…í™•í•œ ê²ƒ)

    Returns:
        (uuid, name)
    """
    cluster_entities = [e for e in entities if e["uuid"] in cluster_uuids]

    if not cluster_entities:
        return ("", "Unknown Cluster")

    # ìš”ì•½ì´ ìˆëŠ” ì—”í‹°í‹° ìš°ì„ , ê·¸ ë‹¤ìŒ ì´ë¦„ ê¸¸ì´ë¡œ ì •ë ¬
    sorted_entities = sorted(
        cluster_entities,
        key=lambda e: (
            0 if e.get("summary") else 1,  # ìš”ì•½ ìˆìœ¼ë©´ ìš°ì„ 
            len(e.get("name", "") or ""),   # ì´ë¦„ ê¸¸ì´
        )
    )

    top = sorted_entities[0]
    return (top["uuid"], top.get("name", top["uuid"]))


def compute_entity_clusters_hybrid(
    client,
    min_cluster_size: int = 3,
    resolution: float = 1.0,
    folder_prefix: str = None,
    min_connections: int = 1
) -> Dict[str, Any]:
    """
    Entity ë…¸ë“œë“¤ì„ PKM Core 8 Type ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§

    ìƒì‚°ì„± ê·¹ëŒ€í™”ë¥¼ ìœ„í•´ HDBSCAN ëŒ€ì‹  PKM 8 Type ì‚¬ìš©:
    - Goal, Project, Task: ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ íë¦„
    - Topic, Concept, Insight: ì§€ì‹ ê´€ë¦¬
    - Question: íƒêµ¬ ì˜ì—­
    - Resource: ì°¸ê³  ìë£Œ
    - Person: ì¸ë§¥ ê´€ë¦¬

    Args:
        client: Neo4j í´ë¼ì´ì–¸íŠ¸
        min_cluster_size: ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸° (pkm_typeì—ì„œëŠ” ë¯¸ì‚¬ìš©)
        resolution: Louvain í•´ìƒë„ (pkm_typeì—ì„œëŠ” ë¯¸ì‚¬ìš©)
        folder_prefix: í´ë” ê²½ë¡œ í•„í„° (ì˜ˆ: '1_í”„ë¡œì íŠ¸/'). í•´ë‹¹ í´ë”ì˜ ë…¸íŠ¸ê°€ MENTIONSí•˜ëŠ” ì—”í‹°í‹°ë§Œ í´ëŸ¬ìŠ¤í„°ë§
        min_connections: ìµœì†Œ ì—°ê²° ë…¸íŠ¸ ìˆ˜ (ê¸°ë³¸ 1). ë‹¨ì¼ ë…¸íŠ¸ ì—°ê²°ë„ í¬í•¨ (ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ì¤‘ìš”í•  ìˆ˜ ìˆìŒ)

    Returns:
        {
            "clusters": [...],
            "edges": [...],
            "total_entities": int,
            "method": str
        }
    """
    folder_info = f" for folder '{folder_prefix}'" if folder_prefix else ""
    logger.info(f"Starting PKM Type entity clustering{folder_info} (min_connections={min_connections})...")

    # PKM Core 8 Types ì •ì˜ (Personì€ ë³„ë„ ì²˜ë¦¬)
    PKM_TYPES = {
        0: {"id": "Goal", "name": "ğŸ¯ Goal", "description": "ì¥ê¸° ëª©í‘œ"},
        1: {"id": "Project", "name": "ğŸ“ Project", "description": "ì§„í–‰ ì¤‘ì¸ í”„ë¡œì íŠ¸"},
        2: {"id": "Task", "name": "âœ… Task", "description": "ì‹¤í–‰ ê°€ëŠ¥í•œ í• ì¼"},
        3: {"id": "Topic", "name": "ğŸ“š Topic", "description": "ì£¼ì œ/ë¶„ì•¼"},
        4: {"id": "Concept", "name": "ğŸ’¡ Concept", "description": "ê°œë…/ì•„ì´ë””ì–´"},
        5: {"id": "Question", "name": "â“ Question", "description": "íƒêµ¬í•  ì§ˆë¬¸"},
        6: {"id": "Insight", "name": "âœ¨ Insight", "description": "í†µì°°/ë°œê²¬"},
        7: {"id": "Resource", "name": "ğŸ“ Resource", "description": "ì°¸ê³  ìë£Œ"}
    }

    try:
        # Step 1: Entity ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (min_connections í•„í„° ì ìš©)
        entities = get_entities_with_embeddings(
            client,
            limit=1000,
            folder_prefix=folder_prefix,
            min_connections=min_connections
        )

        if not entities:
            logger.warning("No entities found for clustering")
            return {
                "clusters": [],
                "edges": [],
                "total_entities": 0,
                "method": "pkm_type",
                "computed_at": datetime.utcnow().isoformat()
            }

        logger.info(f"Found {len(entities)} entities")

        entity_uuids = [e["uuid"] for e in entities]
        uuid_to_entity = {e["uuid"]: e for e in entities}

        # Step 2: RELATES_TO ì—£ì§€ ê°€ì ¸ì˜¤ê¸° (í´ëŸ¬ìŠ¤í„° ê°„ ì—°ê²°ìš©)
        relates_to_edges = get_relates_to_edges(client, entity_uuids)
        logger.info(f"Found {len(relates_to_edges)} RELATES_TO edges")

        # Step 3: PKM Type ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ (HDBSCAN ëŒ€ì²´)
        pkm_clusters = cluster_by_pkm_type(entities)
        n_pkm_types = len(set(pkm_clusters.values()))
        logger.info(f"PKM Type clustering: {n_pkm_types} types found")

        # Step 4: í´ëŸ¬ìŠ¤í„° ë°ì´í„° êµ¬ì„±
        cluster_groups = defaultdict(list)
        for uuid, cluster_id in pkm_clusters.items():
            cluster_groups[cluster_id].append(uuid)

        # í´ëŸ¬ìŠ¤í„° ì •ë³´ ìƒì„± (PKM 8 Core Type ëª¨ë‘ í‘œì‹œ)
        clusters = []
        for cluster_id in range(8):  # 0-7: 8ê°œ Core Type (Person ì œì™¸)
            uuids = cluster_groups.get(cluster_id, [])

            # PKM Type ì •ë³´
            type_info = PKM_TYPES.get(cluster_id, {"id": "Topic", "name": "ğŸ“š Topic", "description": "ì£¼ì œ"})

            # í´ëŸ¬ìŠ¤í„° ë‚´ ì—”í‹°í‹°ë“¤
            cluster_entities = [uuid_to_entity[u] for u in uuids if u in uuid_to_entity]

            # mention_count ê¸°ì¤€ ì •ë ¬ (ìƒìœ„ ì—”í‹°í‹°ê°€ ëŒ€í‘œ)
            cluster_entities.sort(key=lambda e: e.get("mention_count", 0), reverse=True)

            # ìƒ˜í”Œ ì—”í‹°í‹° ì´ë¦„ (ìƒìœ„ 10ê°œ)
            sample_names = [e["name"] for e in cluster_entities[:10]]

            # ë‚´ë¶€ ì—°ê²° ìˆ˜ (RELATES_TO)
            uuid_set = set(uuids)
            internal_edges = sum(
                1 for f, t, _ in relates_to_edges
                if f in uuid_set and t in uuid_set
            )

            clusters.append({
                "id": f"cluster_{type_info['id'].lower()}",
                "name": type_info["name"],
                "pkm_type": type_info["id"],
                "description": type_info["description"],
                "entity_count": len(uuids),
                "entity_uuids": uuids,
                "sample_entities": sample_names,
                "type_distribution": {type_info["id"]: len(uuids)},
                "internal_edges": internal_edges,
                "cohesion_score": internal_edges / max(len(uuids), 1),
                "computed_at": datetime.utcnow().isoformat()
            })

        # ìƒì‚°ì„± íë¦„ ìˆœì„œë¡œ ì •ë ¬: Goal â†’ Project â†’ Task â†’ Topic â†’ Concept â†’ Question â†’ Insight â†’ Resource
        # (ì´ë¯¸ range(8) ìˆœì„œëŒ€ë¡œ ìƒì„±ë¨, ì •ë ¬ ë¶ˆí•„ìš”)

        # í´ëŸ¬ìŠ¤í„° ê°„ ì—£ì§€ ê³„ì‚° (ê³µìœ  RELATES_TO)
        cluster_edges = _compute_cluster_edges(clusters, relates_to_edges)

        return {
            "clusters": clusters,
            "edges": cluster_edges,
            "total_entities": len(entities),
            "clustered_entities": sum(c["entity_count"] for c in clusters),
            "method": "pkm_type",
            "pkm_types_found": n_pkm_types,
            "computed_at": datetime.utcnow().isoformat()
        }

    except Exception as e:
        logger.error(f"PKM Type entity clustering failed: {e}")
        raise


def _compute_cluster_edges(
    clusters: List[Dict[str, Any]],
    relates_to_edges: List[Tuple[str, str, float]]
) -> List[Dict[str, Any]]:
    """í´ëŸ¬ìŠ¤í„° ê°„ ì—°ê²° ê´€ê³„ ê³„ì‚°"""

    # UUID -> cluster_id ë§¤í•‘
    uuid_to_cluster = {}
    for cluster in clusters:
        for uuid in cluster.get("entity_uuids", []):
            uuid_to_cluster[uuid] = cluster["id"]

    # í´ëŸ¬ìŠ¤í„° ê°„ ì—£ì§€ ì¹´ìš´íŠ¸
    cluster_edge_counts = defaultdict(int)

    for from_uuid, to_uuid, weight in relates_to_edges:
        from_cluster = uuid_to_cluster.get(from_uuid)
        to_cluster = uuid_to_cluster.get(to_uuid)

        if from_cluster and to_cluster and from_cluster != to_cluster:
            # ì •ë ¬ëœ í‚¤ë¡œ ì €ì¥ (ë°©í–¥ ë¬´ì‹œ)
            edge_key = tuple(sorted([from_cluster, to_cluster]))
            cluster_edge_counts[edge_key] += 1

    # ì—£ì§€ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    edges = []
    for (from_c, to_c), count in cluster_edge_counts.items():
        if count >= 1:  # ìµœì†Œ 1ê°œ ì—°ê²°
            edges.append({
                "from": from_c,
                "to": to_c,
                "weight": count,
                "relation_type": "INTER_CLUSTER"
            })

    return edges


def get_cluster_detail(
    client,
    cluster_id: str,
    clusters_data: Dict[str, Any]
) -> Optional[Dict[str, Any]]:
    """
    íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì˜ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°

    Args:
        client: Neo4j í´ë¼ì´ì–¸íŠ¸
        cluster_id: í´ëŸ¬ìŠ¤í„° ID
        clusters_data: compute_entity_clusters_hybrid ê²°ê³¼

    Returns:
        í´ëŸ¬ìŠ¤í„° ìƒì„¸ ì •ë³´
    """
    clusters = clusters_data.get("clusters", [])
    target = next((c for c in clusters if c["id"] == cluster_id), None)

    if not target:
        return None

    # í´ëŸ¬ìŠ¤í„° ë‚´ ì—”í‹°í‹°ë“¤ì˜ ìƒì„¸ ì •ë³´
    uuids = target.get("entity_uuids", [])

    cypher = """
    MATCH (e:Entity)
    WHERE e.uuid IN $uuids
    OPTIONAL MATCH (e)-[r:RELATES_TO]-(other:Entity)
    WHERE other.uuid IN $uuids
    RETURN e.uuid as uuid,
           e.name as name,
           e.summary as summary,
           e.pkm_type as pkm_type,
           count(r) as internal_connections
    ORDER BY internal_connections DESC
    """

    results = client.query(cypher, {"uuids": uuids})

    entities = []
    for row in results or []:
        entities.append({
            "uuid": row["uuid"],
            "name": row["name"],
            "summary": row.get("summary", ""),
            "pkm_type": row.get("pkm_type", "Topic"),
            "connections": row.get("internal_connections", 0)
        })

    # ë‚´ë¶€ RELATES_TO ê´€ê³„ë“¤
    cypher_edges = """
    MATCH (e1:Entity)-[r:RELATES_TO]->(e2:Entity)
    WHERE e1.uuid IN $uuids AND e2.uuid IN $uuids
    RETURN e1.uuid as from_uuid, e2.uuid as to_uuid,
           r.fact as fact, r.weight as weight
    """

    edge_results = client.query(cypher_edges, {"uuids": uuids})

    edges = []
    for row in edge_results or []:
        edges.append({
            "from": row["from_uuid"],
            "to": row["to_uuid"],
            "fact": row.get("fact", ""),
            "weight": row.get("weight", 1.0)
        })

    return {
        **target,
        "entities": entities,
        "internal_edges": edges
    }
