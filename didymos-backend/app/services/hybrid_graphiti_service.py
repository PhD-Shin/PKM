"""
Hybrid Graphiti + PKM Ontology Service

Graphitiì˜ Entityì— PKM ì˜¨í†¨ë¡œì§€ ë ˆì´ë¸”(Topic, Project, Task, Person)ì„
ì¶”ê°€í•˜ì—¬ ë‘ ì‹œìŠ¤í…œì˜ ì¥ì ì„ ê²°í•©:

- Graphiti: Temporal KG, ìë™ ì—”í‹°í‹° ìš”ì•½, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
- PKM Ontology: ìš°ë¦¬ cluster_serviceì™€ í˜¸í™˜ë˜ëŠ” ë ˆì´ë¸”

Flow:
1. Graphitiê°€ ë…¸íŠ¸ë¥¼ ì²˜ë¦¬ â†’ Entity ìƒì„± (Episodic -> Entity MENTIONS)
2. í›„ì²˜ë¦¬ë¡œ Entityì— PKM ë ˆì´ë¸” ì¶”ê°€ (LLM ë¶„ë¥˜)
3. Note â†’ Entity MENTIONS ê´€ê³„ ìƒì„± (Episodicì˜ nameì—ì„œ note_id ì¶”ì¶œ)
4. cluster_serviceê°€ PKM ë ˆì´ë¸”ë¡œ í´ëŸ¬ìŠ¤í„°ë§

Note: Graphiti uses 'Entity' and 'Episodic' labels (NOT 'EntityNode' or 'EpisodicNode')
"""

import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import asyncio

from app.db.neo4j import get_neo4j_client
from app.config import settings

logger = logging.getLogger(__name__)

# PKM Core Ontology v2 íƒ€ì… (8ê°œ)
# - Goal: ìµœìƒìœ„ ëª©í‘œ (OKRì˜ O)
# - Project: Goalì„ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ì¤‘ê°„ ë‹¨ìœ„
# - Task: ì‹¤í–‰ ê°€ëŠ¥í•œ ìµœì†Œ ë‹¨ìœ„
# - Topic: ì£¼ì œ/ê°œë… ì¹´í…Œê³ ë¦¬
# - Concept: êµ¬ì²´ì  ê°œë…/ìš©ì–´ (Topicì˜ í•˜ìœ„)
# - Question: ì—°êµ¬ ì§ˆë¬¸ ë˜ëŠ” ë¯¸í•´ê²° ì˜ë¬¸
# - Insight: ë°œê²¬/í†µì°°/ê²°ë¡ 
# - Resource: ì™¸ë¶€ ìë£Œ ì°¸ì¡° (ë…¼ë¬¸, ì±…, URL)
PKM_TYPES = ["Goal", "Project", "Task", "Topic", "Concept", "Question", "Insight", "Resource"]

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ Personë„ ì§€ì› (ê¸°ì¡´ ë°ì´í„°)
PKM_TYPES_LEGACY = ["Person"]

# ì—”í‹°í‹° ì´ë¦„ ìµœì†Œ ê¸¸ì´ (ë„ˆë¬´ ì§§ì€ ì´ë¦„ ì œì™¸)
MIN_ENTITY_NAME_LENGTH = 2
MAX_ENTITY_NAME_LENGTH = 100


def is_valid_entity(name: str) -> bool:
    """
    ì—”í‹°í‹° ì´ë¦„ì˜ ê¸°ë³¸ ìœ íš¨ì„± ê²€ì‚¬ (ìµœì†Œí•œì˜ í•„í„°ë§ë§Œ)

    ë¸”ë™ë¦¬ìŠ¤íŠ¸ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - ëª¨ë“  ëª…ì‚¬ê°€ ë¬¸ë§¥ì— ë”°ë¼ ìœ ìš©í•  ìˆ˜ ìˆìŒ
    ëŒ€ì‹  min_connections í•„í„°ë¡œ ì‹œê°í™” ë‹¨ê³„ì—ì„œ í•„í„°ë§

    Returns:
        True if entity name is valid
    """
    if not name:
        return False

    name_stripped = name.strip()

    # ê¸¸ì´ ì²´í¬
    if len(name_stripped) < MIN_ENTITY_NAME_LENGTH:
        return False
    if len(name_stripped) > MAX_ENTITY_NAME_LENGTH:
        return False

    # ìˆ«ìë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ ê²½ìš° ì œì™¸ (ì˜ˆ: "123", "2024")
    if name_stripped.isdigit():
        return False

    # íŠ¹ìˆ˜ë¬¸ìë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ ê²½ìš° ì œì™¸
    if not any(c.isalnum() for c in name_stripped):
        return False

    return True


# ì—”í‹°í‹° ì´ë¦„ ê¸°ë°˜ ë¶„ë¥˜ ê·œì¹™ (LLM í˜¸ì¶œ ì—†ì´ ë¹ ë¥¸ ë¶„ë¥˜)
# PKM Core Ontology v2 - 8ê°œ íƒ€ì… ë¶„ë¥˜
CLASSIFICATION_RULES = {
    "Goal": [
        # ìµœìƒìœ„ ëª©í‘œ (OKRì˜ O)
        lambda name: any(kw in name.lower() for kw in ["ëª©í‘œ", "goal", "objective", "vision", "ë¯¸ì…˜", "mission"]),
        lambda name: any(kw in name for kw in ["ì™„ì„±", "ë‹¬ì„±", "ì„±ì·¨"]),
    ],
    "Project": [
        # Goalì„ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ì¤‘ê°„ ë‹¨ìœ„
        lambda name: any(kw in name.lower() for kw in ["í”„ë¡œì íŠ¸", "project", "ê°œë°œ", "êµ¬í˜„", "ì‹œìŠ¤í…œ", "chapter", "phase"]),
        lambda name: name.startswith(("PKM", "Didymos", "MVP")),
    ],
    "Task": [
        # ì‹¤í–‰ ê°€ëŠ¥í•œ ìµœì†Œ ë‹¨ìœ„
        lambda name: any(kw in name.lower() for kw in ["todo", "task", "ì‘ì—…", "í• ì¼", "ìˆ˜ì •", "ì¶”ê°€", "êµ¬í˜„í•´ì•¼", "ì‘ì„±", "ê²€í† "]),
        lambda name: name.startswith(("[ ]", "[x]", "TODO", "FIXME")),
    ],
    "Question": [
        # ì—°êµ¬ ì§ˆë¬¸ ë˜ëŠ” ë¯¸í•´ê²° ì˜ë¬¸
        lambda name: name.endswith("?"),
        lambda name: any(kw in name.lower() for kw in ["ì§ˆë¬¸", "question", "ì˜ë¬¸", "ê¶ê¸ˆ", "ì–´ë–»ê²Œ", "ì™œ", "ë¬´ì—‡"]),
        lambda name: name.startswith(("RQ", "Q:", "Q.")),
    ],
    "Insight": [
        # ë°œê²¬/í†µì°°/ê²°ë¡ 
        lambda name: any(kw in name.lower() for kw in ["ì¸ì‚¬ì´íŠ¸", "insight", "ë°œê²¬", "ê²°ë¡ ", "conclusion", "finding", "ë°°ì›€", "ê¹¨ë‹¬ìŒ"]),
        lambda name: name.startswith(("ğŸ’¡", "âœ¨", "Insight:", "Finding:")),
    ],
    "Resource": [
        # ì™¸ë¶€ ìë£Œ ì°¸ì¡° (ë…¼ë¬¸, ì±…, URL)
        lambda name: any(kw in name.lower() for kw in ["ë…¼ë¬¸", "paper", "ì±…", "book", "article", "url", "ë§í¬", "ì°¸ê³ ", "reference"]),
        lambda name: name.startswith(("http", "www.", "ğŸ“š", "ğŸ“„")),
        lambda name: any(ext in name.lower() for ext in [".pdf", ".epub", "arxiv", "doi:"]),
    ],
    "Concept": [
        # êµ¬ì²´ì  ê°œë…/ìš©ì–´ (Topicì˜ í•˜ìœ„)
        # íŠ¹ì • ê¸°ìˆ  ìš©ì–´, ë°©ë²•ë¡ , ì•Œê³ ë¦¬ì¦˜ ë“±
        lambda name: any(kw in name.lower() for kw in [
            "algorithm", "ì•Œê³ ë¦¬ì¦˜", "method", "ë°©ë²•", "technique", "ê¸°ë²•",
            "architecture", "ì•„í‚¤í…ì²˜", "pattern", "íŒ¨í„´", "model", "ëª¨ë¸",
            "framework", "í”„ë ˆì„ì›Œí¬", "protocol", "í”„ë¡œí† ì½œ"
        ]),
        # ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ê¸°ìˆ  ìš©ì–´ (ì˜ˆ: Transformer, BERT, GPT)
        lambda name: len(name) > 2 and name[0].isupper() and any(c.isupper() for c in name[1:]),
    ],
    # Topicì€ ê¸°ë³¸ê°’ (ë‹¤ë¥¸ íƒ€ì…ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ Topic)
    # ê¸°ì¡´ Person ì§€ì› (í•˜ìœ„ í˜¸í™˜ì„±)
    "Person": [
        lambda name: any(suffix in name for suffix in ["ë‹˜", "ì”¨", "êµìˆ˜", "ë°•ì‚¬", "ì„ ìƒ"]),
        lambda name: name.endswith(("ìˆ˜", "í˜¸", "ë¯¼", "ì¤€", "ì§„", "í˜„", "ì„", "ì˜", "í›ˆ")),
    ],
}


def classify_entity_to_pkm_type(entity_name: str, entity_summary: str = None) -> str:
    """
    ì—”í‹°í‹° ì´ë¦„/ìš”ì•½ì„ ê¸°ë°˜ìœ¼ë¡œ PKM íƒ€ì… ë¶„ë¥˜ (Core Ontology v2)

    ë¶„ë¥˜ ì „ëµ:
    1. ì´ë¦„ ê¸°ë°˜ ê·œì¹™ (ê°€ì¥ í™•ì‹¤í•œ ê²½ìš°)
    2. ìš”ì•½ ê¸°ë°˜ ì˜ë¯¸ ë¶„ì„ (Graphitiê°€ ìƒì„±í•œ ìš”ì•½ í™œìš©)
    3. ì´ë¦„ íŒ¨í„´ ë¶„ì„ (ëŒ€ë¬¸ì, íŠ¹ìˆ˜ í˜•ì‹ ë“±)
    4. ê¸°ë³¸ê°’: Topic

    Args:
        entity_name: ì—”í‹°í‹° ì´ë¦„
        entity_summary: Graphitiê°€ ìƒì„±í•œ ì—”í‹°í‹° ìš”ì•½

    Returns:
        PKM íƒ€ì… (Goal, Project, Task, Topic, Concept, Question, Insight, Resource)
    """
    name_lower = entity_name.lower()

    # Step 1: ì´ë¦„ ê¸°ë°˜ ê·œì¹™ (ìš°ì„ ìˆœìœ„ ìˆœì„œëŒ€ë¡œ ì²´í¬)
    # ìˆœì„œ: Goal > Question > Insight > Resource > Task > Project > Concept > Person > Topic
    priority_order = ["Goal", "Question", "Insight", "Resource", "Task", "Project", "Concept", "Person"]

    for pkm_type in priority_order:
        if pkm_type in CLASSIFICATION_RULES:
            for rule in CLASSIFICATION_RULES[pkm_type]:
                try:
                    if rule(entity_name):
                        return pkm_type
                except Exception:
                    continue

    # Step 2: ìš”ì•½ ê¸°ë°˜ ì˜ë¯¸ ë¶„ì„ (í™•ì¥ëœ í‚¤ì›Œë“œ)
    if entity_summary:
        summary_lower = entity_summary.lower()

        # Goal íŒ¨í„´ - ì¥ê¸° ëª©í‘œ, ë¹„ì „, ë°©í–¥
        goal_keywords = [
            "ëª©í‘œ", "goal", "objective", "vision", "ì¥ê¸° ê³„íš", "ë¯¸ì…˜", "mission",
            "ë‹¬ì„±í•˜ê³ ì", "ì´ë£¨ê³ ì", "ìœ„í•´", "ì§€í–¥", "ì¶”êµ¬", "ì§€í–¥ì ", "ë°©í–¥ì„±",
            "ê¶ê·¹ì ", "ìµœì¢…", "ë¹„ì „", "ì „ëµì  ëª©í‘œ", "okr"
        ]
        if any(kw in summary_lower for kw in goal_keywords):
            return "Goal"

        # Question íŒ¨í„´ - ì§ˆë¬¸, ì˜ë¬¸, íƒêµ¬í•  ê²ƒ
        question_keywords = [
            "ì§ˆë¬¸", "question", "ì˜ë¬¸", "ì—°êµ¬ ë¬¸ì œ", "íƒêµ¬", "ì•Œì•„ë³´",
            "ê¶ê¸ˆ", "ì¡°ì‚¬", "research question", "rq", "ì–´ë–»ê²Œ", "ì™œ",
            "ë¬´ì—‡ì¸ì§€", "í™•ì¸ í•„ìš”", "ê²€í†  í•„ìš”", "íŒŒì•… í•„ìš”", "ì•Œì•„ì•¼"
        ]
        if any(kw in summary_lower for kw in question_keywords):
            return "Question"

        # Insight íŒ¨í„´ - ë°œê²¬, ê¹¨ë‹¬ìŒ, ê²°ë¡ 
        insight_keywords = [
            "ë°œê²¬", "insight", "ê²°ë¡ ", "ê¹¨ë‹¬ìŒ", "ë°°ì›€", "í†µì°°", "ì´í•´",
            "ì•Œê²Œ ë¨", "íŒŒì•…ë¨", "í™•ì¸ë¨", "ê¹¨ë‹«", "ì¸ì‚¬ì´íŠ¸", "êµí›ˆ",
            "í•µì‹¬", "ì¤‘ìš”í•œ ì ", "ì‹œì‚¬ì ", "í•¨ì˜", "ì˜ë¯¸í•˜ëŠ”", "learned"
        ]
        if any(kw in summary_lower for kw in insight_keywords):
            return "Insight"

        # Resource íŒ¨í„´ - ì™¸ë¶€ ìë£Œ, ì°¸ê³  ë¬¸í—Œ
        resource_keywords = [
            "ë…¼ë¬¸", "paper", "ì±…", "book", "ì°¸ê³  ìë£Œ", "ì¶œì²˜", "ë§í¬",
            "article", "reference", "ë¬¸í—Œ", "ìë£Œ", "source", "ë¬¸ì„œ",
            "ì €ë„", "journal", "arxiv", "doi", "isbn", "url", "ì›¹ì‚¬ì´íŠ¸",
            "ë¸”ë¡œê·¸", "ê°•ì˜", "lecture", "course", "tutorial", "ê°€ì´ë“œ"
        ]
        if any(kw in summary_lower for kw in resource_keywords):
            return "Resource"

        # Task íŒ¨í„´ - ì‹¤í–‰ ê°€ëŠ¥í•œ í• ì¼
        task_keywords = [
            "í•´ì•¼ í• ", "ì™„ë£Œí•´ì•¼", "task", "todo", "ì‘ì—…", "ì‹¤í–‰",
            "ì²˜ë¦¬", "ìˆ˜í–‰", "ì§„í–‰í•´ì•¼", "ì²´í¬", "í™•ì¸í•´ì•¼", "ì‘ì„±í•´ì•¼",
            "êµ¬í˜„í•´ì•¼", "ìˆ˜ì •í•´ì•¼", "ì¶”ê°€í•´ì•¼", "ì‚­ì œí•´ì•¼", "ë³€ê²½í•´ì•¼",
            "action item", "next step", "í•  ì¼"
        ]
        if any(kw in summary_lower for kw in task_keywords):
            return "Task"

        # Project íŒ¨í„´ - ì¤‘ê°„ ë‹¨ìœ„ í”„ë¡œì íŠ¸
        project_keywords = [
            "í”„ë¡œì íŠ¸", "project", "ê°œë°œ ì¤‘", "êµ¬í˜„", "ì§„í–‰ ì¤‘", "ê³„íš",
            "ì‹œìŠ¤í…œ", "í”Œë«í¼", "ì„œë¹„ìŠ¤", "ì•±", "ì• í”Œë¦¬ì¼€ì´ì…˜", "ëª¨ë“ˆ",
            "ì»´í¬ë„ŒíŠ¸", "feature", "ê¸°ëŠ¥ ê°œë°œ", "ìŠ¤í”„ë¦°íŠ¸", "ë§ˆì¼ìŠ¤í†¤",
            "phase", "ë‹¨ê³„", "initiative", "ì›Œí¬ìŠ¤íŠ¸ë¦¼"
        ]
        if any(kw in summary_lower for kw in project_keywords):
            return "Project"

        # Concept íŒ¨í„´ - ê¸°ìˆ  ê°œë…, ë°©ë²•ë¡ , ì•Œê³ ë¦¬ì¦˜
        concept_keywords = [
            "ê°œë…", "concept", "ë°©ë²•", "method", "ê¸°ë²•", "ì•Œê³ ë¦¬ì¦˜",
            "ê¸°ìˆ ", "ì•„í‚¤í…ì²˜", "architecture", "íŒ¨í„´", "pattern",
            "í”„ë ˆì„ì›Œí¬", "framework", "í”„ë¡œí† ì½œ", "protocol", "ëª¨ë¸",
            "ì´ë¡ ", "theory", "ì›ë¦¬", "principle", "ë²•ì¹™", "ì •ì˜",
            "ìš©ì–´", "terminology", "ì ‘ê·¼ë²•", "approach", "ì „ëµ",
            "í…Œí¬ë‹‰", "technique", "ë©”ì„œë“œ", "ìŠ¤í‚¤ë§ˆ", "êµ¬ì¡°"
        ]
        if any(kw in summary_lower for kw in concept_keywords):
            return "Concept"

        # Person íŒ¨í„´ (í•˜ìœ„ í˜¸í™˜ì„±)
        person_keywords = [
            "ì‚¬ëŒ", "person", "ì—°êµ¬ì›", "í•™ìƒ", "íŒ€ì›", "ì €ì",
            "ë™ë£Œ", "êµìˆ˜", "ë°•ì‚¬", "researcher", "author", "colleague",
            "ê°œë°œì", "developer", "ì—”ì§€ë‹ˆì–´", "engineer", "ë””ìì´ë„ˆ"
        ]
        if any(kw in summary_lower for kw in person_keywords):
            return "Person"

    # Step 3: ì´ë¦„ íŒ¨í„´ ë¶„ì„ (ê·œì¹™ì—ì„œ ëª» ì¡ì€ ì¼€ì´ìŠ¤)

    # ëŒ€ë¬¸ì ì•½ì–´ëŠ” Concept ê°€ëŠ¥ì„± ë†’ìŒ (API, SDK, LLM, GPT ë“±)
    if entity_name.isupper() and len(entity_name) <= 6:
        return "Concept"

    # CamelCase ê¸°ìˆ  ìš©ì–´ëŠ” Concept (GraphQL, TypeScript ë“±)
    if len(entity_name) > 3 and entity_name[0].isupper() and any(c.isupper() for c in entity_name[1:]) and not entity_name.isupper():
        return "Concept"

    # "-ing" ë˜ëŠ” "-tion" ìœ¼ë¡œ ëë‚˜ëŠ” ì˜ì–´ ë‹¨ì–´ëŠ” Concept ê°€ëŠ¥ì„±
    if entity_name.endswith(("ing", "tion", "ment", "ness", "ity")):
        return "Concept"

    # ê¸°ë³¸ê°’: Topic (ì£¼ì œ ì¹´í…Œê³ ë¦¬)
    return "Topic"


async def add_pkm_labels_to_graphiti_entities(
    vault_id: str = None,
    batch_size: int = 100
) -> Dict[str, Any]:
    """
    ê¸°ì¡´ Graphiti EntityNodeì— PKM ë ˆì´ë¸” ì¶”ê°€

    ì´ë¯¸ ë ˆì´ë¸”ì´ ìˆëŠ” ì—”í‹°í‹°ëŠ” ìŠ¤í‚µ

    Args:
        vault_id: íŠ¹ì • vaultë§Œ ì²˜ë¦¬ (Noneì´ë©´ ì „ì²´)
        batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  ì—”í‹°í‹° ìˆ˜

    Returns:
        ì²˜ë¦¬ ê²°ê³¼ í†µê³„
    """
    client = get_neo4j_client()

    try:
        # Step 1: PKM ë ˆì´ë¸”ì´ ì—†ëŠ” Entity ì¡°íšŒ (Core Ontology v2 - 8ê°œ íƒ€ì…)
        cypher_find = """
        MATCH (e:Entity)
        WHERE NOT e:Goal AND NOT e:Project AND NOT e:Task AND NOT e:Topic
          AND NOT e:Concept AND NOT e:Question AND NOT e:Insight AND NOT e:Resource
          AND NOT e:Person
        RETURN e.uuid as uuid, e.name as name, e.summary as summary
        LIMIT $batch_size
        """

        entities = client.query(cypher_find, {"batch_size": batch_size})

        if not entities:
            logger.info("No Entity without PKM labels found")
            return {
                "status": "completed",
                "processed": 0,
                "message": "All Entities already have PKM labels"
            }

        logger.info(f"Found {len(entities)} Entities to classify")

        # Step 2: ê° ì—”í‹°í‹° ë¶„ë¥˜ ë° ë ˆì´ë¸” ì¶”ê°€ (Core Ontology v2 - 8ê°œ íƒ€ì… + Person)
        stats = {
            "Goal": 0, "Project": 0, "Task": 0, "Topic": 0,
            "Concept": 0, "Question": 0, "Insight": 0, "Resource": 0,
            "Person": 0, "errors": 0
        }

        for entity in entities:
            try:
                uuid = entity["uuid"]
                name = entity["name"] or uuid
                summary = entity.get("summary", "")

                # PKM íƒ€ì… ë¶„ë¥˜
                pkm_type = classify_entity_to_pkm_type(name, summary)

                # ë ˆì´ë¸” ì¶”ê°€
                cypher_add_label = f"""
                MATCH (e:Entity {{uuid: $uuid}})
                SET e:{pkm_type}
                SET e.pkm_type = $pkm_type
                SET e.pkm_classified_at = datetime()
                RETURN e.name as name
                """

                result = client.query(cypher_add_label, {"uuid": uuid, "pkm_type": pkm_type})

                if result:
                    stats[pkm_type] += 1
                    logger.debug(f"Added {pkm_type} label to: {name}")

            except Exception as e:
                logger.error(f"Error adding label to {entity.get('name')}: {e}")
                stats["errors"] += 1

        # Core Ontology v2 - 8ê°œ íƒ€ì… + Person
        all_types = PKM_TYPES + PKM_TYPES_LEGACY
        total_processed = sum(stats.get(t, 0) for t in all_types)
        logger.info(f"âœ… PKM labels added: {stats}")

        return {
            "status": "success",
            "processed": total_processed,
            "stats": stats,
            "remaining": await _count_unlabeled_entities(client)
        }

    except Exception as e:
        logger.error(f"Error in add_pkm_labels_to_graphiti_entities: {e}")
        return {
            "status": "error",
            "error": str(e)
        }


async def _count_unlabeled_entities(client) -> int:
    """PKM ë ˆì´ë¸”ì´ ì—†ëŠ” Entity ìˆ˜ ì¡°íšŒ (Core Ontology v2 - 8ê°œ íƒ€ì…)"""
    try:
        result = client.query("""
            MATCH (e:Entity)
            WHERE NOT e:Goal AND NOT e:Project AND NOT e:Task AND NOT e:Topic
              AND NOT e:Concept AND NOT e:Question AND NOT e:Insight AND NOT e:Resource
              AND NOT e:Person
            RETURN count(e) as count
        """, {})
        return result[0]["count"] if result else 0
    except Exception:
        return -1


async def create_mentions_from_episodes(
    vault_id: str = None,
    batch_size: int = 100
) -> Dict[str, Any]:
    """
    Graphiti Episodic-Entity ê´€ê³„ë¥¼ Note-Entity MENTIONS ê´€ê³„ë¡œ ë³€í™˜

    GraphitiëŠ” Episodic â†’ Entity MENTIONS ê´€ê³„ë¥¼ ì‚¬ìš©
    cluster_serviceëŠ” Note â†’ Entity MENTIONS ê´€ê³„ë¥¼ ê¸°ëŒ€

    ì´ í•¨ìˆ˜ëŠ” Episodicì˜ nameì—ì„œ note_idë¥¼ ì¶”ì¶œí•˜ì—¬ (name = 'note_{note_id}')
    Note â†’ Entity MENTIONS ê´€ê³„ë¥¼ ìƒì„±

    Args:
        vault_id: íŠ¹ì • vaultë§Œ ì²˜ë¦¬
        batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  ê´€ê³„ ìˆ˜

    Returns:
        ì²˜ë¦¬ ê²°ê³¼ í†µê³„
    """
    client = get_neo4j_client()

    try:
        # Step 1: Episodic-Entity ê´€ê³„ì—ì„œ Note-Entity MENTIONSê°€ ì—†ëŠ” ê²ƒ ì°¾ê¸°
        # Graphitiì˜ Episodic.name = 'note_{note_id}' í˜•íƒœ
        # Note.note_id = '{path}' í˜•íƒœ
        cypher_find = """
        MATCH (ep:Episodic)-[:MENTIONS]->(e:Entity)
        WHERE ep.name STARTS WITH 'note_'
        WITH ep, e,
             replace(ep.name, 'note_', '') as note_id
        MATCH (n:Note {note_id: note_id})
        WHERE NOT (n)-[:MENTIONS]->(e)
        RETURN n.note_id as note_id, e.uuid as entity_uuid, e.name as entity_name
        LIMIT $batch_size
        """

        relations = client.query(cypher_find, {"batch_size": batch_size})

        if not relations:
            logger.info("No new MENTIONS relationships to create")
            return {
                "status": "completed",
                "created": 0,
                "message": "All Episodic-Entity relations already have Note MENTIONS"
            }

        logger.info(f"Found {len(relations)} MENTIONS relationships to create")

        # Step 2: MENTIONS ê´€ê³„ ìƒì„±
        created = 0
        errors = 0

        for rel in relations:
            try:
                cypher_create = """
                MATCH (n:Note {note_id: $note_id})
                MATCH (e:Entity {uuid: $entity_uuid})
                MERGE (n)-[m:MENTIONS]->(e)
                SET m.created_at = datetime()
                SET m.source = 'graphiti_migration'
                RETURN count(m) as count
                """

                result = client.query(cypher_create, {
                    "note_id": rel["note_id"],
                    "entity_uuid": rel["entity_uuid"]
                })

                if result and result[0]["count"] > 0:
                    created += 1

            except Exception as e:
                logger.error(f"Error creating MENTIONS: {e}")
                errors += 1

        logger.info(f"âœ… Created {created} MENTIONS relationships")

        return {
            "status": "success",
            "created": created,
            "errors": errors
        }

    except Exception as e:
        logger.error(f"Error in create_mentions_from_episodes: {e}")
        return {
            "status": "error",
            "error": str(e)
        }


async def migrate_graphiti_to_hybrid(
    vault_id: str = None,
    max_iterations: int = 10
) -> Dict[str, Any]:
    """
    ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜: Graphiti ìŠ¤í‚¤ë§ˆ â†’ í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤í‚¤ë§ˆ

    1. EntityNodeì— PKM ë ˆì´ë¸” ì¶”ê°€
    2. Episode-Entity â†’ Note-Entity MENTIONS ê´€ê³„ ìƒì„±

    Args:
        vault_id: íŠ¹ì • vaultë§Œ ì²˜ë¦¬
        max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (ë°°ì¹˜ ì²˜ë¦¬)

    Returns:
        ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼
    """
    logger.info(f"ğŸš€ Starting Graphiti â†’ Hybrid migration (vault: {vault_id or 'all'})")

    results = {
        "pkm_labels": {"total_processed": 0, "stats": {}},
        "mentions": {"total_created": 0},
        "iterations": 0
    }

    for i in range(max_iterations):
        results["iterations"] = i + 1

        # Step 1: PKM ë ˆì´ë¸” ì¶”ê°€
        label_result = await add_pkm_labels_to_graphiti_entities(vault_id)

        if label_result.get("processed", 0) > 0:
            results["pkm_labels"]["total_processed"] += label_result["processed"]
            # Core Ontology v2 - 8ê°œ íƒ€ì… + Person
            all_types = PKM_TYPES + PKM_TYPES_LEGACY
            for pkm_type in all_types:
                prev = results["pkm_labels"]["stats"].get(pkm_type, 0)
                results["pkm_labels"]["stats"][pkm_type] = prev + label_result.get("stats", {}).get(pkm_type, 0)

        # Step 2: MENTIONS ê´€ê³„ ìƒì„±
        mentions_result = await create_mentions_from_episodes(vault_id)

        if mentions_result.get("created", 0) > 0:
            results["mentions"]["total_created"] += mentions_result["created"]

        # ë” ì´ìƒ ì²˜ë¦¬í•  ê²ƒì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if label_result.get("processed", 0) == 0 and mentions_result.get("created", 0) == 0:
            logger.info(f"Migration completed after {i + 1} iterations")
            break

        # Rate limiting
        await asyncio.sleep(0.5)

    logger.info(f"âœ… Migration complete: {results}")
    return results


async def process_note_hybrid(
    note_id: str,
    content: str,
    updated_at: datetime = None,
    metadata: dict = None
) -> Dict[str, Any]:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ë…¸íŠ¸ ì²˜ë¦¬

    1. Graphitië¡œ ì—”í‹°í‹° ì¶”ì¶œ (temporal, ìë™ ìš”ì•½)
    2. ì¶”ì¶œëœ EntityNodeì— PKM ë ˆì´ë¸” ì¶”ê°€
    3. Note â†’ EntityNode MENTIONS ê´€ê³„ ìƒì„±

    Args:
        note_id: ë…¸íŠ¸ ID
        content: ë…¸íŠ¸ ë‚´ìš©
        updated_at: ìˆ˜ì • ì‹œê°„
        metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°

    Returns:
        ì²˜ë¦¬ ê²°ê³¼
    """
    from app.services.graphiti_service import async_process_note

    try:
        # Step 1: Graphitië¡œ ì²˜ë¦¬
        graphiti_result = await async_process_note(
            note_id=note_id,
            content=content,
            updated_at=updated_at,
            metadata=metadata
        )

        if graphiti_result.get("status") != "success":
            return graphiti_result

        nodes_extracted = graphiti_result.get("nodes_extracted", 0)

        if nodes_extracted == 0:
            return graphiti_result

        # Step 2: ì¶”ì¶œëœ Entityì— PKM ë ˆì´ë¸” ì¶”ê°€
        client = get_neo4j_client()

        # ì´ ë…¸íŠ¸ì™€ ì—°ê²°ëœ Entity ì°¾ê¸° (Graphiti uses 'Episodic' and 'Entity' labels)
        cypher_find_entities = """
        MATCH (ep:Episodic)-[:MENTIONS]->(e:Entity)
        WHERE ep.name = $episode_name
          AND NOT e:Topic AND NOT e:Project AND NOT e:Task AND NOT e:Person
        RETURN e.uuid as uuid, e.name as name, e.summary as summary
        """

        episode_name = f"note_{note_id}"
        entities = client.query(cypher_find_entities, {"episode_name": episode_name})

        labeled_count = 0

        for entity in (entities or []):
            entity_name = entity["name"] or entity["uuid"]

            # ê¸°ë³¸ ìœ íš¨ì„± ê²€ì‚¬ë§Œ ìˆ˜í–‰ (ë„ˆë¬´ ì§§ê±°ë‚˜ ìˆ«ìë§Œì¸ ê²½ìš°ë§Œ ì œì™¸)
            # ë¸”ë™ë¦¬ìŠ¤íŠ¸ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - min_connections í•„í„°ë¡œ ì‹œê°í™” ë‹¨ê³„ì—ì„œ ì²˜ë¦¬
            if not is_valid_entity(entity_name):
                logger.debug(f"â© Skipping invalid entity: {entity_name}")
                continue

            pkm_type = classify_entity_to_pkm_type(
                entity_name,
                entity.get("summary", "")
            )

            cypher_add_label = f"""
            MATCH (e:Entity {{uuid: $uuid}})
            SET e:{pkm_type}
            SET e.pkm_type = $pkm_type
            SET e.pkm_classified_at = datetime()
            """

            client.query(cypher_add_label, {"uuid": entity["uuid"], "pkm_type": pkm_type})
            labeled_count += 1

        # Step 3: Note â†’ Entity MENTIONS ê´€ê³„ ìƒì„± (ìœ íš¨í•œ ì—”í‹°í‹°ë§Œ)
        cypher_create_mentions = """
        MATCH (n:Note {note_id: $note_id})
        MATCH (ep:Episodic {name: $episode_name})-[:MENTIONS]->(e:Entity)
        WHERE NOT (n)-[:MENTIONS]->(e)
        MERGE (n)-[m:MENTIONS]->(e)
        SET m.created_at = datetime()
        SET m.source = 'graphiti_hybrid'
        RETURN count(m) as count
        """

        mentions_result = client.query(cypher_create_mentions, {
            "note_id": note_id,
            "episode_name": episode_name
        })

        mentions_created = mentions_result[0]["count"] if mentions_result else 0

        return {
            **graphiti_result,
            "pkm_labels_added": labeled_count,
            "mentions_created": mentions_created,
            "hybrid_mode": True
        }

    except Exception as e:
        logger.error(f"Error in hybrid processing for {note_id}: {e}")
        return {
            "status": "error",
            "note_id": note_id,
            "error": str(e)
        }


# Sync wrapper for compatibility
def process_note_to_graph_hybrid(note_id: str, content: str, metadata: dict = None) -> int:
    """
    ë™ê¸° ë˜í¼: ê¸°ì¡´ ontology_serviceì™€ í˜¸í™˜
    """
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(
                    asyncio.run,
                    process_note_hybrid(note_id, content, metadata=metadata)
                )
                result = future.result()
        else:
            result = loop.run_until_complete(
                process_note_hybrid(note_id, content, metadata=metadata)
            )

        return result.get("nodes_extracted", 0)

    except RuntimeError:
        result = asyncio.run(process_note_hybrid(note_id, content, metadata=metadata))
        return result.get("nodes_extracted", 0)
