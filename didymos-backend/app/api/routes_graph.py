"""
Graph Visualization API ë¼ìš°í„°
"""
from fastapi import APIRouter, HTTPException, Query, Depends
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
from app.services.graph_visualization_service import (
    get_note_graph,
    get_note_graph_vis,
    get_user_graph,
    get_entity_graph
)
from app.services.cluster_service import (
    compute_clusters_louvain,
    compute_clusters_semantic,
    get_cached_clusters,
    save_cluster_cache,
    invalidate_cluster_cache,
    generate_llm_summaries,
    is_cluster_cache_stale
)
from app.services.entity_cluster_service import (
    compute_entity_clusters_hybrid,
    get_cluster_detail,
    get_relates_to_edges_with_semantic_types,
    infer_semantic_edge_type,
    PKM_EDGE_TYPE_MATRIX
)
from app.schemas.cluster import (
    ClusteredGraphResponse,
    ClusterComputeRequest,
    ClusterUpdateRequest
)
from app.db.neo4j_bolt import Neo4jBoltClient
import logging

logger = logging.getLogger(__name__)


def get_neo4j_client():
    """Neo4j í´ë¼ì´ì–¸íŠ¸ ì˜ì¡´ì„±"""
    from app.config import settings
    return Neo4jBoltClient(
        uri=settings.neo4j_uri,
        username=settings.neo4j_username,
        password=settings.neo4j_password
    )

router = APIRouter(prefix="/graph", tags=["graph"])


class GraphNode(BaseModel):
    """ê·¸ë˜í”„ ë…¸ë“œ"""
    id: str
    label: str
    type: str
    properties: Dict[str, Any]


class GraphEdge(BaseModel):
    """ê·¸ë˜í”„ ì—£ì§€"""
    from_: str = None  # Use alias to avoid 'from' keyword
    to: str
    type: str
    label: str

    class Config:
        fields = {'from_': 'from'}


class GraphResponse(BaseModel):
    """ê·¸ë˜í”„ ì‘ë‹µ"""
    status: str
    count_nodes: int
    count_edges: int
    nodes: List[Dict[str, Any]]
    edges: List[Dict[str, Any]]


@router.get("/note/{note_id}", response_model=GraphResponse)
async def get_note_graph_view(
    note_id: str,
    depth: int = Query(1, description="íƒìƒ‰ ê¹Šì´", ge=1, le=3)
):
    """
    íŠ¹ì • ë…¸íŠ¸ ì¤‘ì‹¬ì˜ ê·¸ë˜í”„ ì‹œê°í™” ë°ì´í„°

    - note_id: ì¤‘ì‹¬ ë…¸íŠ¸ ID
    - depth: íƒìƒ‰ ê¹Šì´ (1~3)
    """
    try:
        graph_data = get_note_graph_vis(note_id=note_id, hops=depth)

        return GraphResponse(
            status="success",
            count_nodes=len(graph_data["nodes"]),
            count_edges=len(graph_data["edges"]),
            nodes=graph_data["nodes"],
            edges=graph_data["edges"]
        )

    except Exception as e:
        logger.error(f"Failed to get note graph: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to retrieve graph: {str(e)}"
        )


@router.get("/user/{user_id}", response_model=GraphResponse)
async def get_user_graph_view(
    user_id: str,
    vault_id: Optional[str] = Query(None, description="Vault ID (optional)"),
    limit: int = Query(100, description="ìµœëŒ€ ë…¸ë“œ ê°œìˆ˜", ge=10, le=500)
):
    """
    ì‚¬ìš©ìì˜ ì „ì²´ ì§€ì‹ ê·¸ë˜í”„

    - user_id: ì‚¬ìš©ì ID
    - vault_id: Vault ID (optional)
    - limit: ìµœëŒ€ ë…¸ë“œ ê°œìˆ˜ (ê¸°ë³¸ 100, ìµœëŒ€ 5000)
    """
    try:
        graph_data = get_user_graph(
            user_id=user_id,
            vault_id=vault_id,
            limit=limit
        )

        return GraphResponse(
            status="success",
            count_nodes=len(graph_data["nodes"]),
            count_edges=len(graph_data["edges"]),
            nodes=graph_data["nodes"],
            edges=graph_data["edges"]
        )

    except Exception as e:
        logger.error(f"Failed to get user graph: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to retrieve graph: {str(e)}"
        )


@router.get("/entities", response_model=GraphResponse)
async def get_entities_graph_view(
    entity_type: Optional[str] = Query(None, description="ì—”í‹°í‹° íƒ€ì… (Topic, Project, Task, Person)"),
    limit: int = Query(50, description="ìµœëŒ€ ì—”í‹°í‹° ê°œìˆ˜", ge=10, le=200)
):
    """
    ì—”í‹°í‹° ì¤‘ì‹¬ ê·¸ë˜í”„

    - entity_type: í•„í„°ë§í•  ì—”í‹°í‹° íƒ€ì… (optional)
    - limit: ìµœëŒ€ ì—”í‹°í‹° ê°œìˆ˜
    """
    try:
        graph_data = get_entity_graph(
            entity_type=entity_type,
            limit=limit
        )

        return GraphResponse(
            status="success",
            count_nodes=len(graph_data["nodes"]),
            count_edges=len(graph_data["edges"]),
            nodes=graph_data["nodes"],
            edges=graph_data["edges"]
        )

    except Exception as e:
        logger.error(f"Failed to get entity graph: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to retrieve graph: {str(e)}"
        )


@router.get("/vault/clustered", response_model=ClusteredGraphResponse)
async def get_clustered_vault_graph(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    folder_prefix: str = Query(None, description="í´ë” ê²½ë¡œ í•„í„° (ì˜ˆ: '1_í”„ë¡œì íŠ¸/', '2_ì—°êµ¬/')"),
    force_recompute: bool = Query(False, description="ìºì‹œ ë¬´ì‹œí•˜ê³  ì¬ê³„ì‚°"),
    target_clusters: int = Query(10, ge=3, le=50, description="ëª©í‘œ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜"),
    include_llm: bool = Query(False, description="LLM ìš”ì•½ í¬í•¨ (ëŠë¦¼)"),
    method: str = Query("semantic", description="í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•: 'semantic' (UMAP+HDBSCAN) ë˜ëŠ” 'type_based'"),
    warmup: bool = Query(False, description="ë°±ê·¸ë¼ìš´ë“œ ìºì‹œ ì›Œë°ì—… (ì‘ë‹µ ì¦‰ì‹œ ë°˜í™˜)"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
):
    """
    í´ëŸ¬ìŠ¤í„°ë§ëœ Vault ê·¸ë˜í”„

    - vault_id: Vault ID
    - force_recompute: ìºì‹œ ë¬´ì‹œí•˜ê³  ì¬ê³„ì‚°
    - target_clusters: ëª©í‘œ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
    - include_llm: LLM ìš”ì•½ í¬í•¨ ì—¬ë¶€
    - warmup: ë°±ê·¸ë¼ìš´ë“œ ìºì‹œ ì›Œë°ì—… (ì‘ë‹µ ì¦‰ì‹œ ë°˜í™˜)

    **ì‘ë‹µ ì˜ˆì‹œ:**
    ```json
    {
      "status": "success",
      "level": 1,
      "cluster_count": 3,
      "total_nodes": 2543,
      "clusters": [
        {
          "id": "cluster_1",
          "name": "Topic Cluster",
          "level": 1,
          "node_count": 45,
          "summary": "Research topics related to...",
          "key_insights": ["Insight 1", "Insight 2"],
          "importance_score": 8.5
        }
      ],
      "edges": [],
      "last_computed": "2024-12-01T10:00:00"
    }
    ```
    """
    try:
        # Warmup ëª¨ë“œ: ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìºì‹œ ìƒì„±, ì¦‰ì‹œ ì‘ë‹µ ë°˜í™˜
        if warmup:
            import asyncio
            from concurrent.futures import ThreadPoolExecutor

            def background_warmup():
                try:
                    logger.info(f"ğŸ”¥ Background warmup started for vault {vault_id}")
                    result = compute_clusters_semantic(
                        client=client,
                        vault_id=vault_id,
                        target_clusters=target_clusters
                    )
                    if result.get("clusters"):
                        save_cluster_cache(client, vault_id, result["clusters"], result["method"], edges=result.get("edges", []))
                        logger.info(f"âœ… Background warmup completed for vault {vault_id}")
                except Exception as e:
                    logger.error(f"Background warmup failed: {e}")

            executor = ThreadPoolExecutor(max_workers=1)
            executor.submit(background_warmup)

            return ClusteredGraphResponse(
                status="warming_up",
                level=1,
                cluster_count=0,
                total_nodes=0,
                clusters=[],
                edges=[],
                last_computed="warmup_in_progress",
                computation_method="background_warmup"
            )

        # ìºì‹œ í‚¤ì— folder_prefix í¬í•¨
        cache_key = f"{vault_id}:{folder_prefix or 'all'}"

        # ìºì‹œ í™•ì¸ (folder_prefixê°€ ìˆìœ¼ë©´ ìºì‹œ ìŠ¤í‚µ - í´ë”ë³„ ìºì‹œëŠ” ë³„ë„ êµ¬í˜„ í•„ìš”)
        if not force_recompute and not folder_prefix:
            cached = get_cached_clusters(client, vault_id)
            if cached and not is_cluster_cache_stale(client, vault_id, cached.get("computed_at")):
                logger.info(f"âœ… Returning cached clusters for vault {vault_id}")
                return ClusteredGraphResponse(
                    status="success",
                    level=1,
                    cluster_count=len(cached["clusters"]),
                    total_nodes=sum(c.get("node_count", 0) for c in cached["clusters"]),
                    clusters=cached["clusters"],
                    edges=cached.get("edges", []),
                    last_computed=cached["computed_at"],
                    computation_method=cached["method"]
                )
            elif cached:
                logger.info(f"â™»ï¸ Cache stale for vault {vault_id}, recomputing...")

        # í´ëŸ¬ìŠ¤í„° ê³„ì‚° (ë°©ë²• ì„ íƒ)
        folder_info = f" in folder '{folder_prefix}'" if folder_prefix else ""
        logger.info(f"ğŸ”„ Computing clusters for vault {vault_id}{folder_info} using method={method}")
        method_normalized = method.lower()

        if method_normalized in ["semantic", "auto"]:
            result = compute_clusters_semantic(
                client=client,
                vault_id=vault_id,
                target_clusters=target_clusters,
                folder_prefix=folder_prefix
            )
        elif method_normalized in ["type_based", "type"]:
            result = compute_clusters_louvain(
                client=client,
                vault_id=vault_id,
                target_clusters=target_clusters,
                folder_prefix=folder_prefix
            )
        else:
            raise HTTPException(status_code=400, detail="Invalid clustering method")

        # ì˜ë¯¸ë¡ ì  í´ëŸ¬ìŠ¤í„°ë§ì´ ì‹¤íŒ¨í–ˆê±°ë‚˜ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ í´ë°±
        if method_normalized in ["semantic", "auto"] and (not result.get("clusters")):
            logger.info("Semantic clustering returned no clusters. Falling back to type-based.")
            result = compute_clusters_louvain(
                client=client,
                vault_id=vault_id,
                target_clusters=target_clusters,
                folder_prefix=folder_prefix
            )
            result["method"] = "semantic_fallback"

        clusters = result["clusters"]
        edges = result.get("edges", [])

        # LLM ìš”ì•½ ìƒì„± (ì˜µì…˜)
        if include_llm and len(clusters) > 0:
            logger.info("ğŸ¤– Generating LLM summaries with GPT-5 Mini...")
            clusters = generate_llm_summaries(client, vault_id, clusters)

        # ìºì‹œ ì €ì¥ (folder_prefix ì—†ì„ ë•Œë§Œ)
        if not folder_prefix:
            save_cluster_cache(client, vault_id, clusters, result["method"], edges=edges)

        return ClusteredGraphResponse(
            status="success",
            level=1,
            cluster_count=len(clusters),
            total_nodes=result["total_nodes"],
            clusters=clusters,
            edges=edges,
            last_computed=result["computed_at"],
            computation_method=result["method"]
        )

    except Exception as e:
        logger.error(f"Failed to get clustered graph: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to compute clusters: {str(e)}"
        )


@router.post("/vault/clustered/invalidate")
async def invalidate_clusters(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
):
    """
    í´ëŸ¬ìŠ¤í„° ìºì‹œ ë¬´íš¨í™” (ë…¸íŠ¸ ì—…ë°ì´íŠ¸ í›„ í˜¸ì¶œ)

    - vault_id: Vault ID
    """
    try:
        success = invalidate_cluster_cache(client, vault_id)

        if success:
            return {"status": "success", "message": "Cluster cache invalidated"}
        else:
            raise HTTPException(status_code=500, detail="Failed to invalidate cache")

    except Exception as e:
        logger.error(f"Failed to invalidate cluster cache: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to invalidate cache: {str(e)}"
        )


@router.post("/vault/reset-entities")
async def reset_vault_entities(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
):
    """
    ğŸ”´ Vault ì—”í‹°í‹° ì™„ì „ ì´ˆê¸°í™” (MVP ê°œë°œìš©)

    - ëª¨ë“  Topic, Project, Task, Person ì—”í‹°í‹° ì‚­ì œ
    - MENTIONS ê´€ê³„ ì‚­ì œ
    - í´ëŸ¬ìŠ¤í„° ìºì‹œ ë¬´íš¨í™”
    - Note ë…¸ë“œëŠ” ìœ ì§€

    âš ï¸ ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!
    """
    try:
        # 1. Vaultì— ì—°ê²°ëœ ì—”í‹°í‹°ì™€ ê´€ê³„ ì‚­ì œ
        cypher_delete_entities = """
        MATCH (v:Vault {id: $vault_id})-[:HAS_NOTE]->(n:Note)-[m:MENTIONS]->(e)
        WHERE e:Topic OR e:Project OR e:Task OR e:Person
        DELETE m
        WITH DISTINCT e
        WHERE NOT (e)--()
        DELETE e
        RETURN count(e) as deleted_entities
        """

        result1 = client.query(cypher_delete_entities, {"vault_id": vault_id})
        deleted_entities = result1[0]["deleted_entities"] if result1 else 0

        # 2. ê³ ì•„ ì—”í‹°í‹° ì •ë¦¬ (ë‹¤ë¥¸ vaultì—ì„œë„ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ê²½ìš°)
        cypher_cleanup_orphans = """
        MATCH (e)
        WHERE (e:Topic OR e:Project OR e:Task OR e:Person)
          AND NOT (e)--()
        DELETE e
        RETURN count(e) as orphans_deleted
        """

        result2 = client.query(cypher_cleanup_orphans, {})
        orphans_deleted = result2[0]["orphans_deleted"] if result2 else 0

        # 3. ì—”í‹°í‹° ê°„ ê´€ê³„ë„ ì •ë¦¬
        cypher_delete_entity_relations = """
        MATCH (v:Vault {id: $vault_id})-[:HAS_NOTE]->(n:Note)
        WITH COLLECT(n.note_id) as note_ids
        MATCH (e1)-[r:RELATED_TO|PART_OF]->(e2)
        WHERE (e1:Topic OR e1:Project OR e1:Task OR e1:Person)
          AND (e2:Topic OR e2:Project OR e2:Task OR e2:Person)
        DELETE r
        RETURN count(r) as relations_deleted
        """

        result3 = client.query(cypher_delete_entity_relations, {"vault_id": vault_id})
        relations_deleted = result3[0]["relations_deleted"] if result3 else 0

        # 4. í´ëŸ¬ìŠ¤í„° ìºì‹œ ë¬´íš¨í™”
        invalidate_cluster_cache(client, vault_id)

        logger.info(f"ğŸ”´ Reset entities for vault {vault_id}: {deleted_entities} entities, {orphans_deleted} orphans, {relations_deleted} relations")

        return {
            "status": "success",
            "message": "Vault entities reset complete",
            "deleted_entities": deleted_entities,
            "orphans_deleted": orphans_deleted,
            "relations_deleted": relations_deleted
        }

    except Exception as e:
        logger.error(f"Failed to reset vault entities: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to reset entities: {str(e)}"
        )


@router.get("/vault/folders")
async def get_vault_folders(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
):
    """
    Vault ë‚´ í´ë” ëª©ë¡ ì¡°íšŒ (PARA ë…¸íŠ¸ ê¸°ë²• ì§€ì›)

    í´ë”ë³„ ë…¸íŠ¸ ê°œìˆ˜ì™€ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # ë…¸íŠ¸ ê²½ë¡œì—ì„œ í´ë” ì¶”ì¶œ
        cypher = """
        MATCH (v:Vault {id: $vault_id})-[:HAS_NOTE]->(n:Note)
        WITH n.note_id AS note_id
        WITH split(note_id, '/')[0] AS folder
        WHERE folder IS NOT NULL AND folder <> ''
        RETURN folder, count(*) AS note_count
        ORDER BY note_count DESC
        """

        result = client.query(cypher, {"vault_id": vault_id})

        folders = [
            {"folder": r["folder"], "note_count": r["note_count"]}
            for r in (result or [])
        ]

        return {
            "status": "success",
            "vault_id": vault_id,
            "total_folders": len(folders),
            "folders": folders
        }

    except Exception as e:
        logger.error(f"Failed to get vault folders: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get folders: {str(e)}"
        )


@router.get("/debug/stats")
async def get_debug_stats(
    vault_id: str = Query(..., description="Vault ID"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
):
    """
    ë””ë²„ê·¸ìš©: Neo4j ë°ì´í„° í†µê³„ í™•ì¸
    """
    try:
        # 1. Vault ì¡´ì¬ í™•ì¸
        vault_check = client.query(
            "MATCH (v:Vault {id: $vault_id}) RETURN v.id AS id",
            {"vault_id": vault_id}
        )

        # 2. ì „ì²´ Note ìˆ˜
        total_notes = client.query(
            "MATCH (n:Note) RETURN count(n) AS count",
            {}
        )

        # 3. Vaultì— ì—°ê²°ëœ Note ìˆ˜
        vault_notes = client.query(
            "MATCH (v:Vault {id: $vault_id})-[:HAS_NOTE]->(n:Note) RETURN count(n) AS count",
            {"vault_id": vault_id}
        )

        # 4. ì„ë² ë”©ì´ ìˆëŠ” Note ìˆ˜
        notes_with_embedding = client.query(
            "MATCH (v:Vault {id: $vault_id})-[:HAS_NOTE]->(n:Note) WHERE n.embedding IS NOT NULL RETURN count(n) AS count",
            {"vault_id": vault_id}
        )

        # 5. ì „ì²´ Vault ëª©ë¡
        all_vaults = client.query(
            "MATCH (v:Vault) RETURN v.id AS id LIMIT 10",
            {}
        )

        # 6. ì—”í‹°í‹° ìˆ˜ (Topic, Project, Task, Person)
        entity_counts = client.query(
            """
            MATCH (v:Vault {id: $vault_id})-[:HAS_NOTE]->(n:Note)-[:MENTIONS]->(e)
            WHERE e:Topic OR e:Project OR e:Task OR e:Person
            WITH labels(e)[0] AS entity_type, count(DISTINCT e) AS cnt
            RETURN entity_type, cnt
            """,
            {"vault_id": vault_id}
        )

        # 7. Note-Entity MENTIONS ê´€ê³„ ìˆ˜
        mentions_count = client.query(
            """
            MATCH (v:Vault {id: $vault_id})-[:HAS_NOTE]->(n:Note)-[m:MENTIONS]->(e)
            RETURN count(m) AS count
            """,
            {"vault_id": vault_id}
        )

        entity_stats = {r["entity_type"]: r["cnt"] for r in (entity_counts or [])}

        return {
            "vault_id_queried": vault_id,
            "vault_exists": len(vault_check or []) > 0,
            "all_vaults": [v["id"] for v in (all_vaults or [])],
            "total_notes_in_db": (total_notes[0]["count"] if total_notes else 0),
            "notes_in_vault": (vault_notes[0]["count"] if vault_notes else 0),
            "notes_with_embedding": (notes_with_embedding[0]["count"] if notes_with_embedding else 0),
            "entity_counts": entity_stats,
            "total_mentions": (mentions_count[0]["count"] if mentions_count else 0),
        }

    except Exception as e:
        logger.error(f"Debug stats error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/migrate/graphiti-to-hybrid")
async def migrate_graphiti_to_hybrid(
    vault_id: str = Query(None, description="Vault ID (optional, all if not specified)"),
    max_iterations: int = Query(10, description="Maximum migration iterations")
) -> Dict[str, Any]:
    """
    Graphiti Entityì— PKM ë ˆì´ë¸” ì¶”ê°€ ë§ˆì´ê·¸ë ˆì´ì…˜

    Graphitiê°€ ìƒì„±í•œ Entityì— Topic/Project/Task/Person ë ˆì´ë¸”ì„ ì¶”ê°€í•˜ì—¬
    cluster_serviceì™€ í˜¸í™˜ë˜ë„ë¡ í•©ë‹ˆë‹¤.

    Note: Graphiti uses 'Entity' and 'Episodic' labels (NOT 'EntityNode' or 'EpisodicNode')

    ì´ ì‘ì—…ì€ ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    1. Entityì— PKM íƒ€ì… ë ˆì´ë¸” ì¶”ê°€ (Topic, Project, Task, Person)
    2. Episodic-Entity MENTIONS ê´€ê³„ë¥¼ Note-Entity MENTIONSë¡œ ë³€í™˜
    """
    try:
        from app.services.hybrid_graphiti_service import migrate_graphiti_to_hybrid

        logger.info(f"Starting Graphiti â†’ Hybrid migration (vault: {vault_id or 'all'})")

        result = await migrate_graphiti_to_hybrid(
            vault_id=vault_id,
            max_iterations=max_iterations
        )

        return {
            "status": "success",
            "migration_result": result
        }

    except Exception as e:
        logger.error(f"Migration error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/debug/entity-nodes")
async def get_entity_node_stats() -> Dict[str, Any]:
    """
    Graphiti Entity í†µê³„ ì¡°íšŒ

    Entity (Graphiti) ì¤‘ PKM ë ˆì´ë¸”ì´ ìˆëŠ” ê²ƒê³¼ ì—†ëŠ” ê²ƒì˜ ìˆ˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    Note: Graphiti uses 'Entity' label (NOT 'EntityNode')
    """
    try:
        client = get_neo4j_client()

        # ëª¨ë“  ë…¸ë“œ ë ˆì´ë¸” ì¡°íšŒ
        all_labels = client.query("CALL db.labels() YIELD label RETURN label ORDER BY label", {})

        # Entity ì „ì²´ ìˆ˜ (Graphiti uses 'Entity' label)
        total = client.query("MATCH (e:Entity) RETURN count(e) as count", {})

        # Episodic ë…¸ë“œ ìˆ˜ (Graphiti episodes)
        episodic_count = client.query("MATCH (e:Episodic) RETURN count(e) as count", {})

        # PKM ë ˆì´ë¸”ì´ ìˆëŠ” Entity
        with_pkm = client.query("""
            MATCH (e:Entity)
            WHERE e:Topic OR e:Project OR e:Task OR e:Person
            RETURN count(e) as count
        """, {})

        # PKM ë ˆì´ë¸”ì´ ì—†ëŠ” Entity
        without_pkm = client.query("""
            MATCH (e:Entity)
            WHERE NOT e:Topic AND NOT e:Project AND NOT e:Task AND NOT e:Person
            RETURN count(e) as count
        """, {})

        # PKM íƒ€ì…ë³„ í†µê³„
        by_type = client.query("""
            MATCH (e:Entity)
            WHERE e:Topic OR e:Project OR e:Task OR e:Person
            WITH CASE
                WHEN e:Topic THEN 'Topic'
                WHEN e:Project THEN 'Project'
                WHEN e:Task THEN 'Task'
                WHEN e:Person THEN 'Person'
            END as pkm_type
            RETURN pkm_type, count(*) as count
        """, {})

        # Note -> Entity MENTIONS ê´€ê³„ ìˆ˜
        note_entity_mentions = client.query("""
            MATCH (n:Note)-[m:MENTIONS]->(e:Entity)
            RETURN count(m) as count
        """, {})

        # Episodic -> Entity MENTIONS ê´€ê³„ ìˆ˜ (Graphiti)
        episodic_entity_mentions = client.query("""
            MATCH (ep:Episodic)-[m:MENTIONS]->(e:Entity)
            RETURN count(m) as count
        """, {})

        return {
            "all_labels": [r["label"] for r in (all_labels or [])],
            "total_entities": total[0]["count"] if total else 0,
            "total_episodic": episodic_count[0]["count"] if episodic_count else 0,
            "with_pkm_labels": with_pkm[0]["count"] if with_pkm else 0,
            "without_pkm_labels": without_pkm[0]["count"] if without_pkm else 0,
            "by_pkm_type": {r["pkm_type"]: r["count"] for r in (by_type or [])},
            "note_entity_mentions": note_entity_mentions[0]["count"] if note_entity_mentions else 0,
            "episodic_entity_mentions": episodic_entity_mentions[0]["count"] if episodic_entity_mentions else 0
        }

    except Exception as e:
        logger.error(f"Entity stats error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/vault/entities")
async def get_vault_entity_graph(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    limit: int = Query(200, description="Maximum entities to return", ge=10, le=1000),
    min_connections: int = Query(1, description="Minimum RELATES_TO connections", ge=0),
    include_notes: bool = Query(False, description="Include connected notes")
) -> Dict[str, Any]:
    """
    Entity Graph ì‹œê°í™”ë¥¼ ìœ„í•œ ë°ì´í„° ë°˜í™˜

    í´ëŸ¬ìŠ¤í„° ëŒ€ì‹  Entity ë…¸ë“œì™€ RELATES_TO ê´€ê³„ë¥¼ ì§ì ‘ ë°˜í™˜í•˜ì—¬
    ì§„ì •í•œ Knowledge Graph ì‹œê°í™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

    - Entity = ë…¸ë“œ (íƒ€ì…ë³„ ìƒ‰ìƒ: Topic=íŒŒë‘, Project=ì´ˆë¡, Person=ì£¼í™©, Task=ë¹¨ê°•)
    - RELATES_TO = ì—£ì§€ (ì˜ë¯¸ë¡ ì  ì—°ê²°)
    """
    try:
        client = get_neo4j_client()

        # Step 1: Entityë“¤ ì¡°íšŒ (RELATES_TO ê´€ê³„ê°€ ìˆëŠ” ê²ƒ)
        # GraphitiëŠ” Episodic-[:MENTIONS]->Entity êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ
        # Entityë¥¼ ì§ì ‘ ì¡°íšŒí•˜ê³  RELATES_TO ê´€ê³„ë¡œ í•„í„°ë§
        cypher_entities = """
        MATCH (e:Entity)
        OPTIONAL MATCH (e)-[r:RELATES_TO]-(other:Entity)
        WITH e, count(r) as connection_count
        WHERE connection_count >= $min_connections
        RETURN
            e.uuid as id,
            e.name as name,
            e.summary as summary,
            CASE
                WHEN e:Person THEN 'Person'
                WHEN e:Project THEN 'Project'
                WHEN e:Task THEN 'Task'
                ELSE 'Topic'
            END as type,
            connection_count
        ORDER BY connection_count DESC
        LIMIT $limit
        """

        entities = client.query(cypher_entities, {
            "vault_id": vault_id,
            "min_connections": min_connections,
            "limit": limit
        })

        if not entities:
            return {
                "status": "success",
                "node_count": 0,
                "edge_count": 0,
                "nodes": [],
                "edges": [],
                "stats": {"by_type": {}}
            }

        entity_ids = [e["id"] for e in entities]

        # Step 2: Entity ê°„ RELATES_TO ê´€ê³„ ì¡°íšŒ
        cypher_relations = """
        MATCH (e1:Entity)-[r:RELATES_TO]->(e2:Entity)
        WHERE e1.uuid IN $entity_ids AND e2.uuid IN $entity_ids
        RETURN DISTINCT
            e1.uuid as source,
            e2.uuid as target,
            type(r) as rel_type,
            r.fact as fact
        """

        relations = client.query(cypher_relations, {"entity_ids": entity_ids})

        # Step 3: ë…¸ë“œ ë°ì´í„° êµ¬ì„±
        nodes = []
        type_colors = {
            "Topic": "#3498db",    # íŒŒë‘
            "Project": "#2ecc71",  # ì´ˆë¡
            "Person": "#e67e22",   # ì£¼í™©
            "Task": "#e74c3c"      # ë¹¨ê°•
        }

        for e in entities:
            node = {
                "id": e["id"],
                "label": e["name"] or e["id"][:20],
                "type": e["type"],
                "color": type_colors.get(e["type"], "#95a5a6"),
                "size": min(30, 10 + e["connection_count"] * 2),  # ì—°ê²° ìˆ˜ì— ë”°ë¼ í¬ê¸°
                "summary": e.get("summary", ""),
                "connections": e["connection_count"]
            }
            nodes.append(node)

        # Step 4: ì—£ì§€ ë°ì´í„° êµ¬ì„±
        edges = []
        for r in (relations or []):
            edge = {
                "source": r["source"],
                "target": r["target"],
                "type": r["rel_type"],
                "label": r.get("fact", "")[:50] if r.get("fact") else ""
            }
            edges.append(edge)

        # Step 5: ì—°ê²°ëœ ë…¸íŠ¸ ì •ë³´ (ì„ íƒì )
        note_connections = {}
        if include_notes:
            cypher_notes = """
            MATCH (n:Note)-[:MENTIONS]->(e:Entity)
            WHERE e.uuid IN $entity_ids
            RETURN e.uuid as entity_id, collect(DISTINCT n.note_id)[..5] as note_ids
            """
            note_results = client.query(cypher_notes, {"entity_ids": entity_ids})
            for nr in (note_results or []):
                note_connections[nr["entity_id"]] = nr["note_ids"]

            # ë…¸ë“œì— note ì •ë³´ ì¶”ê°€
            for node in nodes:
                node["connected_notes"] = note_connections.get(node["id"], [])

        # íƒ€ì…ë³„ í†µê³„
        stats = {"by_type": {}}
        for e in entities:
            t = e["type"]
            stats["by_type"][t] = stats["by_type"].get(t, 0) + 1

        return {
            "status": "success",
            "node_count": len(nodes),
            "edge_count": len(edges),
            "nodes": nodes,
            "edges": edges,
            "stats": stats
        }

    except Exception as e:
        logger.error(f"Entity graph error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/vault/entity-clusters")
async def get_entity_clusters(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    folder_prefix: str = Query(None, description="í´ë” ê²½ë¡œ í•„í„° (ì˜ˆ: '1_í”„ë¡œì íŠ¸/'). í•´ë‹¹ í´ë” ë…¸íŠ¸ì˜ ì—”í‹°í‹°ë§Œ í´ëŸ¬ìŠ¤í„°ë§"),
    min_cluster_size: int = Query(3, description="Minimum entities per cluster", ge=2, le=20),
    resolution: float = Query(1.0, description="Louvain resolution (higher = more clusters)", ge=0.5, le=3.0),
    min_connections: int = Query(1, description="ìµœì†Œ ì—°ê²° ë…¸íŠ¸ ìˆ˜ (ê¸°ë³¸ 1 = ëª¨ë“  ì—”í‹°í‹° í¬í•¨, 2 = 2ê°œ ì´ìƒ ë…¸íŠ¸ì—ì„œ ì–¸ê¸‰ëœ ì—”í‹°í‹°ë§Œ)", ge=1, le=10),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
) -> Dict[str, Any]:
    """
    í•˜ì´ë¸Œë¦¬ë“œ Entity í´ëŸ¬ìŠ¤í„°ë§

    RELATES_TO ê·¸ë˜í”„ êµ¬ì¡° + name_embedding ë²¡í„° ìœ ì‚¬ë„ë¥¼ ê²°í•©í•˜ì—¬
    Entityë“¤ì„ ì˜ë¯¸ë¡ ì  í´ëŸ¬ìŠ¤í„°ë¡œ ê·¸ë£¹í•‘í•©ë‹ˆë‹¤.

    2nd Brain ì‹œê°í™”ë¥¼ ìœ„í•œ í´ëŸ¬ìŠ¤í„° ë·°:
    - ê° í´ëŸ¬ìŠ¤í„°ëŠ” ì‹œë©˜í‹±í•˜ê²Œ ìœ ì‚¬í•œ ê°œë…ë“¤ì˜ ê·¸ë£¹
    - í´ëŸ¬ìŠ¤í„° ê°„ ì—°ê²°ì€ RELATES_TO ê´€ê³„ì— ê¸°ë°˜
    - í´ëŸ¬ìŠ¤í„° ë‚´ ì—”í‹°í‹°ë“¤ì€ í¼ì³ì„œ ë³¼ ìˆ˜ ìˆìŒ

    **ì•Œê³ ë¦¬ì¦˜:**
    1. RELATES_TO ê·¸ë˜í”„ì—ì„œ Louvain ì»¤ë®¤ë‹ˆí‹° íƒì§€
    2. name_embeddingìœ¼ë¡œ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§
    3. ë‘ ê²°ê³¼ë¥¼ ë³‘í•©í•˜ì—¬ ìµœì¢… í´ëŸ¬ìŠ¤í„° ê²°ì •

    **ì‘ë‹µ ì˜ˆì‹œ:**
    ```json
    {
      "status": "success",
      "cluster_count": 15,
      "total_entities": 635,
      "clusters": [
        {
          "id": "cluster_0",
          "name": "Knowledge Graph",
          "entity_count": 42,
          "sample_entities": ["PKM", "Obsidian", "Neo4j", ...],
          "type_distribution": {"Topic": 35, "Project": 7},
          "cohesion_score": 0.85
        }
      ],
      "edges": [
        {"from": "cluster_0", "to": "cluster_1", "weight": 5}
      ]
    }
    ```
    """
    try:
        folder_info = f" for folder '{folder_prefix}'" if folder_prefix else ""
        logger.info(f"Computing entity clusters for vault {vault_id}{folder_info} (min_connections={min_connections})")

        result = compute_entity_clusters_hybrid(
            client=client,
            min_cluster_size=min_cluster_size,
            resolution=resolution,
            folder_prefix=folder_prefix,
            min_connections=min_connections
        )

        return {
            "status": "success",
            "cluster_count": len(result["clusters"]),
            "total_entities": result["total_entities"],
            "clustered_entities": result.get("clustered_entities", 0),
            "clusters": result["clusters"],
            "edges": result["edges"],
            "method": result["method"],
            "computed_at": result["computed_at"]
        }

    except Exception as e:
        logger.error(f"Entity clustering error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/vault/cleanup-orphan-entities")
async def cleanup_orphan_entities(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    dry_run: bool = Query(True, description="ë¯¸ë¦¬ë³´ê¸° ëª¨ë“œ (ì‹¤ì œ ì‚­ì œ ì•ˆí•¨)"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
) -> Dict[str, Any]:
    """
    ê³ ì•„ ì—”í‹°í‹° ì •ë¦¬

    ì–´ë–¤ Noteì—ë„ ì—°ê²°ë˜ì§€ ì•Šì€ (MENTIONS ê´€ê³„ê°€ ì—†ëŠ”) ì—”í‹°í‹°ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
    dry_run=True (ê¸°ë³¸ê°’)ë©´ ì‚­ì œí•  ì—”í‹°í‹° ëª©ë¡ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    dry_run=Falseë©´ ì‹¤ì œë¡œ ì‚­ì œí•©ë‹ˆë‹¤.

    ì°¸ê³ : ë‹¨ì¼ ë…¸íŠ¸ ì—°ê²° ì—”í‹°í‹°ëŠ” ì‚­ì œí•˜ì§€ ì•ŠìŒ (ë‚˜ì¤‘ì— ì»¤ì§ˆ ìˆ˜ ìˆìŒ)
    """
    try:
        # ê³ ì•„ Entity ì¡°íšŒ (MENTIONS ê´€ê³„ê°€ ì „í˜€ ì—†ëŠ”)
        cypher_orphan_entities = """
        MATCH (e:Entity)
        WHERE NOT (e)<-[:MENTIONS]-(:Note)
          AND NOT (e)<-[:MENTIONS]-(:Episodic)
        RETURN e.uuid as uuid, e.name as name
        """
        orphan_entities = client.query(cypher_orphan_entities, {})

        if dry_run:
            return {
                "status": "preview",
                "message": f"Found {len(orphan_entities or [])} orphan entities. Set dry_run=false to delete.",
                "orphan_count": len(orphan_entities or []),
                "sample_entities": [{"uuid": e["uuid"], "name": e["name"]} for e in (orphan_entities or [])[:50]]
            }

        # ì‹¤ì œ ì‚­ì œ ì‹¤í–‰
        cypher_cleanup_orphans = """
        MATCH (e:Entity)
        WHERE NOT (e)<-[:MENTIONS]-(:Note)
          AND NOT (e)<-[:MENTIONS]-(:Episodic)
        WITH e
        OPTIONAL MATCH (e)-[r]-()
        DELETE r, e
        RETURN count(e) as count
        """
        cleanup_result = client.query(cypher_cleanup_orphans, {})
        deleted_entities = cleanup_result[0]["count"] if cleanup_result else 0

        logger.info(f"ğŸ§¹ Cleaned up {deleted_entities} orphan entities")

        return {
            "status": "success",
            "message": f"Cleaned up {deleted_entities} orphan entities",
            "deleted_entities": deleted_entities
        }

    except Exception as e:
        logger.error(f"Cleanup error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/debug/entity-relations")
async def debug_entity_relations(
    client: Neo4jBoltClient = Depends(get_neo4j_client)
) -> Dict[str, Any]:
    """
    ë””ë²„ê·¸ìš©: Entity ê°„ ê´€ê³„ íƒ€ì… í™•ì¸
    """
    try:
        # ëª¨ë“  Entity ê°„ ê´€ê³„ íƒ€ì… ì¡°íšŒ
        cypher = """
        MATCH (e1:Entity)-[r]->(e2:Entity)
        RETURN type(r) as rel_type, count(*) as count
        ORDER BY count DESC
        LIMIT 20
        """
        results = client.query(cypher, {})

        # ìƒ˜í”Œ ê´€ê³„ ì¡°íšŒ
        cypher_sample = """
        MATCH (e1:Entity)-[r]->(e2:Entity)
        RETURN e1.name as from_name, type(r) as rel_type, e2.name as to_name,
               r.fact as fact
        LIMIT 10
        """
        samples = client.query(cypher_sample, {})

        return {
            "status": "success",
            "relation_types": [{"type": r["rel_type"], "count": r["count"]} for r in results or []],
            "samples": [
                {
                    "from": r["from_name"],
                    "type": r["rel_type"],
                    "to": r["to_name"],
                    "fact": r.get("fact", "")
                }
                for r in samples or []
            ]
        }
    except Exception as e:
        logger.error(f"Debug error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


class EntityClusterDetailRequest(BaseModel):
    """í´ëŸ¬ìŠ¤í„° ìƒì„¸ ì¡°íšŒ ìš”ì²­"""
    cluster_name: str
    entity_uuids: List[str]


@router.post("/vault/entity-clusters/detail")
async def get_entity_cluster_detail_by_uuids(
    request: EntityClusterDetailRequest,
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
) -> Dict[str, Any]:
    """
    í´ëŸ¬ìŠ¤í„° ì—”í‹°í‹° ëª©ë¡ìœ¼ë¡œ ìƒì„¸ ì •ë³´ ì¡°íšŒ

    í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì €ì¥í•œ entity_uuidsë¥¼ ì§ì ‘ ì „ë‹¬ë°›ì•„ ìƒì„¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ì˜ ì¼ê´€ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
    """
    try:
        uuids = request.entity_uuids
        cluster_name = request.cluster_name

        if not uuids:
            raise HTTPException(status_code=400, detail="entity_uuids is required")

        # ì—”í‹°í‹° ìƒì„¸ ì •ë³´ ì¡°íšŒ
        cypher = """
        MATCH (e:Entity)
        WHERE e.uuid IN $uuids
        OPTIONAL MATCH (e)-[r:RELATES_TO]-(other:Entity)
        WHERE other.uuid IN $uuids
        RETURN e.uuid as uuid,
               e.name as name,
               e.summary as summary,
               e.pkm_type as pkm_type,
               count(r) as internal_connections
        ORDER BY internal_connections DESC
        """

        results = client.query(cypher, {"uuids": uuids})

        entities = []
        type_distribution = {}
        for row in results or []:
            pkm_type = row.get("pkm_type", "Topic")
            entities.append({
                "uuid": row["uuid"],
                "name": row["name"],
                "summary": row.get("summary", ""),
                "pkm_type": pkm_type,
                "connections": row.get("internal_connections", 0)
            })
            type_distribution[pkm_type] = type_distribution.get(pkm_type, 0) + 1

        # ë‚´ë¶€ ê´€ê³„ë“¤ + PKM Semantic Edge Type ì¶”ë¡ 
        # PKM Type ì¡°í•©ìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” ê´€ê³„ ìœ í˜• ìë™ ë¶„ë¥˜
        cypher_edges = """
        MATCH (e1:Entity)-[r]->(e2:Entity)
        WHERE e1.uuid IN $uuids AND e2.uuid IN $uuids
        RETURN e1.uuid as from_uuid,
               e1.name as from_name,
               e1.pkm_type as from_type,
               e2.uuid as to_uuid,
               e2.name as to_name,
               e2.pkm_type as to_type,
               type(r) as rel_type,
               r.fact as fact,
               COALESCE(r.weight, 1.0) as weight
        """

        edge_results = client.query(cypher_edges, {"uuids": uuids})

        edges = []
        for row in edge_results or []:
            from_type = row.get("from_type") or "Topic"
            to_type = row.get("to_type") or "Topic"
            fact = row.get("fact", "")

            # PKM Type ê¸°ë°˜ Semantic Edge Type ì¶”ë¡ 
            semantic_info = infer_semantic_edge_type(from_type, to_type, fact)

            edges.append({
                "from": row["from_uuid"],
                "from_name": row.get("from_name", ""),
                "from_type": from_type,
                "to": row["to_uuid"],
                "to_name": row.get("to_name", ""),
                "to_type": to_type,
                "type": row.get("rel_type", "RELATES_TO"),
                "fact": fact,
                "weight": row.get("weight", 1.0),
                # Semantic Edge ì •ë³´ (PKM Type ê¸°ë°˜ ì¶”ë¡ )
                "semantic_type": semantic_info["edge_type"],
                "semantic_label": semantic_info["edge_label"],
                "semantic_description": semantic_info["description"]
            })

        # ê´€ë ¨ ë…¸íŠ¸ ì¡°íšŒ (ì—”í‹°í‹°ë“¤ì´ MENTIONSëœ ë…¸íŠ¸ë“¤)
        cypher_notes = """
        MATCH (n:Note)-[:MENTIONS]->(e:Entity)
        WHERE e.uuid IN $uuids
        RETURN n.note_id as note_id,
               n.title as title,
               n.path as path,
               collect(DISTINCT e.uuid) as entity_uuids,
               collect(DISTINCT e.name) as entity_names
        ORDER BY size(entity_uuids) DESC
        LIMIT 50
        """

        note_results = client.query(cypher_notes, {"uuids": uuids})

        related_notes = []
        for row in note_results or []:
            related_notes.append({
                "note_id": row["note_id"],
                "title": row.get("title", row["note_id"]),
                "path": row.get("path", ""),
                "entity_uuids": row.get("entity_uuids", []),
                "entity_names": row.get("entity_names", []),
                "entity_count": len(row.get("entity_uuids", []))
            })

        # Semantic edge í†µê³„
        semantic_edge_types = {}
        for edge in edges:
            stype = edge.get("semantic_type", "RELATES_TO")
            semantic_edge_types[stype] = semantic_edge_types.get(stype, 0) + 1

        return {
            "status": "success",
            "cluster": {
                "name": cluster_name,
                "entity_count": len(entities),
                "entity_uuids": uuids,
                "type_distribution": type_distribution,
                "entities": entities,
                "internal_edges": edges,
                "related_notes": related_notes,
                # Semantic Edge ë©”íƒ€ë°ì´í„°
                "has_semantic_edges": True,
                "semantic_edge_count": len(edges),
                "semantic_edge_types": semantic_edge_types
            }
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Cluster detail error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/vault/entity-note-graph")
async def get_entity_note_graph(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    folder_prefix: str = Query(None, description="í´ë” ê²½ë¡œ í•„í„°"),
    limit: int = Query(100, description="ìµœëŒ€ ì—”í‹°í‹° ìˆ˜", ge=10, le=500),
    min_note_connections: int = Query(2, description="ìµœì†Œ ë…¸íŠ¸ ì—°ê²° ìˆ˜ (2 = 2ê°œ ì´ìƒ ë…¸íŠ¸ì—ì„œ ì–¸ê¸‰ëœ ì—”í‹°í‹°ë§Œ)", ge=1),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
) -> Dict[str, Any]:
    """
    Entity-Note ì—°ê²° ê·¸ë˜í”„ (2nd Brain ì‹œê°í™”ìš©)

    Entityë¥¼ í†µí•´ Note ê°„ ì—°ê²°ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:
    - Entityê°€ 2ê°œ ì´ìƒì˜ Noteì—ì„œ MENTIONSë˜ë©´, ê·¸ Noteë“¤ì€ "ì—°ê²°ëœ" ê²ƒ
    - Entity ë…¸ë“œ + Note ë…¸ë“œë¥¼ í•¨ê»˜ ì‹œê°í™”
    - Note ê°„ ì‹¤ì œ ì—°ê²°ì„± íŒŒì•… ê°€ëŠ¥

    **ì‘ë‹µ ì˜ˆì‹œ:**
    ```json
    {
      "entities": [
        {"id": "uuid", "name": "PKM", "type": "Topic", "connected_notes": ["note1.md", "note2.md"]}
      ],
      "notes": [
        {"id": "note1.md", "title": "Note 1", "connected_entities": ["uuid1", "uuid2"]}
      ],
      "entity_note_edges": [
        {"entity_id": "uuid", "note_id": "note1.md"}
      ],
      "note_note_edges": [
        {"from": "note1.md", "to": "note2.md", "shared_entities": ["uuid1"], "strength": 3}
      ]
    }
    ```
    """
    try:
        # í´ë” í•„í„° ì¡°ê±´
        folder_condition = "n.note_id STARTS WITH $folder_prefix AND" if folder_prefix else ""
        folder_condition2 = "n2.note_id STARTS WITH $folder_prefix AND" if folder_prefix else ""

        # Step 1: ì—¬ëŸ¬ ë…¸íŠ¸ì—ì„œ ì–¸ê¸‰ëœ ì—”í‹°í‹°ë“¤ ì¡°íšŒ
        # ë‘ ê°€ì§€ ê²½ë¡œ ì§€ì›:
        # 1. Note -[:MENTIONS]-> Entity (hybrid_graphiti_serviceê°€ ìƒì„±í•œ ì§ì ‘ ê´€ê³„)
        # 2. Episodic -[:MENTIONS]-> Entity (Graphitiê°€ ìƒì„±í•œ ê´€ê³„, Note.note_id = Episodic.name)
        cypher_entities = f"""
        // ë°©ë²•1: ì§ì ‘ MENTIONS ê´€ê³„
        MATCH (n:Note)-[:MENTIONS]->(e:Entity)
        WHERE {folder_condition} e.name IS NOT NULL
        WITH e, collect(DISTINCT n.note_id) as direct_notes

        // ë°©ë²•2: Episodic í†µí•œ ì—°ê²° (UNION)
        RETURN e.uuid as uuid, e.name as name, e.summary as summary,
               CASE
                   WHEN e:Goal THEN 'Goal'
                   WHEN e:Project THEN 'Project'
                   WHEN e:Task THEN 'Task'
                   WHEN e:Concept THEN 'Concept'
                   WHEN e:Question THEN 'Question'
                   WHEN e:Insight THEN 'Insight'
                   WHEN e:Resource THEN 'Resource'
                   WHEN e:Person THEN 'Person'
                   ELSE 'Topic'
               END as type,
               direct_notes as note_ids,
               size(direct_notes) as note_count

        UNION

        // Episodic ê¸°ë°˜ ì—°ê²° (Episodic.name = 'note_' + Note.note_id)
        MATCH (ep:Episodic)-[:MENTIONS]->(e:Entity)
        WHERE e.name IS NOT NULL AND ep.name IS NOT NULL AND ep.name STARTS WITH 'note_'
        WITH e, ep, replace(ep.name, 'note_', '') as derived_note_id
        MATCH (n2:Note)
        WHERE {folder_condition2} n2.note_id = derived_note_id
        WITH e, collect(DISTINCT n2.note_id) as episodic_notes
        WHERE size(episodic_notes) > 0
        RETURN e.uuid as uuid, e.name as name, e.summary as summary,
               CASE
                   WHEN e:Goal THEN 'Goal'
                   WHEN e:Project THEN 'Project'
                   WHEN e:Task THEN 'Task'
                   WHEN e:Concept THEN 'Concept'
                   WHEN e:Question THEN 'Question'
                   WHEN e:Insight THEN 'Insight'
                   WHEN e:Resource THEN 'Resource'
                   WHEN e:Person THEN 'Person'
                   ELSE 'Topic'
               END as type,
               episodic_notes as note_ids,
               size(episodic_notes) as note_count
        """

        params = {
            "folder_prefix": folder_prefix or "",
            "min_note_connections": min_note_connections,
            "limit": limit
        }

        entities_result = client.query(cypher_entities, params)

        if not entities_result:
            return {
                "status": "success",
                "entity_count": 0,
                "note_count": 0,
                "entities": [],
                "notes": [],
                "entity_note_edges": [],
                "note_note_edges": [],
                "insights": {}
            }

        # UNION ê²°ê³¼ì—ì„œ ê°™ì€ ì—”í‹°í‹°ì˜ note_idsë¥¼ í•©ì³ì•¼ í•¨
        entity_map = {}  # uuid -> {name, summary, type, note_ids}
        for row in entities_result:
            uuid = row["uuid"]
            if uuid not in entity_map:
                entity_map[uuid] = {
                    "name": row["name"],
                    "summary": row.get("summary", ""),
                    "type": row["type"],
                    "note_ids": set(row["note_ids"] or [])
                }
            else:
                # ê¸°ì¡´ ì—”í‹°í‹°ì— note_ids ì¶”ê°€
                entity_map[uuid]["note_ids"].update(row["note_ids"] or [])

        # min_note_connections í•„í„° ì ìš© ë° ì •ë ¬
        filtered_entities = [
            (uuid, data) for uuid, data in entity_map.items()
            if len(data["note_ids"]) >= min_note_connections
        ]
        # note_count ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ limit ì ìš©
        filtered_entities.sort(key=lambda x: len(x[1]["note_ids"]), reverse=True)
        filtered_entities = filtered_entities[:limit]

        # Entity ë°ì´í„° êµ¬ì„±
        entities = []
        all_note_ids = set()
        entity_note_edges = []

        # PKM Core Ontology v2 - 8ê°œ íƒ€ì… + Person ìƒ‰ìƒ
        type_colors = {
            "Goal": "#9b59b6",      # ë³´ë¼ìƒ‰ - ìµœìƒìœ„ ëª©í‘œ
            "Project": "#2ecc71",   # ì´ˆë¡ìƒ‰ - í”„ë¡œì íŠ¸
            "Task": "#e74c3c",      # ë¹¨ê°„ìƒ‰ - íƒœìŠ¤í¬
            "Topic": "#3498db",     # íŒŒë€ìƒ‰ - ì£¼ì œ
            "Concept": "#1abc9c",   # ì²­ë¡ìƒ‰ - ê°œë…
            "Question": "#f39c12",  # ì£¼í™©ìƒ‰ - ì§ˆë¬¸
            "Insight": "#e91e63",   # ë¶„í™ìƒ‰ - ì¸ì‚¬ì´íŠ¸
            "Resource": "#607d8b",  # íšŒìƒ‰ - ìë£Œ
            "Person": "#e67e22",    # ì˜¤ë Œì§€ìƒ‰ - ì¸ë¬¼ (í•˜ìœ„í˜¸í™˜)
        }

        for uuid, data in filtered_entities:
            note_ids_list = list(data["note_ids"])
            entity = {
                "id": uuid,
                "name": data["name"] or "Unknown",
                "summary": data["summary"],
                "type": data["type"],
                "color": type_colors.get(data["type"], "#95a5a6"),
                "connected_notes": note_ids_list,
                "note_count": len(note_ids_list)
            }
            entities.append(entity)

            # Entity-Note ì—£ì§€ ì¶”ê°€
            for note_id in note_ids_list:
                all_note_ids.add(note_id)
                entity_note_edges.append({
                    "entity_id": uuid,
                    "note_id": note_id
                })

        # Step 2: Note ë°ì´í„° ì¡°íšŒ
        notes = []
        if all_note_ids:
            cypher_notes = """
            MATCH (n:Note)
            WHERE n.note_id IN $note_ids
            RETURN n.note_id as note_id,
                   n.title as title,
                   n.path as path
            """
            notes_result = client.query(cypher_notes, {"note_ids": list(all_note_ids)})

            for row in notes_result or []:
                notes.append({
                    "id": row["note_id"],
                    "title": row.get("title") or row["note_id"].split("/")[-1].replace(".md", ""),
                    "path": row.get("path", row["note_id"])
                })

        # Step 3: Note-Note ì—°ê²° ê³„ì‚° (ê³µìœ  Entity ê¸°ë°˜)
        note_note_edges = []
        note_shared_entities = {}  # {(note1, note2): [entity_ids]}

        for entity in entities:
            note_list = entity["connected_notes"]
            if len(note_list) >= 2:
                # ëª¨ë“  ë…¸íŠ¸ ìŒì— ëŒ€í•´ ê³µìœ  Entity ê¸°ë¡
                for i in range(len(note_list)):
                    for j in range(i + 1, len(note_list)):
                        pair = tuple(sorted([note_list[i], note_list[j]]))
                        if pair not in note_shared_entities:
                            note_shared_entities[pair] = []
                        note_shared_entities[pair].append(entity["id"])

        for (note1, note2), shared_entities in note_shared_entities.items():
            note_note_edges.append({
                "from": note1,
                "to": note2,
                "shared_entities": shared_entities,
                "strength": len(shared_entities)
            })

        # ì—°ê²° ê°•ë„ìˆœ ì •ë ¬
        note_note_edges.sort(key=lambda x: x["strength"], reverse=True)

        return {
            "status": "success",
            "entity_count": len(entities),
            "note_count": len(notes),
            "edge_count": len(note_note_edges),
            "entities": entities,
            "notes": notes,
            "entity_note_edges": entity_note_edges,
            "note_note_edges": note_note_edges[:200]  # ìƒìœ„ 200ê°œ ì—°ê²°ë§Œ
        }

    except Exception as e:
        logger.error(f"Entity-Note graph error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/vault/thinking-insights")
async def get_thinking_insights(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    folder_prefix: str = Query(None, description="í´ë” ê²½ë¡œ í•„í„°"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
) -> Dict[str, Any]:
    """
    ì‚¬ê³  íŒ¨í„´ ì¸ì‚¬ì´íŠ¸ (Palantir Foundry ìŠ¤íƒ€ì¼)

    ë…¸íŠ¸ì™€ ì—”í‹°í‹° ë¶„ì„ì„ í†µí•´:
    1. **ì§‘ì¤‘ ì˜ì—­**: ê°€ì¥ ë§ì´ ì–¸ê¸‰ë˜ëŠ” ì£¼ì œë“¤
    2. **ë¸Œë¦¿ì§€ ê°œë…**: ì—¬ëŸ¬ ì˜ì—­ì„ ì—°ê²°í•˜ëŠ” í•µì‹¬ ì—”í‹°í‹°
    3. **ê³ ë¦½ëœ ì˜ì—­**: ë‹¤ë¥¸ ê²ƒê³¼ ì—°ê²°ì´ ì ì€ ì£¼ì œë“¤
    4. **ì„±ì¥ ì˜ì—­**: ìµœê·¼ í™œë°œí•˜ê²Œ í™•ì¥ë˜ëŠ” ì£¼ì œë“¤
    5. **íƒêµ¬ ì œì•ˆ**: ì—°ê²°ì„ ê°•í™”í•  ìˆ˜ ìˆëŠ” ì˜ì—­

    **ì‘ë‹µ ì˜ˆì‹œ:**
    ```json
    {
      "focus_areas": [
        {"name": "Knowledge Graph", "strength": 15, "trend": "growing"}
      ],
      "bridge_concepts": [
        {"name": "PKM", "connects": ["Research", "Development", "Writing"], "importance": 0.9}
      ],
      "isolated_areas": [
        {"name": "Old Project", "connection_count": 1, "suggestion": "ì—°ê²°í•  ì£¼ì œ ê²€í† "}
      ],
      "growth_areas": [
        {"name": "AI", "recent_notes": 5, "growth_rate": 2.5}
      ],
      "exploration_suggestions": [
        {"area1": "PKM", "area2": "AI", "potential": "ë†’ìŒ", "reason": "ê³µí†µ ê´€ì‹¬ì‚¬ ë°œê²¬"}
      ]
    }
    ```
    """
    try:
        folder_condition = "n.note_id STARTS WITH $folder_prefix AND" if folder_prefix else ""
        params = {"folder_prefix": folder_prefix or ""}

        # 1. ì§‘ì¤‘ ì˜ì—­ (Focus Areas): ê°€ì¥ ë§ì´ ì–¸ê¸‰ë˜ëŠ” ì—”í‹°í‹°
        cypher_focus = f"""
        MATCH (n:Note)-[:MENTIONS]->(e:Entity)
        WHERE {folder_condition} e.name IS NOT NULL
        WITH e, count(DISTINCT n) as mention_count, collect(DISTINCT n.note_id) as notes
        WHERE mention_count >= 2
        RETURN e.uuid as uuid,
               e.name as name,
               CASE WHEN e:Goal THEN 'Goal'
                    WHEN e:Project THEN 'Project'
                    WHEN e:Task THEN 'Task'
                    WHEN e:Concept THEN 'Concept'
                    WHEN e:Question THEN 'Question'
                    WHEN e:Insight THEN 'Insight'
                    WHEN e:Resource THEN 'Resource'
                    WHEN e:Person THEN 'Person'
                    ELSE 'Topic' END as type,
               mention_count,
               notes
        ORDER BY mention_count DESC
        LIMIT 15
        """

        focus_result = client.query(cypher_focus, params)
        focus_areas = []
        for row in focus_result or []:
            focus_areas.append({
                "uuid": row["uuid"],
                "name": row["name"],
                "type": row["type"],
                "strength": row["mention_count"],
                "notes": row["notes"][:5]  # ìƒìœ„ 5ê°œ ë…¸íŠ¸
            })

        # 2. ë¸Œë¦¿ì§€ ê°œë… (Bridge Concepts): ì—¬ëŸ¬ ë‹¤ë¥¸ ì—”í‹°í‹°ë“¤ì„ ì—°ê²°í•˜ëŠ” í—ˆë¸Œ
        cypher_bridge = f"""
        MATCH (n:Note)-[:MENTIONS]->(e1:Entity)
        WHERE {folder_condition} e1.name IS NOT NULL
        WITH n, e1
        MATCH (n)-[:MENTIONS]->(e2:Entity)
        WHERE e2.uuid <> e1.uuid AND e2.name IS NOT NULL
        WITH e1, count(DISTINCT e2) as connected_entities, collect(DISTINCT e2.name)[..5] as connections
        WHERE connected_entities >= 3
        RETURN e1.uuid as uuid,
               e1.name as name,
               connected_entities,
               connections
        ORDER BY connected_entities DESC
        LIMIT 10
        """

        bridge_result = client.query(cypher_bridge, params)
        bridge_concepts = []
        for row in bridge_result or []:
            bridge_concepts.append({
                "uuid": row["uuid"],
                "name": row["name"],
                "connected_count": row["connected_entities"],
                "connects": row["connections"],
                "importance": min(1.0, row["connected_entities"] / 10)
            })

        # 3. ê³ ë¦½ëœ ì˜ì—­ (Isolated Areas): ì—°ê²°ì´ ì ì€ ì—”í‹°í‹°
        cypher_isolated = f"""
        MATCH (n:Note)-[:MENTIONS]->(e:Entity)
        WHERE {folder_condition} e.name IS NOT NULL
        WITH e, count(DISTINCT n) as note_count
        WHERE note_count = 1
        OPTIONAL MATCH (e)-[r:RELATES_TO]-()
        WITH e, note_count, count(r) as relation_count
        WHERE relation_count <= 1
        RETURN e.uuid as uuid,
               e.name as name,
               note_count,
               relation_count
        LIMIT 15
        """

        isolated_result = client.query(cypher_isolated, params)
        isolated_areas = []
        for row in isolated_result or []:
            isolated_areas.append({
                "uuid": row["uuid"],
                "name": row["name"],
                "note_count": row["note_count"],
                "relation_count": row["relation_count"],
                "suggestion": "ë‹¤ë¥¸ ì£¼ì œì™€ ì—°ê²° ê²€í† "
            })

        # 4. íƒ€ì…ë³„ ë¶„í¬ (PKM Core Ontology v2 - 8ê°œ íƒ€ì… + Person)
        cypher_distribution = f"""
        MATCH (n:Note)-[:MENTIONS]->(e:Entity)
        WHERE {folder_condition} e.name IS NOT NULL
        WITH CASE WHEN e:Goal THEN 'Goal'
                  WHEN e:Project THEN 'Project'
                  WHEN e:Task THEN 'Task'
                  WHEN e:Concept THEN 'Concept'
                  WHEN e:Question THEN 'Question'
                  WHEN e:Insight THEN 'Insight'
                  WHEN e:Resource THEN 'Resource'
                  WHEN e:Person THEN 'Person'
                  ELSE 'Topic' END as type,
             count(DISTINCT e) as entity_count,
             count(DISTINCT n) as note_count
        RETURN type, entity_count, note_count
        ORDER BY entity_count DESC
        """

        dist_result = client.query(cypher_distribution, params)
        type_distribution = {}
        for row in dist_result or []:
            type_distribution[row["type"]] = {
                "entity_count": row["entity_count"],
                "note_count": row["note_count"]
            }

        # 5. íƒêµ¬ ì œì•ˆ: ì—°ê²°ì´ ì•½í•˜ì§€ë§Œ ì ì¬ì  ì—°ê²° ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì˜ì—­
        exploration_suggestions = []
        if len(focus_areas) >= 2:
            # ìƒìœ„ ì§‘ì¤‘ ì˜ì—­ ì¤‘ ì§ì ‘ ì—°ê²°ì´ ì—†ëŠ” ìŒ ì°¾ê¸°
            top_entities = [f["uuid"] for f in focus_areas[:8]]

            cypher_unconnected = """
            MATCH (e1:Entity), (e2:Entity)
            WHERE e1.uuid IN $uuids AND e2.uuid IN $uuids
              AND e1.uuid < e2.uuid
              AND NOT (e1)-[:RELATES_TO]-(e2)
            RETURN e1.name as name1, e2.name as name2
            LIMIT 5
            """

            unconnected = client.query(cypher_unconnected, {"uuids": top_entities})
            for row in unconnected or []:
                exploration_suggestions.append({
                    "area1": row["name1"],
                    "area2": row["name2"],
                    "potential": "ë†’ìŒ",
                    "reason": "ë‘˜ ë‹¤ ì§‘ì¤‘ ì˜ì—­ì´ì§€ë§Œ ì§ì ‘ ì—°ê²° ì—†ìŒ"
                })

        # 6. ì‹œê°„ ê¸°ë°˜ íŠ¸ë Œë“œ (Time-based Trends)
        # ìµœê·¼ 7ì¼ vs 30ì¼ í† í”½ ë¹„êµ
        cypher_time_trends = f"""
        MATCH (n:Note)-[:MENTIONS]->(e:Entity)
        WHERE {folder_condition} e.name IS NOT NULL AND n.updated_at IS NOT NULL
        WITH e,
             sum(CASE WHEN n.updated_at >= datetime() - duration('P7D') THEN 1 ELSE 0 END) as recent_7d,
             sum(CASE WHEN n.updated_at >= datetime() - duration('P30D') AND n.updated_at < datetime() - duration('P7D') THEN 1 ELSE 0 END) as older_30d,
             count(DISTINCT n) as total_mentions
        WHERE total_mentions >= 2
        RETURN e.name as name,
               e.uuid as uuid,
               recent_7d,
               older_30d,
               total_mentions,
               CASE
                   WHEN recent_7d > 0 AND older_30d = 0 THEN 'emerging'
                   WHEN recent_7d > older_30d THEN 'growing'
                   WHEN recent_7d < older_30d THEN 'declining'
                   ELSE 'stable'
               END as trend
        ORDER BY recent_7d DESC
        LIMIT 20
        """

        trends_result = client.query(cypher_time_trends, params)

        time_trends = {
            "recent_topics": [],      # ìµœê·¼ 7ì¼ í™œë°œ
            "emerging_topics": [],    # ìƒˆë¡œ ë“±ì¥ (7ì¼ ë‚´ ì‹ ê·œ)
            "declining_topics": [],   # ê°ì†Œ ì¶”ì„¸
            "stable_topics": [],      # ì•ˆì •ì 
            "trend_period": "7d vs 30d"
        }

        for row in trends_result or []:
            topic_info = {
                "name": row["name"],
                "uuid": row["uuid"],
                "recent_count": row["recent_7d"],
                "older_count": row["older_30d"],
                "total": row["total_mentions"]
            }

            if row["trend"] == "emerging":
                time_trends["emerging_topics"].append(topic_info)
            elif row["trend"] == "growing":
                time_trends["recent_topics"].append(topic_info)
            elif row["trend"] == "declining":
                time_trends["declining_topics"].append(topic_info)
            else:
                time_trends["stable_topics"].append(topic_info)

        # ìƒìœ„ 5ê°œì”©ë§Œ ìœ ì§€
        time_trends["recent_topics"] = time_trends["recent_topics"][:5]
        time_trends["emerging_topics"] = time_trends["emerging_topics"][:5]
        time_trends["declining_topics"] = time_trends["declining_topics"][:5]
        time_trends["stable_topics"] = time_trends["stable_topics"][:5]

        # 7. Knowledge Health Score (ì§€ì‹ ê±´ê°•ë„)
        # ì—°ê²° ë°€ë„, ê³ ë¦½ ë¹„ìœ¨, ì™„ì„±ë„ ê³„ì‚°

        # 7-1. ì „ì²´ ë…¸íŠ¸ ìˆ˜ì™€ ì—°ê²°ëœ ë…¸íŠ¸ ìˆ˜
        cypher_health_notes = f"""
        MATCH (n:Note)
        WHERE {folder_condition.replace('AND', '') if folder_condition else '1=1'}
        WITH count(n) as total_notes
        OPTIONAL MATCH (n2:Note)-[:MENTIONS]->(:Entity)
        WHERE {folder_condition.replace('AND', '') if folder_condition else '1=1'}
        WITH total_notes, count(DISTINCT n2) as connected_notes
        RETURN total_notes, connected_notes
        """
        # Simplified query for health metrics
        cypher_health_simple = f"""
        MATCH (n:Note)
        {"WHERE " + folder_condition.replace('AND', '').strip() if folder_condition else ""}
        WITH count(n) as total_notes
        MATCH (n2:Note)-[:MENTIONS]->(:Entity)
        {"WHERE " + folder_condition.replace('AND', '').strip() if folder_condition else ""}
        WITH total_notes, count(DISTINCT n2) as connected_notes
        RETURN total_notes, connected_notes
        """

        health_result = client.query(cypher_health_simple, params)
        total_notes_count = 0
        connected_notes_count = 0

        if health_result and len(health_result) > 0:
            total_notes_count = health_result[0].get("total_notes", 0) or 0
            connected_notes_count = health_result[0].get("connected_notes", 0) or 0

        # 7-2. ê³ ë¦½ ë…¸íŠ¸ ìˆ˜ (ì—”í‹°í‹° ì—°ê²° ì—†ìŒ)
        isolation_ratio = 0.0
        if total_notes_count > 0:
            isolation_ratio = 1 - (connected_notes_count / total_notes_count)

        # 7-3. ì—°ê²° ë°€ë„ (í‰ê·  ì—”í‹°í‹° ì—°ê²° ìˆ˜)
        cypher_density = f"""
        MATCH (n:Note)-[m:MENTIONS]->(:Entity)
        {"WHERE " + folder_condition.replace('AND', '').strip() if folder_condition else ""}
        WITH n, count(m) as entity_count
        RETURN avg(entity_count) as avg_connections, max(entity_count) as max_connections
        """

        density_result = client.query(cypher_density, params)
        avg_connections = 0.0
        max_connections = 0

        if density_result and len(density_result) > 0:
            avg_connections = density_result[0].get("avg_connections", 0) or 0
            max_connections = density_result[0].get("max_connections", 0) or 0

        # ì—°ê²° ë°€ë„ ì ìˆ˜ (0~1, í‰ê·  5ê°œ ì—°ê²° ê¸°ì¤€)
        connection_density = min(1.0, avg_connections / 5) if avg_connections else 0

        # ì™„ì„±ë„ ì ìˆ˜ ê³„ì‚° (ì—°ê²° ë°€ë„ì™€ ê³ ë¦½ ë¹„ìœ¨ ê¸°ë°˜)
        completeness_score = (connection_density * 0.6) + ((1 - isolation_ratio) * 0.4)

        # ì „ì²´ ê±´ê°•ë„ ì ìˆ˜ (0~100)
        overall_health = int(completeness_score * 100)

        # ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±
        health_recommendations = []
        if isolation_ratio > 0.3:
            isolated_count = int(total_notes_count * isolation_ratio)
            health_recommendations.append(f"ê³ ë¦½ ë…¸íŠ¸ {isolated_count}ê°œë¥¼ ë‹¤ë¥¸ ì£¼ì œì™€ ì—°ê²°í•˜ì„¸ìš”")
        if connection_density < 0.5:
            health_recommendations.append("ë…¸íŠ¸ì— ë” ë§ì€ ê°œë…/í† í”½ì„ ì¶”ê°€í•˜ì„¸ìš”")
        if len(isolated_areas) > 10:
            health_recommendations.append(f"ê³ ë¦½ëœ ì˜ì—­ {len(isolated_areas)}ê°œë¥¼ ê²€í† í•˜ì„¸ìš”")
        if len(focus_areas) < 3:
            health_recommendations.append("ì§‘ì¤‘ ì˜ì—­ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ë” ë§ì€ ë…¸íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”")

        if not health_recommendations:
            health_recommendations.append("ì§€ì‹ ê·¸ë˜í”„ê°€ ì˜ ê´€ë¦¬ë˜ê³  ìˆìŠµë‹ˆë‹¤!")

        health_score = {
            "overall": overall_health,
            "connection_density": round(connection_density, 2),
            "isolation_ratio": round(isolation_ratio, 2),
            "completeness_score": round(completeness_score, 2),
            "metrics": {
                "total_notes": total_notes_count,
                "connected_notes": connected_notes_count,
                "avg_connections": round(avg_connections, 1),
                "max_connections": max_connections
            },
            "recommendations": health_recommendations
        }

        # 4. ìš°ì„ ìˆœìœ„ íƒœìŠ¤í¬ (Priority Tasks): Goal/Projectì™€ ì—°ê²°ëœ íƒœìŠ¤í¬
        cypher_tasks = f"""
        MATCH (t:Entity)
        WHERE {folder_condition} t.pkm_type = 'Task'
        OPTIONAL MATCH (t)<-[:REQUIRES]-(p:Entity)
        WHERE p.pkm_type = 'Project'
        OPTIONAL MATCH (p)<-[:ACHIEVED_BY]-(g:Entity)
        WHERE g.pkm_type = 'Goal'
        WITH t, p, g, count((t)<-[]-()) as total_connections
        RETURN t.uuid as uuid,
               t.name as name,
               p.name as project,
               g.name as goal,
               total_connections
        ORDER BY 
            CASE WHEN g IS NOT NULL THEN 2 
                 WHEN p IS NOT NULL THEN 1 
                 ELSE 0 END DESC,
            total_connections DESC
        LIMIT 10
        """

        tasks_result = client.query(cypher_tasks, params)
        priority_tasks = []
        for row in tasks_result or []:
            context = ""
            if row["goal"]:
                context = f"Goal: {row['goal']}"
            elif row["project"]:
                context = f"Project: {row['project']}"
            else:
                context = "No Context"
            
            priority_tasks.append({
                "uuid": row["uuid"],
                "name": row["name"],
                "context": context,
                "importance": row["total_connections"]
            })

        # ì „ì²´ í†µê³„
        total_entities = sum(t["entity_count"] for t in type_distribution.values())
        total_notes = max(t["note_count"] for t in type_distribution.values()) if type_distribution else 0

        return {
            "status": "success",
            "summary": {
                "total_entities": total_entities,
                "total_notes": total_notes,
                "focus_count": len(focus_areas),
                "bridge_count": len(bridge_concepts),
                "isolated_count": len(isolated_areas),
                "health_score": overall_health
            },
            "type_distribution": type_distribution,
            "focus_areas": focus_areas,
            "bridge_concepts": bridge_concepts,
            "isolated_areas": isolated_areas,
            "exploration_suggestions": exploration_suggestions,
            "time_trends": time_trends,
            "health_score": health_score,
            "priority_tasks": priority_tasks
        }

    except Exception as e:
        logger.error(f"Thinking insights error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/vault/reclassify-pkm-types")
async def reclassify_pkm_types(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    batch_size: int = Query(500, description="Batch size", ge=50, le=1000),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
) -> Dict[str, Any]:
    """
    ê¸°ì¡´ Entityì˜ PKM Type ì¬ë¶„ë¥˜ (Resync í•„ìš” ì—†ìŒ!)

    ê°œì„ ëœ ë¶„ë¥˜ ë¡œì§ì„ ê¸°ì¡´ ëª¨ë“  Entityì— ì ìš©í•©ë‹ˆë‹¤.
    - Goal, Concept, Question, Insight, Resource ë¶„ë¥˜ ê°œì„ 
    - Summary ê¸°ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­ ê°•í™”
    - ì´ë¦„ íŒ¨í„´ ë¶„ì„ ì¶”ê°€

    **ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**
    - ë¶„ë¥˜ ë¡œì§ ì—…ë°ì´íŠ¸ í›„ ê¸°ì¡´ ë°ì´í„° ì¬ë¶„ë¥˜
    - ë¹ˆ íƒ€ì…(Goal=0, Concept=0 ë“±) ì±„ìš°ê¸°
    """
    try:
        from app.services.hybrid_graphiti_service import classify_entity_to_pkm_type

        # Step 1: ëª¨ë“  Entity ì¡°íšŒ
        cypher_all_entities = """
        MATCH (e:Entity)
        WHERE e.name IS NOT NULL
        RETURN e.uuid as uuid, e.name as name, e.summary as summary, e.pkm_type as current_type
        LIMIT $batch_size
        """

        entities = client.query(cypher_all_entities, {"batch_size": batch_size})

        if not entities:
            return {
                "status": "success",
                "message": "No entities to reclassify",
                "reclassified": 0
            }

        logger.info(f"Reclassifying {len(entities)} entities...")

        # Step 2: ì¬ë¶„ë¥˜ ë° ì—…ë°ì´íŠ¸
        stats = {
            "Goal": 0, "Project": 0, "Task": 0, "Topic": 0,
            "Concept": 0, "Question": 0, "Insight": 0, "Resource": 0,
            "Person": 0, "changed": 0, "unchanged": 0, "errors": 0
        }

        for entity in entities:
            try:
                uuid = entity["uuid"]
                name = entity["name"]
                summary = entity.get("summary", "")
                current_type = entity.get("current_type", "Topic")

                # ìƒˆ ë¶„ë¥˜
                new_type = classify_entity_to_pkm_type(name, summary)

                # í†µê³„ ì—…ë°ì´íŠ¸
                stats[new_type] = stats.get(new_type, 0) + 1

                if new_type != current_type:
                    stats["changed"] += 1

                    # ê¸°ì¡´ PKM ë ˆì´ë¸” ì œê±°í•˜ê³  ìƒˆ ë ˆì´ë¸” ì¶”ê°€
                    cypher_update = f"""
                    MATCH (e:Entity {{uuid: $uuid}})
                    REMOVE e:Goal, e:Project, e:Task, e:Topic, e:Concept, e:Question, e:Insight, e:Resource, e:Person
                    SET e:{new_type}
                    SET e.pkm_type = $pkm_type
                    SET e.pkm_reclassified_at = datetime()
                    RETURN e.name as name
                    """

                    client.query(cypher_update, {"uuid": uuid, "pkm_type": new_type})
                else:
                    stats["unchanged"] += 1

            except Exception as e:
                logger.error(f"Error reclassifying {entity.get('name')}: {e}")
                stats["errors"] += 1

        logger.info(f"âœ… Reclassification complete: {stats}")

        return {
            "status": "success",
            "message": f"Reclassified {len(entities)} entities",
            "total_processed": len(entities),
            "changed": stats["changed"],
            "unchanged": stats["unchanged"],
            "errors": stats["errors"],
            "type_distribution": {
                "Goal": stats["Goal"],
                "Project": stats["Project"],
                "Task": stats["Task"],
                "Topic": stats["Topic"],
                "Concept": stats["Concept"],
                "Question": stats["Question"],
                "Insight": stats["Insight"],
                "Resource": stats["Resource"],
                "Person": stats["Person"]
            }
        }

    except Exception as e:
        logger.error(f"Reclassification error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/vault/migrate-note-mentions")
async def migrate_note_mentions(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    batch_size: int = Query(100, description="Batch size", ge=10, le=500),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
) -> Dict[str, Any]:
    """
    Graphiti Episodic-Entity MENTIONS ê´€ê³„ë¥¼ Note-Entity MENTIONSë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜

    GraphitiëŠ” Episodic â†’ Entity MENTIONS ê´€ê³„ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
    2nd Brain ì‹œê°í™”ë¥¼ ìœ„í•´ì„œëŠ” Note â†’ Entity MENTIONS ê´€ê³„ê°€ í•„ìš”í•©ë‹ˆë‹¤.

    ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ê¸°ì¡´ Episodic-Entity ê´€ê³„ë¥¼ Note-Entity ê´€ê³„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    try:
        from app.services.hybrid_graphiti_service import create_mentions_from_episodes, add_pkm_labels_to_graphiti_entities

        # Step 1: PKM ë ˆì´ë¸” ì¶”ê°€
        label_result = await add_pkm_labels_to_graphiti_entities(vault_id, batch_size)

        # Step 2: Note-Entity MENTIONS ê´€ê³„ ìƒì„±
        mentions_result = await create_mentions_from_episodes(vault_id, batch_size)

        return {
            "status": "success",
            "pkm_labels": label_result,
            "mentions": mentions_result
        }

    except Exception as e:
        logger.error(f"Migration error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# =============================================================================
# Palantir-Style Bidirectional Feedback APIs
# Kinetic Layer (UI) â†” Semantic Layer (Neo4j) ì–‘ë°©í–¥ ë™ê¸°í™”
# =============================================================================

class EntityPkmTypeUpdateRequest(BaseModel):
    """Entity PKM Type ë³€ê²½ ìš”ì²­"""
    entity_uuid: str
    new_pkm_type: str  # Goal, Project, Task, Topic, Concept, Question, Insight, Resource, Person


class EntitySummaryUpdateRequest(BaseModel):
    """Entity Summary ë³€ê²½ ìš”ì²­"""
    entity_uuid: str
    new_summary: str


class BulkEntityUpdateRequest(BaseModel):
    """ì—¬ëŸ¬ Entity ì¼ê´„ ì—…ë°ì´íŠ¸ ìš”ì²­"""
    updates: List[Dict[str, str]]  # [{entity_uuid, new_pkm_type}, ...]


@router.post("/entity/update-pkm-type")
async def update_entity_pkm_type(
    request: EntityPkmTypeUpdateRequest,
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
) -> Dict[str, Any]:
    """
    ğŸ”„ Palantir-Style Bidirectional Feedback: Entity PKM Type ì—…ë°ì´íŠ¸

    ì‚¬ìš©ìê°€ UIì—ì„œ Entityì˜ PKM Typeì„ ë³€ê²½í•˜ë©´:
    1. Neo4jì˜ Entity ë…¸ë“œ ë ˆì´ë¸” ì—…ë°ì´íŠ¸
    2. pkm_type ì†ì„± ì—…ë°ì´íŠ¸
    3. user_modified_at íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë¡ (ì‚¬ìš©ì ìˆ˜ì • ì¶”ì )

    **Palantir Foundry ì² í•™:**
    - Kinetic Layer (UI ì¡°ì‘) â†’ Semantic Layer (ì§€ì‹ ê·¸ë˜í”„) ë°˜ì˜
    - ì‚¬ìš©ìì˜ ë„ë©”ì¸ ì§€ì‹ì´ ì‹œìŠ¤í…œì„ ê°œì„ 
    - í•™ìŠµ í”¼ë“œë°± ë£¨í”„ í˜•ì„±

    **ìœ íš¨í•œ PKM Type:**
    - Goal: ì¥ê¸° ëª©í‘œ, ë¹„ì „
    - Project: í”„ë¡œì íŠ¸, ê²°ê³¼ë¬¼
    - Task: í•  ì¼, ì•¡ì…˜ ì•„ì´í…œ
    - Topic: ì£¼ì œ, ë¶„ì•¼
    - Concept: ê°œë…, ë°©ë²•ë¡ 
    - Question: ì§ˆë¬¸, ì—°êµ¬ ë¬¸ì œ
    - Insight: ì¸ì‚¬ì´íŠ¸, ë°œê²¬
    - Resource: ìë£Œ, ì°¸ê³ ë¬¸í—Œ
    - Person: ì¸ë¬¼ (í•˜ìœ„í˜¸í™˜)
    """
    valid_types = ["Goal", "Project", "Task", "Topic", "Concept", "Question", "Insight", "Resource", "Person"]

    if request.new_pkm_type not in valid_types:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid PKM type. Valid types: {valid_types}"
        )

    try:
        entity_uuid = request.entity_uuid
        new_type = request.new_pkm_type

        # ê¸°ì¡´ ì—”í‹°í‹° í™•ì¸
        check_result = client.query(
            "MATCH (e:Entity {uuid: $uuid}) RETURN e.name as name, e.pkm_type as current_type",
            {"uuid": entity_uuid}
        )

        if not check_result:
            raise HTTPException(status_code=404, detail=f"Entity not found: {entity_uuid}")

        old_type = check_result[0].get("current_type", "Unknown")
        entity_name = check_result[0].get("name", "Unknown")

        # ë ˆì´ë¸” ë° ì†ì„± ì—…ë°ì´íŠ¸
        cypher_update = f"""
        MATCH (e:Entity {{uuid: $uuid}})
        REMOVE e:Goal, e:Project, e:Task, e:Topic, e:Concept, e:Question, e:Insight, e:Resource, e:Person
        SET e:{new_type}
        SET e.pkm_type = $pkm_type
        SET e.user_modified_at = datetime()
        SET e.user_modified_type = $pkm_type
        RETURN e.name as name, e.pkm_type as new_type
        """

        update_result = client.query(cypher_update, {"uuid": entity_uuid, "pkm_type": new_type})

        logger.info(f"ğŸ”„ Bidirectional update: Entity '{entity_name}' type changed {old_type} â†’ {new_type}")

        return {
            "status": "success",
            "message": f"Entity type updated: {old_type} â†’ {new_type}",
            "entity": {
                "uuid": entity_uuid,
                "name": entity_name,
                "old_type": old_type,
                "new_type": new_type
            },
            "feedback_type": "palantir_bidirectional"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Entity update error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/entity/update-summary")
async def update_entity_summary(
    request: EntitySummaryUpdateRequest,
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
) -> Dict[str, Any]:
    """
    ğŸ”„ Entity Summary ì—…ë°ì´íŠ¸ (ì‚¬ìš©ì í”¼ë“œë°±)

    Graphitiê°€ ìƒì„±í•œ Entity Summaryë¥¼ ì‚¬ìš©ìê°€ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ë” ì •í™•í•œ ì„¤ëª…ìœ¼ë¡œ ì§€ì‹ ê·¸ë˜í”„ í’ˆì§ˆì„ ê°œì„ í•©ë‹ˆë‹¤.
    """
    try:
        entity_uuid = request.entity_uuid
        new_summary = request.new_summary

        # ê¸°ì¡´ ì—”í‹°í‹° í™•ì¸
        check_result = client.query(
            "MATCH (e:Entity {uuid: $uuid}) RETURN e.name as name, e.summary as old_summary",
            {"uuid": entity_uuid}
        )

        if not check_result:
            raise HTTPException(status_code=404, detail=f"Entity not found: {entity_uuid}")

        old_summary = check_result[0].get("old_summary", "")
        entity_name = check_result[0].get("name", "Unknown")

        # Summary ì—…ë°ì´íŠ¸
        cypher_update = """
        MATCH (e:Entity {uuid: $uuid})
        SET e.summary = $new_summary
        SET e.user_modified_at = datetime()
        SET e.user_modified_summary = true
        RETURN e.name as name, e.summary as summary
        """

        client.query(cypher_update, {"uuid": entity_uuid, "new_summary": new_summary})

        logger.info(f"ğŸ”„ Bidirectional update: Entity '{entity_name}' summary updated")

        return {
            "status": "success",
            "message": "Entity summary updated",
            "entity": {
                "uuid": entity_uuid,
                "name": entity_name,
                "old_summary": old_summary[:100] + "..." if len(old_summary) > 100 else old_summary,
                "new_summary": new_summary[:100] + "..." if len(new_summary) > 100 else new_summary
            },
            "feedback_type": "palantir_bidirectional"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Entity summary update error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/entity/bulk-update-types")
async def bulk_update_entity_types(
    request: BulkEntityUpdateRequest,
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
) -> Dict[str, Any]:
    """
    ğŸ”„ ì—¬ëŸ¬ Entity PKM Type ì¼ê´„ ì—…ë°ì´íŠ¸

    í´ëŸ¬ìŠ¤í„° ë·°ì—ì„œ ì—¬ëŸ¬ ì—”í‹°í‹°ë¥¼ í•œ ë²ˆì— ì¬ë¶„ë¥˜í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    valid_types = ["Goal", "Project", "Task", "Topic", "Concept", "Question", "Insight", "Resource", "Person"]

    try:
        updates = request.updates
        if not updates:
            raise HTTPException(status_code=400, detail="No updates provided")

        success_count = 0
        error_count = 0
        results = []

        for update in updates:
            entity_uuid = update.get("entity_uuid")
            new_type = update.get("new_pkm_type")

            if not entity_uuid or not new_type:
                error_count += 1
                continue

            if new_type not in valid_types:
                error_count += 1
                results.append({"uuid": entity_uuid, "status": "error", "reason": "invalid_type"})
                continue

            try:
                cypher_update = f"""
                MATCH (e:Entity {{uuid: $uuid}})
                REMOVE e:Goal, e:Project, e:Task, e:Topic, e:Concept, e:Question, e:Insight, e:Resource, e:Person
                SET e:{new_type}
                SET e.pkm_type = $pkm_type
                SET e.user_modified_at = datetime()
                RETURN e.name as name
                """

                result = client.query(cypher_update, {"uuid": entity_uuid, "pkm_type": new_type})

                if result:
                    success_count += 1
                    results.append({"uuid": entity_uuid, "status": "success", "new_type": new_type})
                else:
                    error_count += 1
                    results.append({"uuid": entity_uuid, "status": "error", "reason": "not_found"})

            except Exception as e:
                error_count += 1
                results.append({"uuid": entity_uuid, "status": "error", "reason": str(e)})

        logger.info(f"ğŸ”„ Bulk update: {success_count} success, {error_count} errors")

        return {
            "status": "success" if error_count == 0 else "partial",
            "total": len(updates),
            "success_count": success_count,
            "error_count": error_count,
            "results": results[:50],  # ìƒìœ„ 50ê°œë§Œ ë°˜í™˜
            "feedback_type": "palantir_bidirectional_bulk"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Bulk update error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/entity/user-modifications")
async def get_user_modifications(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    limit: int = Query(50, description="Maximum results", ge=10, le=200),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
) -> Dict[str, Any]:
    """
    ğŸ“Š ì‚¬ìš©ì ìˆ˜ì • ì´ë ¥ ì¡°íšŒ

    Palantir-style í”¼ë“œë°±ìœ¼ë¡œ ì‚¬ìš©ìê°€ ìˆ˜ì •í•œ Entity ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì–´ë–¤ Entityê°€ ìë™ ë¶„ë¥˜ì™€ ë‹¤ë¥´ê²Œ ìˆ˜ì •ë˜ì—ˆëŠ”ì§€ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    **í™œìš©:**
    - ìë™ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ ê°œì„ ì„ ìœ„í•œ í•™ìŠµ ë°ì´í„°
    - ì‚¬ìš©ì ë„ë©”ì¸ ì§€ì‹ íŒ¨í„´ ë¶„ì„
    - ì‹œìŠ¤í…œ ì •í™•ë„ ì¸¡ì •
    """
    try:
        cypher = """
        MATCH (e:Entity)
        WHERE e.user_modified_at IS NOT NULL
        RETURN e.uuid as uuid,
               e.name as name,
               e.pkm_type as current_type,
               e.user_modified_type as modified_to,
               e.user_modified_summary as summary_modified,
               e.user_modified_at as modified_at
        ORDER BY e.user_modified_at DESC
        LIMIT $limit
        """

        results = client.query(cypher, {"limit": limit})

        modifications = []
        for row in results or []:
            modifications.append({
                "uuid": row["uuid"],
                "name": row["name"],
                "current_type": row.get("current_type"),
                "modified_to": row.get("modified_to"),
                "summary_modified": row.get("summary_modified", False),
                "modified_at": row.get("modified_at")
            })

        # ìˆ˜ì • í†µê³„
        type_changes = {}
        for mod in modifications:
            if mod.get("modified_to"):
                type_changes[mod["modified_to"]] = type_changes.get(mod["modified_to"], 0) + 1

        return {
            "status": "success",
            "total_modifications": len(modifications),
            "modifications": modifications,
            "type_change_distribution": type_changes,
            "feedback_type": "palantir_user_feedback_history"
        }

    except Exception as e:
        logger.error(f"User modifications query error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/vault/debug-graph-structure")
async def debug_graph_structure(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
) -> Dict[str, Any]:
    """
    ë””ë²„ê·¸ìš©: ê·¸ë˜í”„ ë°ì´í„° êµ¬ì¡° í™•ì¸
    - Episodic ë…¸ë“œ ìƒ˜í”Œ
    - Entity ë…¸ë“œ ìƒ˜í”Œ
    - Note ë…¸ë“œ ìƒ˜í”Œ
    - MENTIONS ê´€ê³„ í™•ì¸
    """
    try:
        # 1. Episodic ìƒ˜í”Œ
        episodic_sample = client.query("""
            MATCH (ep:Episodic)
            RETURN ep.name as name, ep.uuid as uuid, labels(ep) as labels
            LIMIT 5
        """, {})

        # 2. Episodic -> Entity MENTIONS ê´€ê³„
        ep_entity_mentions = client.query("""
            MATCH (ep:Episodic)-[m:MENTIONS]->(e:Entity)
            RETURN ep.name as ep_name, e.name as entity_name, e.uuid as entity_uuid
            LIMIT 10
        """, {})

        # 3. Note -> Entity MENTIONS ê´€ê³„
        note_entity_mentions = client.query("""
            MATCH (n:Note)-[m:MENTIONS]->(e:Entity)
            RETURN n.note_id as note_id, e.name as entity_name, e.uuid as entity_uuid
            LIMIT 10
        """, {})

        # 4. Note ìƒ˜í”Œ
        note_sample = client.query("""
            MATCH (n:Note)
            RETURN n.note_id as note_id, n.title as title
            LIMIT 5
        """, {})

        # 5. Entity ë…¸ë“œ ìˆ˜
        entity_count = client.query("""
            MATCH (e:Entity)
            RETURN count(e) as count
        """, {})

        return {
            "episodic_sample": episodic_sample or [],
            "ep_entity_mentions": ep_entity_mentions or [],
            "note_entity_mentions": note_entity_mentions or [],
            "note_sample": note_sample or [],
            "entity_count": entity_count[0]["count"] if entity_count else 0
        }
    except Exception as e:
        logger.error(f"Debug error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
