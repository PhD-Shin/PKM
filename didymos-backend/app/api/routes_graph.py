"""
Graph Visualization API ë¼ìš°í„°
"""
from fastapi import APIRouter, HTTPException, Query, Depends
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
from app.services.graph_visualization_service import (
    get_note_graph,
    get_note_graph_vis,
    get_user_graph,
    get_entity_graph
)
from app.services.cluster_service import (
    compute_clusters_louvain,
    compute_clusters_semantic,
    get_cached_clusters,
    save_cluster_cache,
    invalidate_cluster_cache,
    generate_llm_summaries,
    is_cluster_cache_stale
)
from app.schemas.cluster import (
    ClusteredGraphResponse,
    ClusterComputeRequest,
    ClusterUpdateRequest
)
from app.db.neo4j_bolt import Neo4jBoltClient
import logging

logger = logging.getLogger(__name__)


def get_neo4j_client():
    """Neo4j í´ë¼ì´ì–¸íŠ¸ ì˜ì¡´ì„±"""
    from app.config import settings
    return Neo4jBoltClient(
        uri=settings.neo4j_uri,
        username=settings.neo4j_username,
        password=settings.neo4j_password
    )

router = APIRouter(prefix="/graph", tags=["graph"])


class GraphNode(BaseModel):
    """ê·¸ëž˜í”„ ë…¸ë“œ"""
    id: str
    label: str
    type: str
    properties: Dict[str, Any]


class GraphEdge(BaseModel):
    """ê·¸ëž˜í”„ ì—£ì§€"""
    from_: str = None  # Use alias to avoid 'from' keyword
    to: str
    type: str
    label: str

    class Config:
        fields = {'from_': 'from'}


class GraphResponse(BaseModel):
    """ê·¸ëž˜í”„ ì‘ë‹µ"""
    status: str
    count_nodes: int
    count_edges: int
    nodes: List[Dict[str, Any]]
    edges: List[Dict[str, Any]]


@router.get("/note/{note_id}", response_model=GraphResponse)
async def get_note_graph_view(
    note_id: str,
    depth: int = Query(1, description="íƒìƒ‰ ê¹Šì´", ge=1, le=3)
):
    """
    íŠ¹ì • ë…¸íŠ¸ ì¤‘ì‹¬ì˜ ê·¸ëž˜í”„ ì‹œê°í™” ë°ì´í„°

    - note_id: ì¤‘ì‹¬ ë…¸íŠ¸ ID
    - depth: íƒìƒ‰ ê¹Šì´ (1~3)
    """
    try:
        graph_data = get_note_graph_vis(note_id=note_id, hops=depth)

        return GraphResponse(
            status="success",
            count_nodes=len(graph_data["nodes"]),
            count_edges=len(graph_data["edges"]),
            nodes=graph_data["nodes"],
            edges=graph_data["edges"]
        )

    except Exception as e:
        logger.error(f"Failed to get note graph: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to retrieve graph: {str(e)}"
        )


@router.get("/user/{user_id}", response_model=GraphResponse)
async def get_user_graph_view(
    user_id: str,
    vault_id: Optional[str] = Query(None, description="Vault ID (optional)"),
    limit: int = Query(100, description="ìµœëŒ€ ë…¸ë“œ ê°œìˆ˜", ge=10, le=500)
):
    """
    ì‚¬ìš©ìžì˜ ì „ì²´ ì§€ì‹ ê·¸ëž˜í”„

    - user_id: ì‚¬ìš©ìž ID
    - vault_id: Vault ID (optional)
    - limit: ìµœëŒ€ ë…¸ë“œ ê°œìˆ˜ (ê¸°ë³¸ 100, ìµœëŒ€ 5000)
    """
    try:
        graph_data = get_user_graph(
            user_id=user_id,
            vault_id=vault_id,
            limit=limit
        )

        return GraphResponse(
            status="success",
            count_nodes=len(graph_data["nodes"]),
            count_edges=len(graph_data["edges"]),
            nodes=graph_data["nodes"],
            edges=graph_data["edges"]
        )

    except Exception as e:
        logger.error(f"Failed to get user graph: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to retrieve graph: {str(e)}"
        )


@router.get("/entities", response_model=GraphResponse)
async def get_entities_graph_view(
    entity_type: Optional[str] = Query(None, description="ì—”í‹°í‹° íƒ€ìž… (Topic, Project, Task, Person)"),
    limit: int = Query(50, description="ìµœëŒ€ ì—”í‹°í‹° ê°œìˆ˜", ge=10, le=200)
):
    """
    ì—”í‹°í‹° ì¤‘ì‹¬ ê·¸ëž˜í”„

    - entity_type: í•„í„°ë§í•  ì—”í‹°í‹° íƒ€ìž… (optional)
    - limit: ìµœëŒ€ ì—”í‹°í‹° ê°œìˆ˜
    """
    try:
        graph_data = get_entity_graph(
            entity_type=entity_type,
            limit=limit
        )

        return GraphResponse(
            status="success",
            count_nodes=len(graph_data["nodes"]),
            count_edges=len(graph_data["edges"]),
            nodes=graph_data["nodes"],
            edges=graph_data["edges"]
        )

    except Exception as e:
        logger.error(f"Failed to get entity graph: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to retrieve graph: {str(e)}"
        )


@router.get("/vault/clustered", response_model=ClusteredGraphResponse)
async def get_clustered_vault_graph(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    folder_prefix: str = Query(None, description="í´ë” ê²½ë¡œ í•„í„° (ì˜ˆ: '1_í”„ë¡œì íŠ¸/', '2_ì—°êµ¬/')"),
    force_recompute: bool = Query(False, description="ìºì‹œ ë¬´ì‹œí•˜ê³  ìž¬ê³„ì‚°"),
    target_clusters: int = Query(10, ge=3, le=50, description="ëª©í‘œ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜"),
    include_llm: bool = Query(False, description="LLM ìš”ì•½ í¬í•¨ (ëŠë¦¼)"),
    method: str = Query("semantic", description="í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•: 'semantic' (UMAP+HDBSCAN) ë˜ëŠ” 'type_based'"),
    warmup: bool = Query(False, description="ë°±ê·¸ë¼ìš´ë“œ ìºì‹œ ì›Œë°ì—… (ì‘ë‹µ ì¦‰ì‹œ ë°˜í™˜)"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
):
    """
    í´ëŸ¬ìŠ¤í„°ë§ëœ Vault ê·¸ëž˜í”„

    - vault_id: Vault ID
    - force_recompute: ìºì‹œ ë¬´ì‹œí•˜ê³  ìž¬ê³„ì‚°
    - target_clusters: ëª©í‘œ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
    - include_llm: LLM ìš”ì•½ í¬í•¨ ì—¬ë¶€
    - warmup: ë°±ê·¸ë¼ìš´ë“œ ìºì‹œ ì›Œë°ì—… (ì‘ë‹µ ì¦‰ì‹œ ë°˜í™˜)

    **ì‘ë‹µ ì˜ˆì‹œ:**
    ```json
    {
      "status": "success",
      "level": 1,
      "cluster_count": 3,
      "total_nodes": 2543,
      "clusters": [
        {
          "id": "cluster_1",
          "name": "Topic Cluster",
          "level": 1,
          "node_count": 45,
          "summary": "Research topics related to...",
          "key_insights": ["Insight 1", "Insight 2"],
          "importance_score": 8.5
        }
      ],
      "edges": [],
      "last_computed": "2024-12-01T10:00:00"
    }
    ```
    """
    try:
        # Warmup ëª¨ë“œ: ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìºì‹œ ìƒì„±, ì¦‰ì‹œ ì‘ë‹µ ë°˜í™˜
        if warmup:
            import asyncio
            from concurrent.futures import ThreadPoolExecutor

            def background_warmup():
                try:
                    logger.info(f"ðŸ”¥ Background warmup started for vault {vault_id}")
                    result = compute_clusters_semantic(
                        client=client,
                        vault_id=vault_id,
                        target_clusters=target_clusters
                    )
                    if result.get("clusters"):
                        save_cluster_cache(client, vault_id, result["clusters"], result["method"], edges=result.get("edges", []))
                        logger.info(f"âœ… Background warmup completed for vault {vault_id}")
                except Exception as e:
                    logger.error(f"Background warmup failed: {e}")

            executor = ThreadPoolExecutor(max_workers=1)
            executor.submit(background_warmup)

            return ClusteredGraphResponse(
                status="warming_up",
                level=1,
                cluster_count=0,
                total_nodes=0,
                clusters=[],
                edges=[],
                last_computed="warmup_in_progress",
                computation_method="background_warmup"
            )

        # ìºì‹œ í‚¤ì— folder_prefix í¬í•¨
        cache_key = f"{vault_id}:{folder_prefix or 'all'}"

        # ìºì‹œ í™•ì¸ (folder_prefixê°€ ìžˆìœ¼ë©´ ìºì‹œ ìŠ¤í‚µ - í´ë”ë³„ ìºì‹œëŠ” ë³„ë„ êµ¬í˜„ í•„ìš”)
        if not force_recompute and not folder_prefix:
            cached = get_cached_clusters(client, vault_id)
            if cached and not is_cluster_cache_stale(client, vault_id, cached.get("computed_at")):
                logger.info(f"âœ… Returning cached clusters for vault {vault_id}")
                return ClusteredGraphResponse(
                    status="success",
                    level=1,
                    cluster_count=len(cached["clusters"]),
                    total_nodes=sum(c.get("node_count", 0) for c in cached["clusters"]),
                    clusters=cached["clusters"],
                    edges=cached.get("edges", []),
                    last_computed=cached["computed_at"],
                    computation_method=cached["method"]
                )
            elif cached:
                logger.info(f"â™»ï¸ Cache stale for vault {vault_id}, recomputing...")

        # í´ëŸ¬ìŠ¤í„° ê³„ì‚° (ë°©ë²• ì„ íƒ)
        folder_info = f" in folder '{folder_prefix}'" if folder_prefix else ""
        logger.info(f"ðŸ”„ Computing clusters for vault {vault_id}{folder_info} using method={method}")
        method_normalized = method.lower()

        if method_normalized in ["semantic", "auto"]:
            result = compute_clusters_semantic(
                client=client,
                vault_id=vault_id,
                target_clusters=target_clusters,
                folder_prefix=folder_prefix
            )
        elif method_normalized in ["type_based", "type"]:
            result = compute_clusters_louvain(
                client=client,
                vault_id=vault_id,
                target_clusters=target_clusters,
                folder_prefix=folder_prefix
            )
        else:
            raise HTTPException(status_code=400, detail="Invalid clustering method")

        # ì˜ë¯¸ë¡ ì  í´ëŸ¬ìŠ¤í„°ë§ì´ ì‹¤íŒ¨í–ˆê±°ë‚˜ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ í´ë°±
        if method_normalized in ["semantic", "auto"] and (not result.get("clusters")):
            logger.info("Semantic clustering returned no clusters. Falling back to type-based.")
            result = compute_clusters_louvain(
                client=client,
                vault_id=vault_id,
                target_clusters=target_clusters,
                folder_prefix=folder_prefix
            )
            result["method"] = "semantic_fallback"

        clusters = result["clusters"]
        edges = result.get("edges", [])

        # LLM ìš”ì•½ ìƒì„± (ì˜µì…˜)
        if include_llm and len(clusters) > 0:
            logger.info("ðŸ¤– Generating LLM summaries with GPT-5 Mini...")
            clusters = generate_llm_summaries(client, vault_id, clusters)

        # ìºì‹œ ì €ìž¥ (folder_prefix ì—†ì„ ë•Œë§Œ)
        if not folder_prefix:
            save_cluster_cache(client, vault_id, clusters, result["method"], edges=edges)

        return ClusteredGraphResponse(
            status="success",
            level=1,
            cluster_count=len(clusters),
            total_nodes=result["total_nodes"],
            clusters=clusters,
            edges=edges,
            last_computed=result["computed_at"],
            computation_method=result["method"]
        )

    except Exception as e:
        logger.error(f"Failed to get clustered graph: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to compute clusters: {str(e)}"
        )


@router.post("/vault/clustered/invalidate")
async def invalidate_clusters(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
):
    """
    í´ëŸ¬ìŠ¤í„° ìºì‹œ ë¬´íš¨í™” (ë…¸íŠ¸ ì—…ë°ì´íŠ¸ í›„ í˜¸ì¶œ)

    - vault_id: Vault ID
    """
    try:
        success = invalidate_cluster_cache(client, vault_id)

        if success:
            return {"status": "success", "message": "Cluster cache invalidated"}
        else:
            raise HTTPException(status_code=500, detail="Failed to invalidate cache")

    except Exception as e:
        logger.error(f"Failed to invalidate cluster cache: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to invalidate cache: {str(e)}"
        )


@router.get("/vault/folders")
async def get_vault_folders(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
):
    """
    Vault ë‚´ í´ë” ëª©ë¡ ì¡°íšŒ (PARA ë…¸íŠ¸ ê¸°ë²• ì§€ì›)

    í´ë”ë³„ ë…¸íŠ¸ ê°œìˆ˜ì™€ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # ë…¸íŠ¸ ê²½ë¡œì—ì„œ í´ë” ì¶”ì¶œ
        cypher = """
        MATCH (v:Vault {id: $vault_id})-[:HAS_NOTE]->(n:Note)
        WITH n.note_id AS note_id
        WITH split(note_id, '/')[0] AS folder
        WHERE folder IS NOT NULL AND folder <> ''
        RETURN folder, count(*) AS note_count
        ORDER BY note_count DESC
        """

        result = client.query(cypher, {"vault_id": vault_id})

        folders = [
            {"folder": r["folder"], "note_count": r["note_count"]}
            for r in (result or [])
        ]

        return {
            "status": "success",
            "vault_id": vault_id,
            "total_folders": len(folders),
            "folders": folders
        }

    except Exception as e:
        logger.error(f"Failed to get vault folders: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get folders: {str(e)}"
        )
