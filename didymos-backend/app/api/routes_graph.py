"""
Graph Visualization API ë¼ìš°í„°
"""
from fastapi import APIRouter, HTTPException, Query, Depends
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
from app.services.graph_visualization_service import (
    get_note_graph,
    get_note_graph_vis,
    get_user_graph,
    get_entity_graph
)
from app.services.cluster_service import (
    compute_clusters_louvain,
    compute_clusters_semantic,
    get_cached_clusters,
    save_cluster_cache,
    invalidate_cluster_cache,
    generate_llm_summaries,
    is_cluster_cache_stale
)
from app.schemas.cluster import (
    ClusteredGraphResponse,
    ClusterComputeRequest,
    ClusterUpdateRequest
)
from app.db.neo4j_bolt import Neo4jBoltClient
import logging

logger = logging.getLogger(__name__)


def get_neo4j_client():
    """Neo4j í´ë¼ì´ì–¸íŠ¸ ì˜ì¡´ì„±"""
    from app.config import settings
    return Neo4jBoltClient(
        uri=settings.neo4j_uri,
        username=settings.neo4j_username,
        password=settings.neo4j_password
    )

router = APIRouter(prefix="/graph", tags=["graph"])


class GraphNode(BaseModel):
    """ê·¸ë˜í”„ ë…¸ë“œ"""
    id: str
    label: str
    type: str
    properties: Dict[str, Any]


class GraphEdge(BaseModel):
    """ê·¸ë˜í”„ ì—£ì§€"""
    from_: str = None  # Use alias to avoid 'from' keyword
    to: str
    type: str
    label: str

    class Config:
        fields = {'from_': 'from'}


class GraphResponse(BaseModel):
    """ê·¸ë˜í”„ ì‘ë‹µ"""
    status: str
    count_nodes: int
    count_edges: int
    nodes: List[Dict[str, Any]]
    edges: List[Dict[str, Any]]


@router.get("/note/{note_id}", response_model=GraphResponse)
async def get_note_graph_view(
    note_id: str,
    depth: int = Query(1, description="íƒìƒ‰ ê¹Šì´", ge=1, le=3)
):
    """
    íŠ¹ì • ë…¸íŠ¸ ì¤‘ì‹¬ì˜ ê·¸ë˜í”„ ì‹œê°í™” ë°ì´í„°

    - note_id: ì¤‘ì‹¬ ë…¸íŠ¸ ID
    - depth: íƒìƒ‰ ê¹Šì´ (1~3)
    """
    try:
        graph_data = get_note_graph_vis(note_id=note_id, hops=depth)

        return GraphResponse(
            status="success",
            count_nodes=len(graph_data["nodes"]),
            count_edges=len(graph_data["edges"]),
            nodes=graph_data["nodes"],
            edges=graph_data["edges"]
        )

    except Exception as e:
        logger.error(f"Failed to get note graph: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to retrieve graph: {str(e)}"
        )


@router.get("/user/{user_id}", response_model=GraphResponse)
async def get_user_graph_view(
    user_id: str,
    vault_id: Optional[str] = Query(None, description="Vault ID (optional)"),
    limit: int = Query(100, description="ìµœëŒ€ ë…¸ë“œ ê°œìˆ˜", ge=10, le=500)
):
    """
    ì‚¬ìš©ìì˜ ì „ì²´ ì§€ì‹ ê·¸ë˜í”„

    - user_id: ì‚¬ìš©ì ID
    - vault_id: Vault ID (optional)
    - limit: ìµœëŒ€ ë…¸ë“œ ê°œìˆ˜ (ê¸°ë³¸ 100, ìµœëŒ€ 5000)
    """
    try:
        graph_data = get_user_graph(
            user_id=user_id,
            vault_id=vault_id,
            limit=limit
        )

        return GraphResponse(
            status="success",
            count_nodes=len(graph_data["nodes"]),
            count_edges=len(graph_data["edges"]),
            nodes=graph_data["nodes"],
            edges=graph_data["edges"]
        )

    except Exception as e:
        logger.error(f"Failed to get user graph: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to retrieve graph: {str(e)}"
        )


@router.get("/entities", response_model=GraphResponse)
async def get_entities_graph_view(
    entity_type: Optional[str] = Query(None, description="ì—”í‹°í‹° íƒ€ì… (Topic, Project, Task, Person)"),
    limit: int = Query(50, description="ìµœëŒ€ ì—”í‹°í‹° ê°œìˆ˜", ge=10, le=200)
):
    """
    ì—”í‹°í‹° ì¤‘ì‹¬ ê·¸ë˜í”„

    - entity_type: í•„í„°ë§í•  ì—”í‹°í‹° íƒ€ì… (optional)
    - limit: ìµœëŒ€ ì—”í‹°í‹° ê°œìˆ˜
    """
    try:
        graph_data = get_entity_graph(
            entity_type=entity_type,
            limit=limit
        )

        return GraphResponse(
            status="success",
            count_nodes=len(graph_data["nodes"]),
            count_edges=len(graph_data["edges"]),
            nodes=graph_data["nodes"],
            edges=graph_data["edges"]
        )

    except Exception as e:
        logger.error(f"Failed to get entity graph: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to retrieve graph: {str(e)}"
        )


@router.get("/vault/clustered", response_model=ClusteredGraphResponse)
async def get_clustered_vault_graph(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    folder_prefix: str = Query(None, description="í´ë” ê²½ë¡œ í•„í„° (ì˜ˆ: '1_í”„ë¡œì íŠ¸/', '2_ì—°êµ¬/')"),
    force_recompute: bool = Query(False, description="ìºì‹œ ë¬´ì‹œí•˜ê³  ì¬ê³„ì‚°"),
    target_clusters: int = Query(10, ge=3, le=50, description="ëª©í‘œ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜"),
    include_llm: bool = Query(False, description="LLM ìš”ì•½ í¬í•¨ (ëŠë¦¼)"),
    method: str = Query("semantic", description="í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•: 'semantic' (UMAP+HDBSCAN) ë˜ëŠ” 'type_based'"),
    warmup: bool = Query(False, description="ë°±ê·¸ë¼ìš´ë“œ ìºì‹œ ì›Œë°ì—… (ì‘ë‹µ ì¦‰ì‹œ ë°˜í™˜)"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
):
    """
    í´ëŸ¬ìŠ¤í„°ë§ëœ Vault ê·¸ë˜í”„

    - vault_id: Vault ID
    - force_recompute: ìºì‹œ ë¬´ì‹œí•˜ê³  ì¬ê³„ì‚°
    - target_clusters: ëª©í‘œ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
    - include_llm: LLM ìš”ì•½ í¬í•¨ ì—¬ë¶€
    - warmup: ë°±ê·¸ë¼ìš´ë“œ ìºì‹œ ì›Œë°ì—… (ì‘ë‹µ ì¦‰ì‹œ ë°˜í™˜)

    **ì‘ë‹µ ì˜ˆì‹œ:**
    ```json
    {
      "status": "success",
      "level": 1,
      "cluster_count": 3,
      "total_nodes": 2543,
      "clusters": [
        {
          "id": "cluster_1",
          "name": "Topic Cluster",
          "level": 1,
          "node_count": 45,
          "summary": "Research topics related to...",
          "key_insights": ["Insight 1", "Insight 2"],
          "importance_score": 8.5
        }
      ],
      "edges": [],
      "last_computed": "2024-12-01T10:00:00"
    }
    ```
    """
    try:
        # Warmup ëª¨ë“œ: ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìºì‹œ ìƒì„±, ì¦‰ì‹œ ì‘ë‹µ ë°˜í™˜
        if warmup:
            import asyncio
            from concurrent.futures import ThreadPoolExecutor

            def background_warmup():
                try:
                    logger.info(f"ğŸ”¥ Background warmup started for vault {vault_id}")
                    result = compute_clusters_semantic(
                        client=client,
                        vault_id=vault_id,
                        target_clusters=target_clusters
                    )
                    if result.get("clusters"):
                        save_cluster_cache(client, vault_id, result["clusters"], result["method"], edges=result.get("edges", []))
                        logger.info(f"âœ… Background warmup completed for vault {vault_id}")
                except Exception as e:
                    logger.error(f"Background warmup failed: {e}")

            executor = ThreadPoolExecutor(max_workers=1)
            executor.submit(background_warmup)

            return ClusteredGraphResponse(
                status="warming_up",
                level=1,
                cluster_count=0,
                total_nodes=0,
                clusters=[],
                edges=[],
                last_computed="warmup_in_progress",
                computation_method="background_warmup"
            )

        # ìºì‹œ í‚¤ì— folder_prefix í¬í•¨
        cache_key = f"{vault_id}:{folder_prefix or 'all'}"

        # ìºì‹œ í™•ì¸ (folder_prefixê°€ ìˆìœ¼ë©´ ìºì‹œ ìŠ¤í‚µ - í´ë”ë³„ ìºì‹œëŠ” ë³„ë„ êµ¬í˜„ í•„ìš”)
        if not force_recompute and not folder_prefix:
            cached = get_cached_clusters(client, vault_id)
            if cached and not is_cluster_cache_stale(client, vault_id, cached.get("computed_at")):
                logger.info(f"âœ… Returning cached clusters for vault {vault_id}")
                return ClusteredGraphResponse(
                    status="success",
                    level=1,
                    cluster_count=len(cached["clusters"]),
                    total_nodes=sum(c.get("node_count", 0) for c in cached["clusters"]),
                    clusters=cached["clusters"],
                    edges=cached.get("edges", []),
                    last_computed=cached["computed_at"],
                    computation_method=cached["method"]
                )
            elif cached:
                logger.info(f"â™»ï¸ Cache stale for vault {vault_id}, recomputing...")

        # í´ëŸ¬ìŠ¤í„° ê³„ì‚° (ë°©ë²• ì„ íƒ)
        folder_info = f" in folder '{folder_prefix}'" if folder_prefix else ""
        logger.info(f"ğŸ”„ Computing clusters for vault {vault_id}{folder_info} using method={method}")
        method_normalized = method.lower()

        if method_normalized in ["semantic", "auto"]:
            result = compute_clusters_semantic(
                client=client,
                vault_id=vault_id,
                target_clusters=target_clusters,
                folder_prefix=folder_prefix
            )
        elif method_normalized in ["type_based", "type"]:
            result = compute_clusters_louvain(
                client=client,
                vault_id=vault_id,
                target_clusters=target_clusters,
                folder_prefix=folder_prefix
            )
        else:
            raise HTTPException(status_code=400, detail="Invalid clustering method")

        # ì˜ë¯¸ë¡ ì  í´ëŸ¬ìŠ¤í„°ë§ì´ ì‹¤íŒ¨í–ˆê±°ë‚˜ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ í´ë°±
        if method_normalized in ["semantic", "auto"] and (not result.get("clusters")):
            logger.info("Semantic clustering returned no clusters. Falling back to type-based.")
            result = compute_clusters_louvain(
                client=client,
                vault_id=vault_id,
                target_clusters=target_clusters,
                folder_prefix=folder_prefix
            )
            result["method"] = "semantic_fallback"

        clusters = result["clusters"]
        edges = result.get("edges", [])

        # LLM ìš”ì•½ ìƒì„± (ì˜µì…˜)
        if include_llm and len(clusters) > 0:
            logger.info("ğŸ¤– Generating LLM summaries with GPT-5 Mini...")
            clusters = generate_llm_summaries(client, vault_id, clusters)

        # ìºì‹œ ì €ì¥ (folder_prefix ì—†ì„ ë•Œë§Œ)
        if not folder_prefix:
            save_cluster_cache(client, vault_id, clusters, result["method"], edges=edges)

        return ClusteredGraphResponse(
            status="success",
            level=1,
            cluster_count=len(clusters),
            total_nodes=result["total_nodes"],
            clusters=clusters,
            edges=edges,
            last_computed=result["computed_at"],
            computation_method=result["method"]
        )

    except Exception as e:
        logger.error(f"Failed to get clustered graph: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to compute clusters: {str(e)}"
        )


@router.post("/vault/clustered/invalidate")
async def invalidate_clusters(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
):
    """
    í´ëŸ¬ìŠ¤í„° ìºì‹œ ë¬´íš¨í™” (ë…¸íŠ¸ ì—…ë°ì´íŠ¸ í›„ í˜¸ì¶œ)

    - vault_id: Vault ID
    """
    try:
        success = invalidate_cluster_cache(client, vault_id)

        if success:
            return {"status": "success", "message": "Cluster cache invalidated"}
        else:
            raise HTTPException(status_code=500, detail="Failed to invalidate cache")

    except Exception as e:
        logger.error(f"Failed to invalidate cluster cache: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to invalidate cache: {str(e)}"
        )


@router.post("/vault/reset-entities")
async def reset_vault_entities(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
):
    """
    ğŸ”´ Vault ì—”í‹°í‹° ì™„ì „ ì´ˆê¸°í™” (MVP ê°œë°œìš©)

    - ëª¨ë“  Topic, Project, Task, Person ì—”í‹°í‹° ì‚­ì œ
    - MENTIONS ê´€ê³„ ì‚­ì œ
    - í´ëŸ¬ìŠ¤í„° ìºì‹œ ë¬´íš¨í™”
    - Note ë…¸ë“œëŠ” ìœ ì§€

    âš ï¸ ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!
    """
    try:
        # 1. Vaultì— ì—°ê²°ëœ ì—”í‹°í‹°ì™€ ê´€ê³„ ì‚­ì œ
        cypher_delete_entities = """
        MATCH (v:Vault {id: $vault_id})-[:HAS_NOTE]->(n:Note)-[m:MENTIONS]->(e)
        WHERE e:Topic OR e:Project OR e:Task OR e:Person
        DELETE m
        WITH DISTINCT e
        WHERE NOT (e)--()
        DELETE e
        RETURN count(e) as deleted_entities
        """

        result1 = client.query(cypher_delete_entities, {"vault_id": vault_id})
        deleted_entities = result1[0]["deleted_entities"] if result1 else 0

        # 2. ê³ ì•„ ì—”í‹°í‹° ì •ë¦¬ (ë‹¤ë¥¸ vaultì—ì„œë„ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ê²½ìš°)
        cypher_cleanup_orphans = """
        MATCH (e)
        WHERE (e:Topic OR e:Project OR e:Task OR e:Person)
          AND NOT (e)--()
        DELETE e
        RETURN count(e) as orphans_deleted
        """

        result2 = client.query(cypher_cleanup_orphans, {})
        orphans_deleted = result2[0]["orphans_deleted"] if result2 else 0

        # 3. ì—”í‹°í‹° ê°„ ê´€ê³„ë„ ì •ë¦¬
        cypher_delete_entity_relations = """
        MATCH (v:Vault {id: $vault_id})-[:HAS_NOTE]->(n:Note)
        WITH COLLECT(n.note_id) as note_ids
        MATCH (e1)-[r:RELATED_TO|PART_OF]->(e2)
        WHERE (e1:Topic OR e1:Project OR e1:Task OR e1:Person)
          AND (e2:Topic OR e2:Project OR e2:Task OR e2:Person)
        DELETE r
        RETURN count(r) as relations_deleted
        """

        result3 = client.query(cypher_delete_entity_relations, {"vault_id": vault_id})
        relations_deleted = result3[0]["relations_deleted"] if result3 else 0

        # 4. í´ëŸ¬ìŠ¤í„° ìºì‹œ ë¬´íš¨í™”
        invalidate_cluster_cache(client, vault_id)

        logger.info(f"ğŸ”´ Reset entities for vault {vault_id}: {deleted_entities} entities, {orphans_deleted} orphans, {relations_deleted} relations")

        return {
            "status": "success",
            "message": "Vault entities reset complete",
            "deleted_entities": deleted_entities,
            "orphans_deleted": orphans_deleted,
            "relations_deleted": relations_deleted
        }

    except Exception as e:
        logger.error(f"Failed to reset vault entities: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to reset entities: {str(e)}"
        )


@router.get("/vault/folders")
async def get_vault_folders(
    vault_id: str = Query(..., description="Vault ID"),
    user_token: str = Query(..., description="User token"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
):
    """
    Vault ë‚´ í´ë” ëª©ë¡ ì¡°íšŒ (PARA ë…¸íŠ¸ ê¸°ë²• ì§€ì›)

    í´ë”ë³„ ë…¸íŠ¸ ê°œìˆ˜ì™€ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # ë…¸íŠ¸ ê²½ë¡œì—ì„œ í´ë” ì¶”ì¶œ
        cypher = """
        MATCH (v:Vault {id: $vault_id})-[:HAS_NOTE]->(n:Note)
        WITH n.note_id AS note_id
        WITH split(note_id, '/')[0] AS folder
        WHERE folder IS NOT NULL AND folder <> ''
        RETURN folder, count(*) AS note_count
        ORDER BY note_count DESC
        """

        result = client.query(cypher, {"vault_id": vault_id})

        folders = [
            {"folder": r["folder"], "note_count": r["note_count"]}
            for r in (result or [])
        ]

        return {
            "status": "success",
            "vault_id": vault_id,
            "total_folders": len(folders),
            "folders": folders
        }

    except Exception as e:
        logger.error(f"Failed to get vault folders: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get folders: {str(e)}"
        )


@router.get("/debug/stats")
async def get_debug_stats(
    vault_id: str = Query(..., description="Vault ID"),
    client: Neo4jBoltClient = Depends(get_neo4j_client)
):
    """
    ë””ë²„ê·¸ìš©: Neo4j ë°ì´í„° í†µê³„ í™•ì¸
    """
    try:
        # 1. Vault ì¡´ì¬ í™•ì¸
        vault_check = client.query(
            "MATCH (v:Vault {id: $vault_id}) RETURN v.id AS id",
            {"vault_id": vault_id}
        )

        # 2. ì „ì²´ Note ìˆ˜
        total_notes = client.query(
            "MATCH (n:Note) RETURN count(n) AS count",
            {}
        )

        # 3. Vaultì— ì—°ê²°ëœ Note ìˆ˜
        vault_notes = client.query(
            "MATCH (v:Vault {id: $vault_id})-[:HAS_NOTE]->(n:Note) RETURN count(n) AS count",
            {"vault_id": vault_id}
        )

        # 4. ì„ë² ë”©ì´ ìˆëŠ” Note ìˆ˜
        notes_with_embedding = client.query(
            "MATCH (v:Vault {id: $vault_id})-[:HAS_NOTE]->(n:Note) WHERE n.embedding IS NOT NULL RETURN count(n) AS count",
            {"vault_id": vault_id}
        )

        # 5. ì „ì²´ Vault ëª©ë¡
        all_vaults = client.query(
            "MATCH (v:Vault) RETURN v.id AS id LIMIT 10",
            {}
        )

        # 6. ì—”í‹°í‹° ìˆ˜ (Topic, Project, Task, Person)
        entity_counts = client.query(
            """
            MATCH (v:Vault {id: $vault_id})-[:HAS_NOTE]->(n:Note)-[:MENTIONS]->(e)
            WHERE e:Topic OR e:Project OR e:Task OR e:Person
            WITH labels(e)[0] AS entity_type, count(DISTINCT e) AS cnt
            RETURN entity_type, cnt
            """,
            {"vault_id": vault_id}
        )

        # 7. Note-Entity MENTIONS ê´€ê³„ ìˆ˜
        mentions_count = client.query(
            """
            MATCH (v:Vault {id: $vault_id})-[:HAS_NOTE]->(n:Note)-[m:MENTIONS]->(e)
            RETURN count(m) AS count
            """,
            {"vault_id": vault_id}
        )

        entity_stats = {r["entity_type"]: r["cnt"] for r in (entity_counts or [])}

        return {
            "vault_id_queried": vault_id,
            "vault_exists": len(vault_check or []) > 0,
            "all_vaults": [v["id"] for v in (all_vaults or [])],
            "total_notes_in_db": (total_notes[0]["count"] if total_notes else 0),
            "notes_in_vault": (vault_notes[0]["count"] if vault_notes else 0),
            "notes_with_embedding": (notes_with_embedding[0]["count"] if notes_with_embedding else 0),
            "entity_counts": entity_stats,
            "total_mentions": (mentions_count[0]["count"] if mentions_count else 0),
        }

    except Exception as e:
        logger.error(f"Debug stats error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/migrate/graphiti-to-hybrid")
async def migrate_graphiti_to_hybrid(
    vault_id: str = Query(None, description="Vault ID (optional, all if not specified)"),
    max_iterations: int = Query(10, description="Maximum migration iterations")
) -> Dict[str, Any]:
    """
    Graphiti EntityNodeì— PKM ë ˆì´ë¸” ì¶”ê°€ ë§ˆì´ê·¸ë ˆì´ì…˜

    Graphitiê°€ ìƒì„±í•œ EntityNodeì— Topic/Project/Task/Person ë ˆì´ë¸”ì„ ì¶”ê°€í•˜ì—¬
    cluster_serviceì™€ í˜¸í™˜ë˜ë„ë¡ í•©ë‹ˆë‹¤.

    ì´ ì‘ì—…ì€ ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    1. EntityNodeì— PKM íƒ€ì… ë ˆì´ë¸” ì¶”ê°€ (Topic, Project, Task, Person)
    2. Episode-Entity ê´€ê³„ë¥¼ Note-Entity MENTIONSë¡œ ë³€í™˜
    """
    try:
        from app.services.hybrid_graphiti_service import migrate_graphiti_to_hybrid

        logger.info(f"Starting Graphiti â†’ Hybrid migration (vault: {vault_id or 'all'})")

        result = await migrate_graphiti_to_hybrid(
            vault_id=vault_id,
            max_iterations=max_iterations
        )

        return {
            "status": "success",
            "migration_result": result
        }

    except Exception as e:
        logger.error(f"Migration error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/debug/entity-nodes")
async def get_entity_node_stats() -> Dict[str, Any]:
    """
    Graphiti EntityNode í†µê³„ ì¡°íšŒ

    EntityNode ì¤‘ PKM ë ˆì´ë¸”ì´ ìˆëŠ” ê²ƒê³¼ ì—†ëŠ” ê²ƒì˜ ìˆ˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    try:
        client = get_neo4j_client()

        # EntityNode ì „ì²´ ìˆ˜
        total = client.query("MATCH (e:EntityNode) RETURN count(e) as count", {})

        # PKM ë ˆì´ë¸”ì´ ìˆëŠ” EntityNode
        with_pkm = client.query("""
            MATCH (e:EntityNode)
            WHERE e:Topic OR e:Project OR e:Task OR e:Person
            RETURN count(e) as count
        """, {})

        # PKM ë ˆì´ë¸”ì´ ì—†ëŠ” EntityNode
        without_pkm = client.query("""
            MATCH (e:EntityNode)
            WHERE NOT e:Topic AND NOT e:Project AND NOT e:Task AND NOT e:Person
            RETURN count(e) as count
        """, {})

        # PKM íƒ€ì…ë³„ í†µê³„
        by_type = client.query("""
            MATCH (e:EntityNode)
            WHERE e:Topic OR e:Project OR e:Task OR e:Person
            WITH CASE
                WHEN e:Topic THEN 'Topic'
                WHEN e:Project THEN 'Project'
                WHEN e:Task THEN 'Task'
                WHEN e:Person THEN 'Person'
            END as pkm_type
            RETURN pkm_type, count(*) as count
        """, {})

        return {
            "total_entity_nodes": total[0]["count"] if total else 0,
            "with_pkm_labels": with_pkm[0]["count"] if with_pkm else 0,
            "without_pkm_labels": without_pkm[0]["count"] if without_pkm else 0,
            "by_pkm_type": {r["pkm_type"]: r["count"] for r in (by_type or [])}
        }

    except Exception as e:
        logger.error(f"EntityNode stats error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
